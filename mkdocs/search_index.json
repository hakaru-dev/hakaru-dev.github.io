{
    "docs": [
        {
            "location": "/", 
            "text": "Hakaru\n\n\n\n\n\nHakaru is a probabilistic programming language. A probabilistic programming\nlanguage is a language specifically designed for manipulating probability\ndistributions. These sorts of languages are great for machine learning and\nstochastic modeling.\n\n\nOverview\n\n\nThis manual provides a guide for how to use Hakaru.\n\n\nIntroduction\n\n\nWhat is Probabilistic Programming\n\n\nProbabilistic programming systems allow us to write programs which\ndescribe probability distributions, and provide mechanisms to\nsample and condition the distributions they represent on data. In\nthis page, we give a sense of the sorts of problems Hakaru is\ngreat at solving, and how you would describe them in Hakaru.\n\n\nInstallation\n\n\nLearn how to install Hakaru\n\n\nQuickstart\n\n\nGet started with this quickstart page. Where we show\nhow to sample and condition from a small Hakaru program.\n\n\nExamples\n\n\nHere we go through several more involved examples of the kinds of\nproblems Hakaru is uniquely well-suited to solve.\n\n\nIn particular, we describe a model for Gaussian Mixture Models and\nusing a form of Bayesian Naives Bayes as applied to document\nclassification.\n\n\nLanguage Guide\n\n\nThe language section provides an overview of the syntax of Hakaru as\nwell as some of the primitives in the language.\n\n\nRandom Primitives\n\n\nThese are the built-in probability distributions.\n\n\nLet and Bind\n\n\nThis is how we can give names to subexpressions and a\ndraw from a probability distribution.\n\n\nConditionals\n\n\nHakaru supports a restricted \nif\n expression\n\n\nTypes and Coercions\n\n\nHakaru is a simply-typed language. This section\ndescribes the types available and functions for\nmoving between them.\n\n\nFunctions\n\n\nDefining and using functions\n\n\nDatatypes and match\n\n\nHakaru supports a few built-in datatypes, and offers functionality for\ntaking them apart and reconstructing them.\n\n\nArrays and loops\n\n\nWe offer special support for arrays, and for probability\ndistributions over arrays.\nWe also express loops that compute sums and products.\n\n\nTransformations\n\n\nHakaru implements its inference algorithms predominately as\nprogram transformations. The following are the major ones\nour system provides.\n\n\nExpect\n\n\nDisintegrate\n\n\nSimplify\n\n\nMetropolis Hastings\n\n\nInternals\n\n\nThe internals section of the manual provides some insight into how\nHakaru is implemented and offers guidance into how the system can\nbe extended.\n\n\nAST\n\n\nABT\n\n\nDatums\n\n\nCoercions\n\n\nTransformations\n\n\nTesting\n\n\nAdding a Language Feature", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "This manual provides a guide for how to use Hakaru.", 
            "title": "Overview"
        }, 
        {
            "location": "/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/#what-is-probabilistic-programming", 
            "text": "Probabilistic programming systems allow us to write programs which\ndescribe probability distributions, and provide mechanisms to\nsample and condition the distributions they represent on data. In\nthis page, we give a sense of the sorts of problems Hakaru is\ngreat at solving, and how you would describe them in Hakaru.", 
            "title": "What is Probabilistic Programming"
        }, 
        {
            "location": "/#installation", 
            "text": "Learn how to install Hakaru", 
            "title": "Installation"
        }, 
        {
            "location": "/#quickstart", 
            "text": "Get started with this quickstart page. Where we show\nhow to sample and condition from a small Hakaru program.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#examples", 
            "text": "Here we go through several more involved examples of the kinds of\nproblems Hakaru is uniquely well-suited to solve.  In particular, we describe a model for Gaussian Mixture Models and\nusing a form of Bayesian Naives Bayes as applied to document\nclassification.", 
            "title": "Examples"
        }, 
        {
            "location": "/#language-guide", 
            "text": "The language section provides an overview of the syntax of Hakaru as\nwell as some of the primitives in the language.", 
            "title": "Language Guide"
        }, 
        {
            "location": "/#random-primitives", 
            "text": "These are the built-in probability distributions.", 
            "title": "Random Primitives"
        }, 
        {
            "location": "/#let-and-bind", 
            "text": "This is how we can give names to subexpressions and a\ndraw from a probability distribution.", 
            "title": "Let and Bind"
        }, 
        {
            "location": "/#conditionals", 
            "text": "Hakaru supports a restricted  if  expression", 
            "title": "Conditionals"
        }, 
        {
            "location": "/#types-and-coercions", 
            "text": "Hakaru is a simply-typed language. This section\ndescribes the types available and functions for\nmoving between them.", 
            "title": "Types and Coercions"
        }, 
        {
            "location": "/#functions", 
            "text": "Defining and using functions", 
            "title": "Functions"
        }, 
        {
            "location": "/#datatypes-and-match", 
            "text": "Hakaru supports a few built-in datatypes, and offers functionality for\ntaking them apart and reconstructing them.", 
            "title": "Datatypes and match"
        }, 
        {
            "location": "/#arrays-and-loops", 
            "text": "We offer special support for arrays, and for probability\ndistributions over arrays.\nWe also express loops that compute sums and products.", 
            "title": "Arrays and loops"
        }, 
        {
            "location": "/#transformations", 
            "text": "Hakaru implements its inference algorithms predominately as\nprogram transformations. The following are the major ones\nour system provides.  Expect  Disintegrate  Simplify  Metropolis Hastings", 
            "title": "Transformations"
        }, 
        {
            "location": "/#internals", 
            "text": "The internals section of the manual provides some insight into how\nHakaru is implemented and offers guidance into how the system can\nbe extended.  AST  ABT  Datums  Coercions  Transformations  Testing  Adding a Language Feature", 
            "title": "Internals"
        }, 
        {
            "location": "/intro/probprog/", 
            "text": "What is Probabilistic Programming?\n\n\nProbabilistic programs are programs which represent probability\ndistributions. For example, the program \npoisson(5)\n represents the\npoisson distribution with a rate of five. Why do we need a\nlanguage for describing probability distributions?\n\n\nThe world is intrinsically an uncertain place. When we try to predict\nwhat will happen in the world given some data we have collected, we\nare inherently engaging in some sort of probabilistic modeling. In\nprobabilistic modeling, we treat the quantity we wish to predict as a\nparameter, and then describe our data as some noisy function of this\nparameter. This function is called \nlikelihood\n, and depending on which\nstatistical regime you use can be used in predominately two ways.\n\n\nFor instance, we might want to estimate the average time it takes for\na bus to arrive at a stop, based on actual arrival times. In this situation,\nthe likelihood function would be:\n\n\n$$ x \\sim \\text{Poisson}(\\lambda) $$\n\n\nwhere $x$ is the actual arrival time, and $\\lambda$ is the quantity we\nwish to predict. In other words, this likelihood says our data is a\nnoisy measurement of ther average waiting time which follows a Poisson\ndistribution. We can also represent this likelihood function as a\ndensity function which for a given choice of $\\lambda$ returns how\nlikely it is for $x$ to be generated under that parameter.\n\n\n$$ f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} $$\n\n\nUnder a frequentist regime we perform maximum likelihood, where we find\nthe best parameter by finding the $\\lambda$ which maximizes $f$.\n\n\nUnder a Bayesian regime, we don\nt estimate a single best value for the\nparameter. Instead we place a prior distribution on the parameters and\nestimate that posterior distribution conditioned on our data.\n\n\nIn Hakaru, it is possible to use either regime for solving your\nproblems.  We will call the distribution or program which describes\nour data the \nmodel\n.\n\n\nTug of War\n\n\nWe demonstrate the value of this problem-solving approach using a\nsimplified version of the\n\ntug of war\n\nexample from probmods.org. For this problem we will take a Bayesian\napproach to prediction.\n\n\nFor this problem, we have three friends, Alice, Bob and Carol who take\nturns playing a tug of war against each other and we\nd like to know\nwhich of them is the strongest. We can pose this problem as a\nprobabilistic program. In particular, we will try to predict who will\nwin match3 given we have observed who won the first two matches.\n\n\nWe can start by assuming each player\ns strength comes from a standard\nnormal distribution. Then we assume the strength they pull with some\nnormal distribution centered around their true strength, and the\nperson who pulled harder wins.\n\n\ndef pulls(strength real):\n    normal(strength, 1)\n\ndef winner(a real, b real):\n    a_pull \n~ pulls(a)\n    b_pull \n~ pulls(b)\n    return (a_pull \n b_pull)\n\nalice \n~ normal(0,1)\nbob   \n~ normal(0,1)\ncarol \n~ normal(0,1)\n\nmatch1 \n~ winner(alice, bob)\nmatch2 \n~ winner(bob, carol)\nmatch3 \n~ winner(alice, carol)\n\n\n\n\nWe then restrict the set of events to only those where Alice won the\nfirst match and Bob won the second, and return the results of the\nthird match.\n\n\nif match1 \n match2:\n   return match3\nelse:\n   reject. measure(bool)\n\n\n\n\nWe can then run the above model using hakaru, which shows that Alice\nis likely to win her match against Carol.\n\n\nhakaru tugofwar.hk | head -n 10000 | sort | uniq -c\n   3060 false\n   6940 true\n\n\n\n\nSimulation vs Inference\n\n\nOf course, in the above program we performed inference, by taking\nour model and throwing out all events that didn\nt agree with\nthe data we had. How well would this work if we changed our\nmodel slightly? Suppose our data wasn\nt boolean values, but instead\nthe difference of strengths, and we want to not just whether Alice\nwill win, but by how much.\n\n\nAs we pose more complex questions, posing our models as rejection\nsamplers becomes increasing inefficient.\n\n\n\n    \n\n        \nTODO\n\n    \n\n    \n\n        Explain simplify and mh", 
            "title": "What is Probabilistic Programming"
        }, 
        {
            "location": "/intro/probprog/#what-is-probabilistic-programming", 
            "text": "Probabilistic programs are programs which represent probability\ndistributions. For example, the program  poisson(5)  represents the\npoisson distribution with a rate of five. Why do we need a\nlanguage for describing probability distributions?  The world is intrinsically an uncertain place. When we try to predict\nwhat will happen in the world given some data we have collected, we\nare inherently engaging in some sort of probabilistic modeling. In\nprobabilistic modeling, we treat the quantity we wish to predict as a\nparameter, and then describe our data as some noisy function of this\nparameter. This function is called  likelihood , and depending on which\nstatistical regime you use can be used in predominately two ways.  For instance, we might want to estimate the average time it takes for\na bus to arrive at a stop, based on actual arrival times. In this situation,\nthe likelihood function would be:  $$ x \\sim \\text{Poisson}(\\lambda) $$  where $x$ is the actual arrival time, and $\\lambda$ is the quantity we\nwish to predict. In other words, this likelihood says our data is a\nnoisy measurement of ther average waiting time which follows a Poisson\ndistribution. We can also represent this likelihood function as a\ndensity function which for a given choice of $\\lambda$ returns how\nlikely it is for $x$ to be generated under that parameter.  $$ f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} $$  Under a frequentist regime we perform maximum likelihood, where we find\nthe best parameter by finding the $\\lambda$ which maximizes $f$.  Under a Bayesian regime, we don t estimate a single best value for the\nparameter. Instead we place a prior distribution on the parameters and\nestimate that posterior distribution conditioned on our data.  In Hakaru, it is possible to use either regime for solving your\nproblems.  We will call the distribution or program which describes\nour data the  model .", 
            "title": "What is Probabilistic Programming?"
        }, 
        {
            "location": "/intro/probprog/#tug-of-war", 
            "text": "We demonstrate the value of this problem-solving approach using a\nsimplified version of the tug of war \nexample from probmods.org. For this problem we will take a Bayesian\napproach to prediction.  For this problem, we have three friends, Alice, Bob and Carol who take\nturns playing a tug of war against each other and we d like to know\nwhich of them is the strongest. We can pose this problem as a\nprobabilistic program. In particular, we will try to predict who will\nwin match3 given we have observed who won the first two matches.  We can start by assuming each player s strength comes from a standard\nnormal distribution. Then we assume the strength they pull with some\nnormal distribution centered around their true strength, and the\nperson who pulled harder wins.  def pulls(strength real):\n    normal(strength, 1)\n\ndef winner(a real, b real):\n    a_pull  ~ pulls(a)\n    b_pull  ~ pulls(b)\n    return (a_pull   b_pull)\n\nalice  ~ normal(0,1)\nbob    ~ normal(0,1)\ncarol  ~ normal(0,1)\n\nmatch1  ~ winner(alice, bob)\nmatch2  ~ winner(bob, carol)\nmatch3  ~ winner(alice, carol)  We then restrict the set of events to only those where Alice won the\nfirst match and Bob won the second, and return the results of the\nthird match.  if match1   match2:\n   return match3\nelse:\n   reject. measure(bool)  We can then run the above model using hakaru, which shows that Alice\nis likely to win her match against Carol.  hakaru tugofwar.hk | head -n 10000 | sort | uniq -c\n   3060 false\n   6940 true", 
            "title": "Tug of War"
        }, 
        {
            "location": "/intro/probprog/#simulation-vs-inference", 
            "text": "Of course, in the above program we performed inference, by taking\nour model and throwing out all events that didn t agree with\nthe data we had. How well would this work if we changed our\nmodel slightly? Suppose our data wasn t boolean values, but instead\nthe difference of strengths, and we want to not just whether Alice\nwill win, but by how much.  As we pose more complex questions, posing our models as rejection\nsamplers becomes increasing inefficient.", 
            "title": "Simulation vs Inference"
        }, 
        {
            "location": "/intro/installation/", 
            "text": "Installation\n\n\nInstall Hakaru by cloning the latest version from our Github repo\n\n\ngit clone https://github.com/hakaru-dev/hakaru\ncd hakaru\n\n\n\n\nHakaru can then be installed either with \ncabal install\n or \nstack install\n\n\nInstalling on Windows\n\n\nDue to a \nghc bug\n, until ghc 8.0\nis released Windows machines need to install the logfloat library separately\n\n\ncabal install -j logfloat -f -useffi\ncd hakaru\ncabal install\n\n\n\n\nMaple extension\n\n\nWithin Hakaru, we use \nMaple\n to perform\ncomputer-algebra guided optimizations. To get access to these optimizations\nyou must have a licensed copy of Maple installed.\n\n\nIn addition to this, we must autoload some Maple libraries that come\nwith the system to access this functionality\n\n\nexport LOCAL_MAPLE=\n`which maple`\n\ncd hakaru/maple\nmaple update-archive.mpl\necho 'libname := \n/path-to-hakaru/hakaru/maple\n,libname:' \n ~/.mapleinit\n\n\n\n\nUnder Windows the instructions become\n\n\nSETX LOCAL_MAPLE \npath to Maple bin directory\n\\cmaple.exe\n\ncd hakaru\\maple \ncmaple update-archive.mpl\necho 'libname := \nC:\\\\\npath to hakaru\n\\\\hakaru\\\\maple\n,libname:' \n \nC:\\\npath to maple\n\\lib\\maple.ini\n\n\n\n\n\nIf the Maple extension has been properly installed running\n\n\necho \nnormal(0,1)\n | simplify -\n\n\n\n\nshould return\n\n\nnormal(0, 1)\n\n\n\n\nIf the \nLOCAL_MAPLE\n environment variable is not set, then \nsimplify\n\ndefaults to invoking \nssh\n to access a remote installation of Maple.\nThe invocation is\n\n\n$MAPLE_SSH\n -l \n$MAPLE_USER\n \n$MAPLE_SERVER\n \n$MAPLE_COMMAND -q -t\n\n\n\n\n\nand defaults to\n\n\n/usr/bin/ssh -l ppaml karst.uits.iu.edu \nmaple -q -t", 
            "title": "Installation"
        }, 
        {
            "location": "/intro/installation/#installation", 
            "text": "Install Hakaru by cloning the latest version from our Github repo  git clone https://github.com/hakaru-dev/hakaru\ncd hakaru  Hakaru can then be installed either with  cabal install  or  stack install", 
            "title": "Installation"
        }, 
        {
            "location": "/intro/installation/#installing-on-windows", 
            "text": "Due to a  ghc bug , until ghc 8.0\nis released Windows machines need to install the logfloat library separately  cabal install -j logfloat -f -useffi\ncd hakaru\ncabal install", 
            "title": "Installing on Windows"
        }, 
        {
            "location": "/intro/installation/#maple-extension", 
            "text": "Within Hakaru, we use  Maple  to perform\ncomputer-algebra guided optimizations. To get access to these optimizations\nyou must have a licensed copy of Maple installed.  In addition to this, we must autoload some Maple libraries that come\nwith the system to access this functionality  export LOCAL_MAPLE= `which maple` \ncd hakaru/maple\nmaple update-archive.mpl\necho 'libname :=  /path-to-hakaru/hakaru/maple ,libname:'   ~/.mapleinit  Under Windows the instructions become  SETX LOCAL_MAPLE  path to Maple bin directory \\cmaple.exe \ncd hakaru\\maple \ncmaple update-archive.mpl\necho 'libname :=  C:\\\\ path to hakaru \\\\hakaru\\\\maple ,libname:'    C:\\ path to maple \\lib\\maple.ini   If the Maple extension has been properly installed running  echo  normal(0,1)  | simplify -  should return  normal(0, 1)  If the  LOCAL_MAPLE  environment variable is not set, then  simplify \ndefaults to invoking  ssh  to access a remote installation of Maple.\nThe invocation is  $MAPLE_SSH  -l  $MAPLE_USER   $MAPLE_SERVER   $MAPLE_COMMAND -q -t   and defaults to  /usr/bin/ssh -l ppaml karst.uits.iu.edu  maple -q -t", 
            "title": "Maple extension"
        }, 
        {
            "location": "/intro/quickstart/", 
            "text": "Quickstart\n\n\nAssuming you have Hakaru \ninstalled\n, let\ns\nsample a simple a model.\n\n\nx \n~ bern(0.5)\ny \n~ match x:\n      true:  normal(0,1)\n      false: uniform(0,1)\nreturn (y,x)\n\n\n\n\nThe generative model here has us flip a coin with bias 0.5, and then\nhave \nx\n be a draw from that distribution. We then check if \nx\n is\ntrue or false. Based on that we either have \ny\n be a draw from\na normal or uniform distribution, and then we return both \nx\n and \ny\n.\nBecause we are choosing between a normal and a uniform distribution,\nprograms like these are sometimes called \nmixture\n models.\n\n\nAssuming we save this file to \ntwomixture.hk\n we can sample from it by\npassing it as an argument to the \nhakaru\n command. \n\n\nhakaru twomixture.hk\n\n\n\n\nHakaru will then produce an infinite stream of samples from the\ndistribution this program represents.\n\n\n(0.8614855008328531, false)\n(0.27145378737815007, false)\n(6.137461559047042e-4, false)\n(0.9699201771404777, true)\n(1.2904529857533733, true)\n(8.605226081336681e-2, false)\n(-0.7713069511457459, true)\n(0.18162205213257607, true)\n(-1.143049106224509, true)\n(0.3667084406816875, false)\n...\n\n\n\n\nOf course, Hakaru wouldn\nt be very interesting if that was all it\ndid. Often what we wish to do is condition a distribution on\ndata. Suppose for \ntwomixture.hk\n we knew \ny\n, and would like to\nsample \nx\n conditioned on this information. We can symbolically\nproduce the unnormalized conditional distribution, which we call the\n\ndisintegration\n of the program.\n\n\ndisintegrate twomixture.hk\n\n\n\n\nThis returns\n\n\nfn x2 real: \n weight(0.5,\n        weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n                1.0 * \n                recip(natroot((2.0 * pi), 2))),\n               x = true\n               return x)) \n|\n \n weight(0.5,\n        match (not((x2 \n 0.0)) \n not((1.0 \n x2))): \n         true: \n          x = false\n          return x\n         false: reject. measure(bool))\n\n\n\n\nDisintegrate returns a function, to make it easier to sample\nfrom, we\nll give a value for x2. We\nll call this file\n\ntwomixture2.hk\n\n\nx2 = 0.3\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x)) \n|\n \nweight(0.5,\n    match (not((x2 \n 0.0)) \n not((1.0 \n x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))\n\n\n\n\nWhich we can run through some unix commands to get a sense of\nthe distribution\n\n\nhakaru twomixture2.hk | head -n 1000 | sort | uniq -c\n\n    526 false\n    474 true\n\n\n\n\nAs we can see, when x2 = 0.3, the uniform distribution is slightly more\nlikely. If we change x2 to be 3.0\n\n\nx2 = 3.0\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x)) \n|\n \nweight(0.5,\n    match (not((x2 \n 0.0)) \n not((1.0 \n x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))\n\n\n\n\nThis reflects that only the normal case is possible.\n\n\nhakaru twomixture3.hk | head -n 1000 | sort | uniq -c\n\n    1000 true", 
            "title": "Quick Example"
        }, 
        {
            "location": "/intro/quickstart/#quickstart", 
            "text": "Assuming you have Hakaru  installed , let s\nsample a simple a model.  x  ~ bern(0.5)\ny  ~ match x:\n      true:  normal(0,1)\n      false: uniform(0,1)\nreturn (y,x)  The generative model here has us flip a coin with bias 0.5, and then\nhave  x  be a draw from that distribution. We then check if  x  is\ntrue or false. Based on that we either have  y  be a draw from\na normal or uniform distribution, and then we return both  x  and  y .\nBecause we are choosing between a normal and a uniform distribution,\nprograms like these are sometimes called  mixture  models.  Assuming we save this file to  twomixture.hk  we can sample from it by\npassing it as an argument to the  hakaru  command.   hakaru twomixture.hk  Hakaru will then produce an infinite stream of samples from the\ndistribution this program represents.  (0.8614855008328531, false)\n(0.27145378737815007, false)\n(6.137461559047042e-4, false)\n(0.9699201771404777, true)\n(1.2904529857533733, true)\n(8.605226081336681e-2, false)\n(-0.7713069511457459, true)\n(0.18162205213257607, true)\n(-1.143049106224509, true)\n(0.3667084406816875, false)\n...  Of course, Hakaru wouldn t be very interesting if that was all it\ndid. Often what we wish to do is condition a distribution on\ndata. Suppose for  twomixture.hk  we knew  y , and would like to\nsample  x  conditioned on this information. We can symbolically\nproduce the unnormalized conditional distribution, which we call the disintegration  of the program.  disintegrate twomixture.hk  This returns  fn x2 real: \n weight(0.5,\n        weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n                1.0 * \n                recip(natroot((2.0 * pi), 2))),\n               x = true\n               return x))  |  \n weight(0.5,\n        match (not((x2   0.0))   not((1.0   x2))): \n         true: \n          x = false\n          return x\n         false: reject. measure(bool))  Disintegrate returns a function, to make it easier to sample\nfrom, we ll give a value for x2. We ll call this file twomixture2.hk  x2 = 0.3\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x))  |  \nweight(0.5,\n    match (not((x2   0.0))   not((1.0   x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))  Which we can run through some unix commands to get a sense of\nthe distribution  hakaru twomixture2.hk | head -n 1000 | sort | uniq -c\n\n    526 false\n    474 true  As we can see, when x2 = 0.3, the uniform distribution is slightly more\nlikely. If we change x2 to be 3.0  x2 = 3.0\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x))  |  \nweight(0.5,\n    match (not((x2   0.0))   not((1.0   x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))  This reflects that only the normal case is possible.  hakaru twomixture3.hk | head -n 1000 | sort | uniq -c\n\n    1000 true", 
            "title": "Quickstart"
        }, 
        {
            "location": "/lang/rand/", 
            "text": "Primitive Probability Distributions\n\n\nHakaru comes with a small set of primitive probability\ndistributions.\n\n\n\n\n\n\n\n\nnormal(mean. \nreal\n, standard_deviation. \nprob\n): \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nunivariate Normal (Gaussian) distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuniform(low. \nreal\n, high. \nreal\n): \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nUniform distribution is a continuous univariate distribution defined from low to high\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngamma(shape. \nprob\n, scale. \nprob\n): \nmeasure(prob)\n \n\n\n\n\n\n\n\n\n\n\nGamma distribution with shape and scale parameterization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbeta(a. \nprob\n, b. \nprob\n): \nmeasure(prob)\n \n\n\n\n\n\n\n\n\n\n\nBeta distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npoisson(l. \nprob\n): \nmeasure(nat)\n \n\n\n\n\n\n\n\n\n\n\nPoisson distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategorical(v. \narray(prob)\n): \nmeasure(nat)\n \n\n\n\n\n\n\n\n\n\n\nCategorical distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndirac(x. \na\n): \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\nDirac distribution\n\n\n\n\n\n\n\n\nThe Dirac distribution appears often enough, that we have given an\nadditional keyword in our language for it: \nreturn\n. The following\nprograms are equivalent.\n\n\ndirac(3)\n\n\n\n\nreturn 3\n\n\n\n\n\n\n\n\n\n\nlebesgue: \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nthe distribution constant over the real line\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweight(x. \nprob\n, m. \nmeasure(a)\n): \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\na \nm\n distribution, reweighted by \nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreject: \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\nThe distribution over the empty set\n\n\n\n\n\n\n\n\nFinally, we have a binary choice operator \n|\n, which takes two\ndistributions, and returns an unnormalized distribution which returns\none or the other.  For example, to get a distribution which where with\nprobability 0.5 draws from a uniform(0,1), and probability 0.5 draws\nfrom uniform(5,6).\n\n\nweight(0.5, uniform(0,1)) \n|\n\nweight(0.5, uniform(5,6))", 
            "title": "Random Primitives"
        }, 
        {
            "location": "/lang/rand/#primitive-probability-distributions", 
            "text": "Hakaru comes with a small set of primitive probability\ndistributions.", 
            "title": "Primitive Probability Distributions"
        }, 
        {
            "location": "/lang/letbind/", 
            "text": "Let and Bind\n\n\nIn Hakaru, we can give names for expressions to our programs with \n=\n,\nwhich we call \nLet\n. This gives us the ability to share computation\nthat might be needed in the program.\n\n\nx = 2\nx + 3\n\n\n\n\nWe can use \n=\n to give a name to any expression in our language. The\nname you assign is in scope for the rest of the body it was defined in.\n\n\nBind\n\n\nHakaru also has the operator \n~\n. This operator, which call \nBind\n\ncan only be used with expressions that denote probability distributions.\nBind allows us to talk about draws from a distribution using a name for\nany particular value that could have come from that distribution.\n\n\n# Bad\nx \n~ 2 + 3\nx\n\n\n\n\n# Good\nx \n~ normal(0,1)\nreturn x\n\n\n\n\nBecause Bind is about draws from a distribution, the rest of the body\nmust also denote a probability distribution.\n\n\n# Bad\nx \n~ normal(0,1)\nx\n\n\n\n\n# Good\nx \n~ normal(0,1)\nreturn x\n\n\n\n\nTo help distinguish Let and Bind. Here is a probabilistic program, where we\nlet \nf\n be equal to the normal distribution, and take draws from \nf\n.\n\n\nf = normal(0,1)\nx \n~ f\nreturn x*x", 
            "title": "Let and Bind"
        }, 
        {
            "location": "/lang/letbind/#let-and-bind", 
            "text": "In Hakaru, we can give names for expressions to our programs with  = ,\nwhich we call  Let . This gives us the ability to share computation\nthat might be needed in the program.  x = 2\nx + 3  We can use  =  to give a name to any expression in our language. The\nname you assign is in scope for the rest of the body it was defined in.", 
            "title": "Let and Bind"
        }, 
        {
            "location": "/lang/letbind/#bind", 
            "text": "Hakaru also has the operator  ~ . This operator, which call  Bind \ncan only be used with expressions that denote probability distributions.\nBind allows us to talk about draws from a distribution using a name for\nany particular value that could have come from that distribution.  # Bad\nx  ~ 2 + 3\nx  # Good\nx  ~ normal(0,1)\nreturn x  Because Bind is about draws from a distribution, the rest of the body\nmust also denote a probability distribution.  # Bad\nx  ~ normal(0,1)\nx  # Good\nx  ~ normal(0,1)\nreturn x  To help distinguish Let and Bind. Here is a probabilistic program, where we\nlet  f  be equal to the normal distribution, and take draws from  f .  f = normal(0,1)\nx  ~ f\nreturn x*x", 
            "title": "Bind"
        }, 
        {
            "location": "/lang/cond/", 
            "text": "Conditionals\n\n\nHakaru supports an \nif\n expression. This if must have two\nbodies. There exists no special syntax for \nelse if\n like\nyou might find in Python.\n\n\na  = 4\nb  = 5\nif a \n b:\n   a + 1\nelse:\n   b - 2", 
            "title": "Conditionals"
        }, 
        {
            "location": "/lang/cond/#conditionals", 
            "text": "Hakaru supports an  if  expression. This if must have two\nbodies. There exists no special syntax for  else if  like\nyou might find in Python.  a  = 4\nb  = 5\nif a   b:\n   a + 1\nelse:\n   b - 2", 
            "title": "Conditionals"
        }, 
        {
            "location": "/lang/coercions/", 
            "text": "Types and Coercions\n\n\nHakaru is a simply-typed language which has\na few basic types and some more complicated\nones which can be built out of simpler types.\n\n\nTypes\n\n\n\n\nnat is the type for natural numbers. This includes zero.\n\n\nint is the integer type.\n\n\nprob is the type for positive real number. This includes zero.\n\n\nreal is the type for real numbers.\n\n\narray(x) is the type for arrays where each element is type x\n\n\nmeasure(x) is the type for probability distributions whose\n  sample space is type x\n\n\n\n\nCoercions\n\n\nFor the primitive numeric types we also offer coercion functions.\n\n\n\n\nprob2real\n\n\nint2real\n\n\nnat2int\n\n\nreal2prob\n\n\nreal2int\n\n\nint2nat\n\n\n\n\nFor the ones which are always safe to apply such as \nnat2int\n we will\nautomatically insert them if it is required for the program to typecheck.", 
            "title": "Coercions"
        }, 
        {
            "location": "/lang/coercions/#types-and-coercions", 
            "text": "Hakaru is a simply-typed language which has\na few basic types and some more complicated\nones which can be built out of simpler types.", 
            "title": "Types and Coercions"
        }, 
        {
            "location": "/lang/coercions/#types", 
            "text": "nat is the type for natural numbers. This includes zero.  int is the integer type.  prob is the type for positive real number. This includes zero.  real is the type for real numbers.  array(x) is the type for arrays where each element is type x  measure(x) is the type for probability distributions whose\n  sample space is type x", 
            "title": "Types"
        }, 
        {
            "location": "/lang/coercions/#coercions", 
            "text": "For the primitive numeric types we also offer coercion functions.   prob2real  int2real  nat2int  real2prob  real2int  int2nat   For the ones which are always safe to apply such as  nat2int  we will\nautomatically insert them if it is required for the program to typecheck.", 
            "title": "Coercions"
        }, 
        {
            "location": "/lang/functions/", 
            "text": "Functions\n\n\nFunctions can be defined using a Python-inspired style syntax. One\nnotable difference is that each argument must be followed by its\ntype.\n\n\ndef add(x real, y real):\n    x + y\n\nadd(4,5)\n\n\n\n\nWe may optionally provide a type for the return value of a function if\nwe wish.\n\n\ndef add(x. real, y. real) real:\n    x + y\n\nadd(4,5)\n\n\n\n\nAnonymous functions\n\n\nIf you don\nt wish to name your functions, we also offer a syntax\nfor anonymous functions. These only take on argument and must be\ngiven a type alongside the variable name.\n\n\nfn x real: x + 1\n\n\n\n\nInternally, there are only one argument anonymous functions, and\nlets. The first example is equivalent to the following.\n\n\nadd = fn x real:\n         fn y real:\n            x + y\nadd(4,5)", 
            "title": "Functions and Let"
        }, 
        {
            "location": "/lang/functions/#functions", 
            "text": "Functions can be defined using a Python-inspired style syntax. One\nnotable difference is that each argument must be followed by its\ntype.  def add(x real, y real):\n    x + y\n\nadd(4,5)  We may optionally provide a type for the return value of a function if\nwe wish.  def add(x. real, y. real) real:\n    x + y\n\nadd(4,5)", 
            "title": "Functions"
        }, 
        {
            "location": "/lang/functions/#anonymous-functions", 
            "text": "If you don t wish to name your functions, we also offer a syntax\nfor anonymous functions. These only take on argument and must be\ngiven a type alongside the variable name.  fn x real: x + 1  Internally, there are only one argument anonymous functions, and\nlets. The first example is equivalent to the following.  add = fn x real:\n         fn y real:\n            x + y\nadd(4,5)", 
            "title": "Anonymous functions"
        }, 
        {
            "location": "/lang/datatypes/", 
            "text": "Data types and Match\n\n\nHakaru with several built-in data types.\n\n\n\n\npair\n\n\nunit\n\n\neither\n\n\nbool\n\n\n\n\nMatch\n\n\nWe use \nmatch\n to deconstruct out data types\nand access their elements.\n\n\nmatch left(3). either(int,bool):\n  left(x) : 1\n  right(x): 2\n\n\n\n\nWe do include special syntax for pairs\n\n\nmatch (1,2):\n  (x,y): x + y", 
            "title": "Datatypes and match"
        }, 
        {
            "location": "/lang/datatypes/#data-types-and-match", 
            "text": "Hakaru with several built-in data types.   pair  unit  either  bool", 
            "title": "Data types and Match"
        }, 
        {
            "location": "/lang/datatypes/#match", 
            "text": "We use  match  to deconstruct out data types\nand access their elements.  match left(3). either(int,bool):\n  left(x) : 1\n  right(x): 2  We do include special syntax for pairs  match (1,2):\n  (x,y): x + y", 
            "title": "Match"
        }, 
        {
            "location": "/lang/arrays/", 
            "text": "Arrays and Plate\n\n\nHakaru provides special syntax for arrays, which\nis distinct from the other data types.\n\n\nArrays\n\n\nTo construct arrays, we provide an index variable, size argument, and\nan expression body. This body is evaluated for each index of the\narray. For example, to construct the array \n[0,1,2,3]\n:\n\n\narray i of 4: i\n\n\n\n\nArray Literals\n\n\nWe can also create arrays using the literal syntax a comma delimited\nlist surrounded by brackets: \n[0,1,2,3]\n\n\nPlate\n\n\nBeyond, arrays Hakaru includes special syntax for describing measures\nover arrays called \nplate\n. Plate using the same syntax as \narray\n but\nthe body must have a measure type. It returns a measure over arrays.\nFor example, if we wish to have a distribution over three independent\nnormal distributions we would do so as follows:\n\n\nplate _ of 3: normal(0,1)\n\n\n\n\nArray size and indexing\n\n\nIf \na\n is an array, then \nsize(a)\n is its number of elements, which is a \nnat\n.\nIf \ni\n is a \nnat\n then \na[i]\n is the element of \na\n at index \ni\n.\nIndices start at zero, so the maximum valid value of \ni\n is \nsize(a)-1\n.\n\n\nLoops\n\n\nWe also express loops that compute sums (\nsummate\n) and products (\nproduct\n).\nThe syntax of these loops begins by declaring an \ninclusive\n lower bound and\nan \nexclusive\n upper bound.  For example, the factorial of \nn\n is not\n\nproduct i from 0 to n: i\n but rather \nproduct i from 0 to n+1: i\n.\nThis convention takes some getting used to but it makes it easy to deal\nwith arrays.  For example, if \na\n is an array of numbers then their sum is\n\nsummate i from 0 to size(a): a[i]\n.", 
            "title": "Arrays"
        }, 
        {
            "location": "/lang/arrays/#arrays-and-plate", 
            "text": "Hakaru provides special syntax for arrays, which\nis distinct from the other data types.", 
            "title": "Arrays and Plate"
        }, 
        {
            "location": "/lang/arrays/#arrays", 
            "text": "To construct arrays, we provide an index variable, size argument, and\nan expression body. This body is evaluated for each index of the\narray. For example, to construct the array  [0,1,2,3] :  array i of 4: i", 
            "title": "Arrays"
        }, 
        {
            "location": "/lang/arrays/#array-literals", 
            "text": "We can also create arrays using the literal syntax a comma delimited\nlist surrounded by brackets:  [0,1,2,3]", 
            "title": "Array Literals"
        }, 
        {
            "location": "/lang/arrays/#plate", 
            "text": "Beyond, arrays Hakaru includes special syntax for describing measures\nover arrays called  plate . Plate using the same syntax as  array  but\nthe body must have a measure type. It returns a measure over arrays.\nFor example, if we wish to have a distribution over three independent\nnormal distributions we would do so as follows:  plate _ of 3: normal(0,1)", 
            "title": "Plate"
        }, 
        {
            "location": "/lang/arrays/#array-size-and-indexing", 
            "text": "If  a  is an array, then  size(a)  is its number of elements, which is a  nat .\nIf  i  is a  nat  then  a[i]  is the element of  a  at index  i .\nIndices start at zero, so the maximum valid value of  i  is  size(a)-1 .", 
            "title": "Array size and indexing"
        }, 
        {
            "location": "/lang/arrays/#loops", 
            "text": "We also express loops that compute sums ( summate ) and products ( product ).\nThe syntax of these loops begins by declaring an  inclusive  lower bound and\nan  exclusive  upper bound.  For example, the factorial of  n  is not product i from 0 to n: i  but rather  product i from 0 to n+1: i .\nThis convention takes some getting used to but it makes it easy to deal\nwith arrays.  For example, if  a  is an array of numbers then their sum is summate i from 0 to size(a): a[i] .", 
            "title": "Loops"
        }, 
        {
            "location": "/transforms/expect/", 
            "text": "Expectation transformation\n\n\nThe expectation transformation takes a program representing a measure,\nand a function over the sample space, and returns a program computing\nthe expectation over that measure with respect to the given function.\n\n\nExpect\n\n\nExpect can be used inside programs with the \nexpect\n keyword.\n\n\nexpect x uniform(1,3):\n    real2prob(2*x + 1)\n\n\n\n\nThis program computes the expectation of \nuniform(1,3)\n using the\nfunction \n2*x + 1\n. This program expands to the following:\n\n\nintegrate x from 1 to 3: \n recip(real2prob(3 - 1)) * real2prob(2*x + 1)\n\n\n\n\nThis can be optimized by \nsimplify\n into \n5\n.\n\n\nNormalize\n\n\nWe also provide a \nnormalize\n command. This command takes as input a\nprogram representing any measure and reweights it into a program\nrepresenting a probability distribution.\n\n\nFor example in a slightly contrived example, we can weight a normal\ndistribution by two. Normalizing it will then remove this weight.\n\n\n echo \nweight(2, normal(0,1))\n | normalize | simplify -\nnormal(0, 1)", 
            "title": "Expect"
        }, 
        {
            "location": "/transforms/expect/#expectation-transformation", 
            "text": "The expectation transformation takes a program representing a measure,\nand a function over the sample space, and returns a program computing\nthe expectation over that measure with respect to the given function.", 
            "title": "Expectation transformation"
        }, 
        {
            "location": "/transforms/expect/#expect", 
            "text": "Expect can be used inside programs with the  expect  keyword.  expect x uniform(1,3):\n    real2prob(2*x + 1)  This program computes the expectation of  uniform(1,3)  using the\nfunction  2*x + 1 . This program expands to the following:  integrate x from 1 to 3: \n recip(real2prob(3 - 1)) * real2prob(2*x + 1)  This can be optimized by  simplify  into  5 .", 
            "title": "Expect"
        }, 
        {
            "location": "/transforms/expect/#normalize", 
            "text": "We also provide a  normalize  command. This command takes as input a\nprogram representing any measure and reweights it into a program\nrepresenting a probability distribution.  For example in a slightly contrived example, we can weight a normal\ndistribution by two. Normalizing it will then remove this weight.   echo  weight(2, normal(0,1))  | normalize | simplify -\nnormal(0, 1)", 
            "title": "Normalize"
        }, 
        {
            "location": "/transforms/disintegrate/", 
            "text": "Disintegrations transformation\n\n\nThe disintegration transformation takes as input a program\nrepresenting a joint probability distribution, and returns\na program which represents an posterior distribution.\n\n\nFor example, if we have the following joint distribution \nhello.hk\n\n\n\u03b8 \n~ normal(0,1)\nx \n~ normal(\u03b8,1)\nreturn (x,\u03b8)\n\n\n\n\nWhen we call \ndisintegrate hello.hk\n we obtain:\n\n\nfn x2 real: \n \u03b8 \n~ normal(0, 1)\n x7 \n~ weight((exp((negate(((x2 - \u03b8) ^ 2)) / 2))\n                / \n               1\n                / \n               sqrt((2 * pi))),\n              return ())\n return \u03b8\n\n\n\n\nThis represents the posterior on \n\u03b8\n given a value of \nx\n which\nhas been renamed \nx2\n.\n\n\nDensity\n\n\nFind the density of a probability distribution at a particular\npoint is actually a special-case of disintegrate and is\ndefined in terms of it.", 
            "title": "Disintegrate"
        }, 
        {
            "location": "/transforms/disintegrate/#disintegrations-transformation", 
            "text": "The disintegration transformation takes as input a program\nrepresenting a joint probability distribution, and returns\na program which represents an posterior distribution.  For example, if we have the following joint distribution  hello.hk  \u03b8  ~ normal(0,1)\nx  ~ normal(\u03b8,1)\nreturn (x,\u03b8)  When we call  disintegrate hello.hk  we obtain:  fn x2 real: \n \u03b8  ~ normal(0, 1)\n x7  ~ weight((exp((negate(((x2 - \u03b8) ^ 2)) / 2))\n                / \n               1\n                / \n               sqrt((2 * pi))),\n              return ())\n return \u03b8  This represents the posterior on  \u03b8  given a value of  x  which\nhas been renamed  x2 .", 
            "title": "Disintegrations transformation"
        }, 
        {
            "location": "/transforms/disintegrate/#density", 
            "text": "Find the density of a probability distribution at a particular\npoint is actually a special-case of disintegrate and is\ndefined in terms of it.", 
            "title": "Density"
        }, 
        {
            "location": "/transforms/simplify/", 
            "text": "Simplify transformation\n\n\nThe simplify transformation provides a way to automaticaly improve our\nprograms. Simplify works by turning our programs into their expectation\nrepresentation and sending to Maple to be algebraically-simplified.\n\n\nFor example, the following represents a program from values of type\n\nprob\n to a measure of real numbers.\n\n\nfn a prob:\n  x \n~ normal(a,1)\n  y \n~ normal(x,1)\n  z \n~ normal(y,1)\n  return z\n\n\n\n\nAnd it will simplify to the following equivalent program.\n\n\nfn a prob: normal(prob2real(a), sqrt(3))", 
            "title": "Simplify"
        }, 
        {
            "location": "/transforms/simplify/#simplify-transformation", 
            "text": "The simplify transformation provides a way to automaticaly improve our\nprograms. Simplify works by turning our programs into their expectation\nrepresentation and sending to Maple to be algebraically-simplified.  For example, the following represents a program from values of type prob  to a measure of real numbers.  fn a prob:\n  x  ~ normal(a,1)\n  y  ~ normal(x,1)\n  z  ~ normal(y,1)\n  return z  And it will simplify to the following equivalent program.  fn a prob: normal(prob2real(a), sqrt(3))", 
            "title": "Simplify transformation"
        }, 
        {
            "location": "/internals/ast/", 
            "text": "Internal Representation of Hakaru terms\n\n\nDatakind\n\n\nTerm\n\n\nSCons and SArgs\n\n\nPrimOp\n\n\nMeasureOp\n\n\nArrayOp", 
            "title": "AST and Hakaru Datakind"
        }, 
        {
            "location": "/internals/ast/#internal-representation-of-hakaru-terms", 
            "text": "", 
            "title": "Internal Representation of Hakaru terms"
        }, 
        {
            "location": "/internals/ast/#datakind", 
            "text": "", 
            "title": "Datakind"
        }, 
        {
            "location": "/internals/ast/#term", 
            "text": "", 
            "title": "Term"
        }, 
        {
            "location": "/internals/ast/#scons-and-sargs", 
            "text": "", 
            "title": "SCons and SArgs"
        }, 
        {
            "location": "/internals/ast/#primop", 
            "text": "", 
            "title": "PrimOp"
        }, 
        {
            "location": "/internals/ast/#measureop", 
            "text": "", 
            "title": "MeasureOp"
        }, 
        {
            "location": "/internals/ast/#arrayop", 
            "text": "", 
            "title": "ArrayOp"
        }, 
        {
            "location": "/internals/abt/", 
            "text": "Abstract Binding Trees\n\n\nHakaru makes use of many program transformations in its codebase.\nBecause of this, a special mechanism is included for handing\nvariable bindings and substitutions. We abstract this into its\nown typeclass called \nABT\n.\n\n\nBelow is an excerpt of this typeclass\n\n\nclass ABT (syn :: ([k] -\n k -\n *) -\n k -\n *) (abt :: [k] -\n k -\n *) | abt -\n syn where\n    -- Smart constructors for building a 'View' and then injecting it into the @abt@.\n    syn  :: syn abt  a -\n abt '[] a\n    var  :: Variable a -\n abt '[] a\n    bind :: Variable a -\n abt xs b -\n abt (a ': xs) b\n    caseBind :: abt (x ': xs) a -\n (Variable x -\n abt xs a -\n r) -\n r\n    ...", 
            "title": "ABT"
        }, 
        {
            "location": "/internals/abt/#abstract-binding-trees", 
            "text": "Hakaru makes use of many program transformations in its codebase.\nBecause of this, a special mechanism is included for handing\nvariable bindings and substitutions. We abstract this into its\nown typeclass called  ABT .  Below is an excerpt of this typeclass  class ABT (syn :: ([k] -  k -  *) -  k -  *) (abt :: [k] -  k -  *) | abt -  syn where\n    -- Smart constructors for building a 'View' and then injecting it into the @abt@.\n    syn  :: syn abt  a -  abt '[] a\n    var  :: Variable a -  abt '[] a\n    bind :: Variable a -  abt xs b -  abt (a ': xs) b\n    caseBind :: abt (x ': xs) a -  (Variable x -  abt xs a -  r) -  r\n    ...", 
            "title": "Abstract Binding Trees"
        }, 
        {
            "location": "/internals/datums/", 
            "text": "", 
            "title": "Datums"
        }, 
        {
            "location": "/internals/coercions/", 
            "text": "", 
            "title": "Coercions"
        }, 
        {
            "location": "/internals/transforms/", 
            "text": "", 
            "title": "Transformaitons"
        }, 
        {
            "location": "/internals/testing/", 
            "text": "", 
            "title": "Testing"
        }, 
        {
            "location": "/internals/newfeature/", 
            "text": "Adding a feature to the Hakaru language\n\n\nTo add a feature to the Hakaru language you must\n\n\n\n\nAdd an entry to the AST\n\n\nUpdate symbol resolution and optionally the parser to recognize this construct\n\n\nUpdate the pretty printers if this is something exposed to users\n\n\nUpdate the typechecker to handle it\n\n\nUpdate all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it\n\n\nUpdate the sampler if this primitive is intended to exist at runtime\n\n\nUpdate the compilers to emit the right code for this symbol\n\n\n\n\nWe give an example of what this looks like by adding \ndouble\n to the language.", 
            "title": "Adding a Language Feautre"
        }, 
        {
            "location": "/internals/newfeature/#adding-a-feature-to-the-hakaru-language", 
            "text": "To add a feature to the Hakaru language you must   Add an entry to the AST  Update symbol resolution and optionally the parser to recognize this construct  Update the pretty printers if this is something exposed to users  Update the typechecker to handle it  Update all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it  Update the sampler if this primitive is intended to exist at runtime  Update the compilers to emit the right code for this symbol   We give an example of what this looks like by adding  double  to the language.", 
            "title": "Adding a feature to the Hakaru language"
        }, 
        {
            "location": "/examples/", 
            "text": "Examples\n\n\nGaussian Mixture Model\n\n\nBelow is a model for a Gaussian Mixture model. This can be seen\nas a Bayesian version of K-means clustering.\n\n\n# Prelude to define dirichlet\ndef add(a prob, b prob):\n    a + b\n\ndef sum(a array(prob)):\n    reduce(add, 0, a)\n\ndef normalize(x array(prob)):\n    total = sum(x)\n    array i of size(x):\n       x[i] / total\n\ndef dirichlet(as array(prob)):\n    xs \n~ plate i of int2nat(size(as)-1):\n            beta(summate j from i+1 to size(as): as[j],\n                 as[i])\n    return array i of size(as):\n             x = product j from 0 to i: xs[j]\n             x * if i+1==size(as): 1 else: real2prob(1-xs[i])\n\n\n# num of clusters\nK = 5\n# num of points\nN = 20\n\n# prior probability of picking cluster K\npi  \n~ dirichlet(array _ of K: 1)\n# prior on mean and precision\nmu  \n~ plate _ of K:\n         normal(0, 5e-9)\ntau \n~ plate _ of K:\n         gamma(2, 0.05)\n# observed data\nx   \n~ plate _ of N:\n         i \n~ categorical(pi)\n         normal(mu[i], tau[i])\n\nreturn (x, mu). pair(array(real), array(real))\n\n\n\n\nLatent Dirichlet Allocation\n\n\nBelow is the LDA topic model.\n\n\nK = 2 # number of topics\nM = 3 # number of docs\nV = 7 # size of vocabulary\n\n# number of words in each document\ndoc = [4, 5, 3]\n\ntopic_prior = array _ of K: 1.0\nword_prior  = array _ of V: 1.0\n\nphi \n~ plate _ of K:     # word dist for topic k\n         dirichlet(word_prior)\n\n# likelihood\nz   \n~ plate m of M:\n         theta \n~ dirichlet(topic_prior)\n         plate _ of doc[m]: # topic marker for word n in doc m\n           categorical(theta)\n\nw   \n~ plate m of M: # for doc m\n         plate n of doc[m]: # for word n in doc m\n           categorical(phi[z[m][n]])\n\nreturn (w, z)", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#gaussian-mixture-model", 
            "text": "Below is a model for a Gaussian Mixture model. This can be seen\nas a Bayesian version of K-means clustering.  # Prelude to define dirichlet\ndef add(a prob, b prob):\n    a + b\n\ndef sum(a array(prob)):\n    reduce(add, 0, a)\n\ndef normalize(x array(prob)):\n    total = sum(x)\n    array i of size(x):\n       x[i] / total\n\ndef dirichlet(as array(prob)):\n    xs  ~ plate i of int2nat(size(as)-1):\n            beta(summate j from i+1 to size(as): as[j],\n                 as[i])\n    return array i of size(as):\n             x = product j from 0 to i: xs[j]\n             x * if i+1==size(as): 1 else: real2prob(1-xs[i])\n\n\n# num of clusters\nK = 5\n# num of points\nN = 20\n\n# prior probability of picking cluster K\npi   ~ dirichlet(array _ of K: 1)\n# prior on mean and precision\nmu   ~ plate _ of K:\n         normal(0, 5e-9)\ntau  ~ plate _ of K:\n         gamma(2, 0.05)\n# observed data\nx    ~ plate _ of N:\n         i  ~ categorical(pi)\n         normal(mu[i], tau[i])\n\nreturn (x, mu). pair(array(real), array(real))", 
            "title": "Gaussian Mixture Model"
        }, 
        {
            "location": "/examples/#latent-dirichlet-allocation", 
            "text": "Below is the LDA topic model.  K = 2 # number of topics\nM = 3 # number of docs\nV = 7 # size of vocabulary\n\n# number of words in each document\ndoc = [4, 5, 3]\n\ntopic_prior = array _ of K: 1.0\nword_prior  = array _ of V: 1.0\n\nphi  ~ plate _ of K:     # word dist for topic k\n         dirichlet(word_prior)\n\n# likelihood\nz    ~ plate m of M:\n         theta  ~ dirichlet(topic_prior)\n         plate _ of doc[m]: # topic marker for word n in doc m\n           categorical(theta)\n\nw    ~ plate m of M: # for doc m\n         plate n of doc[m]: # for word n in doc m\n           categorical(phi[z[m][n]])\n\nreturn (w, z)", 
            "title": "Latent Dirichlet Allocation"
        }
    ]
}