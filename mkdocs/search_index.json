{
    "docs": [
        {
            "location": "/",
            "text": "Hakaru\n\n\n\n\n\nHakaru is a simply-typed probabilistic programming language, designed for easy specification of probabilistic models and inference algorithms. This type of language is useful\nfor the development of machine learning algorithms and stochastic modeling. Hakaru enables the design of modular probabilistic inference programs by providing:\n\n\n\n\nA language for representing probabilistic distributions, queries, and inferences\n\n\nMethods for transforming probabilistic information, such as conditional probability and probabilistic inference, using computer algebra\n\n\n\n\nThis documentation provides information for installing and using Hakaru. Sample programs are included to demonstrate some of Hakaru\u2019s functionality and Hakaru implementation\ndetails are included to help guide future Hakaru developments and extensions.\n\n\nWarning: This code is alpha and experimental.\n\n\nContact us at ppaml@indiana.edu if you have any questions or concerns.\n\n\nIntroduction\n\n\nThe Introduction presents probabilistic programming and illustrates how Hakaru can be used to solve and describe these types of problems, how to install Hakaru on your \nmachine, and some sample programs to get you started.\n\n\nWhat is Probabilistic Programming?\n\n\nProbabilistic programming systems allow us to write programs which describe probability distributions, and provide mechanisms to sample and condition the distributions \nthey represent on data. In this page, we give a sense of the sorts of problems Hakaru is great at solving, and how you would describe them in Hakaru.\n\n\nInstalling Hakaru\n\n\nYou can install Hakaru on Linux, OSX, and Windows and extend its functionality using MapleSoft\u2019s Maple. \n\n\nGenerating Samples from your Hakaru Program\n\n\nYou can use the \nhakaru\n command to generate samples from your probabilistic model.\n\n\nQuick Start: A Mixture Model Example\n\n\nThis page will introduce you to Hakaru\u2019s basic functionality by creating a program to sample and condition a mixture model of a coin toss.\n\n\nExamples\n\n\nTwo examples, a Gaussian Mixture Model and a Latent Dirichlet Allocation (LDA) topic model, highlight the types of problems that Hakaru is uniquely suited to help you solve.\n\n\nCompiling to Haskell\n\n\nA Hakaru program can be ported into Haskell which can then be converted into machine code for other applications.\n\n\nCompiling to C\n\n\nDepending on the scale, a Hakaru program might be resource-intensive to run. In these situations, you could port your Hakaru program to C using the \nhkc\n command to take\nadvantage of other tools such as OpenMP for parallel processing. \n\n\nLanguage Guide\n\n\nThe Language Guide presents an overview of Hakaru\u2019s language primitives and core functionality.\n\n\nPrimitive Probability Distributions\n\n\nCommon probability distributions, such as the normal distribution, are already encoded in Hakaru and are considered to be language primitives. This page provides usage\ninstructions for accessing the primitive distributions in your programs.\n\n\nLet and Bind\n\n\nLet (\n=\n) and Bind (\n<~\n) enable the use of variables in Hakaru programs, which is essential for extracting a value from a probability distribution.\n\n\nConditionals\n\n\nHakaru supports a restricted \nif\n expression for selections between two conditions.\n\n\nFunctions\n\n\nHakaru supports both named and anonymous function definitions.\n\n\nTypes and Coercions\n\n\nHakaru has basic types which can also be combined to make complex ones. To aid in the communication of information between Hakaru functions, coercions are defined to allow \nconversions between compatible types.\n\n\nData Types and Match\n\n\nHakaru supports some built-in data types from Haskell. The \nmatch\n function is used for deconstructing them to extract their elements and to reconstructing data back into\nthese data types.\n\n\nArrays and Plate\n\n\nHakaru has special syntax for arrays, which is considered distinct from the other supported data types. A specialized array, \nplate\n, is used for describing measures over\narrays.\n\n\nLoops\n\n\nHakaru loops are specialized to compute the summation or product of the elements in an array.\n\n\nExpect\n\n\nThe expectation feature (\nexpect\n) computes expectation of a measure with respect to a given function. \n\n\nTransformations\n\n\nHakaru includes some inference algorithms that you can use to transform your probabilistic models into other forms to extract desireable information. Its inference \nalgorithms are implemented predominantly as program transformations.\n\n\nNote:\n By default, Hakaru assigns a weight to each generated sample. Typically a weight of one is used, but it is possible for the weights to vary between samples. This \nmight result in differing results from the original and transformed programs when summarizing a program\u2019s output by counting them.\n\n\nNormalize\n\n\nThe normalization transformation (\nnormalize\n) reweights a program so that it represents a normal distribution.\n\n\nDisintegrate\n\n\nThe disintegration transformation (\ndisintegrate\n) produces a program representing the conditional distribution based on a joint probability distribution. This command\nis equivalent to model conditioning in probability theory. \n\n\nDensity\n\n\nThe density transformation (\ndensity\n) is used to create a conditional distribution model that is used to estimate the density of the distribution at a particular point.\n\n\nSimplify\n\n\nThe simplify transformation (\nsimplify\n) is used to improve Hakaru programs by simplifying probabilistic models using computer algebra. This transformation requires the\nuse of Maple.\n\n\nMetropolis Hastings\n\n\nThe Metropolis Hastings transform (\nmh\n) is used to convert a Hakaru program into a Metropolis Hastings transition kernel.\n\n\nInternals\n\n\nThe internals section of the manual provides some insight into how Hakaru is implemented and offers guidance into how the system can be extended.\n\n\n\n\nAST\n\n\nABT\n\n\nDatums\n\n\nCoercions\n\n\nTransformations\n\n\nTesting Hakaru modules\n\n\nAdding a Language Feature\n\n\n\n\nCiting Us\n\n\nWhen referring to Hakaru, please cite the following \nacademic paper\n:\n\n\nP. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic \nProgramming, pp. 62-79, 2016.\n\n\n@inproceedings{narayanan2016probabilistic,\n    title = {Probabilistic inference by program transformation in Hakaru (system description)},\n    author = {Narayanan, Praveen and Carette, Jacques and Romano, Wren and Shan, Chung{-}chieh and Zinkov, Robert},\n    booktitle = {International Symposium on Functional and Logic Programming - 13th International Symposium, {FLOPS} 2016, Kochi, Japan, March 4-6, 2016, Proceedings},\n    pages = {62--79},\n    year = {2016},\n    organization = {Springer},\n    url = {http://dx.doi.org/10.1007/978-3-319-29604-3_5},\n    doi = {10.1007/978-3-319-29604-3_5},\n}",
            "title": "Home"
        },
        {
            "location": "/#introduction",
            "text": "The Introduction presents probabilistic programming and illustrates how Hakaru can be used to solve and describe these types of problems, how to install Hakaru on your \nmachine, and some sample programs to get you started.",
            "title": "Introduction"
        },
        {
            "location": "/#what-is-probabilistic-programming",
            "text": "Probabilistic programming systems allow us to write programs which describe probability distributions, and provide mechanisms to sample and condition the distributions \nthey represent on data. In this page, we give a sense of the sorts of problems Hakaru is great at solving, and how you would describe them in Hakaru.",
            "title": "What is Probabilistic Programming?"
        },
        {
            "location": "/#installing-hakaru",
            "text": "You can install Hakaru on Linux, OSX, and Windows and extend its functionality using MapleSoft\u2019s Maple.",
            "title": "Installing Hakaru"
        },
        {
            "location": "/#generating-samples-from-your-hakaru-program",
            "text": "You can use the  hakaru  command to generate samples from your probabilistic model.",
            "title": "Generating Samples from your Hakaru Program"
        },
        {
            "location": "/#quick-start-a-mixture-model-example",
            "text": "This page will introduce you to Hakaru\u2019s basic functionality by creating a program to sample and condition a mixture model of a coin toss.",
            "title": "Quick Start: A Mixture Model Example"
        },
        {
            "location": "/#examples",
            "text": "Two examples, a Gaussian Mixture Model and a Latent Dirichlet Allocation (LDA) topic model, highlight the types of problems that Hakaru is uniquely suited to help you solve.",
            "title": "Examples"
        },
        {
            "location": "/#compiling-to-haskell",
            "text": "A Hakaru program can be ported into Haskell which can then be converted into machine code for other applications.",
            "title": "Compiling to Haskell"
        },
        {
            "location": "/#compiling-to-c",
            "text": "Depending on the scale, a Hakaru program might be resource-intensive to run. In these situations, you could port your Hakaru program to C using the  hkc  command to take\nadvantage of other tools such as OpenMP for parallel processing.",
            "title": "Compiling to C"
        },
        {
            "location": "/#language-guide",
            "text": "The Language Guide presents an overview of Hakaru\u2019s language primitives and core functionality.",
            "title": "Language Guide"
        },
        {
            "location": "/#primitive-probability-distributions",
            "text": "Common probability distributions, such as the normal distribution, are already encoded in Hakaru and are considered to be language primitives. This page provides usage\ninstructions for accessing the primitive distributions in your programs.",
            "title": "Primitive Probability Distributions"
        },
        {
            "location": "/#let-and-bind",
            "text": "Let ( = ) and Bind ( <~ ) enable the use of variables in Hakaru programs, which is essential for extracting a value from a probability distribution.",
            "title": "Let and Bind"
        },
        {
            "location": "/#conditionals",
            "text": "Hakaru supports a restricted  if  expression for selections between two conditions.",
            "title": "Conditionals"
        },
        {
            "location": "/#functions",
            "text": "Hakaru supports both named and anonymous function definitions.",
            "title": "Functions"
        },
        {
            "location": "/#types-and-coercions",
            "text": "Hakaru has basic types which can also be combined to make complex ones. To aid in the communication of information between Hakaru functions, coercions are defined to allow \nconversions between compatible types.",
            "title": "Types and Coercions"
        },
        {
            "location": "/#data-types-and-match",
            "text": "Hakaru supports some built-in data types from Haskell. The  match  function is used for deconstructing them to extract their elements and to reconstructing data back into\nthese data types.",
            "title": "Data Types and Match"
        },
        {
            "location": "/#arrays-and-plate",
            "text": "Hakaru has special syntax for arrays, which is considered distinct from the other supported data types. A specialized array,  plate , is used for describing measures over\narrays.",
            "title": "Arrays and Plate"
        },
        {
            "location": "/#loops",
            "text": "Hakaru loops are specialized to compute the summation or product of the elements in an array.",
            "title": "Loops"
        },
        {
            "location": "/#expect",
            "text": "The expectation feature ( expect ) computes expectation of a measure with respect to a given function.",
            "title": "Expect"
        },
        {
            "location": "/#transformations",
            "text": "Hakaru includes some inference algorithms that you can use to transform your probabilistic models into other forms to extract desireable information. Its inference \nalgorithms are implemented predominantly as program transformations.  Note:  By default, Hakaru assigns a weight to each generated sample. Typically a weight of one is used, but it is possible for the weights to vary between samples. This \nmight result in differing results from the original and transformed programs when summarizing a program\u2019s output by counting them.",
            "title": "Transformations"
        },
        {
            "location": "/#normalize",
            "text": "The normalization transformation ( normalize ) reweights a program so that it represents a normal distribution.",
            "title": "Normalize"
        },
        {
            "location": "/#disintegrate",
            "text": "The disintegration transformation ( disintegrate ) produces a program representing the conditional distribution based on a joint probability distribution. This command\nis equivalent to model conditioning in probability theory.",
            "title": "Disintegrate"
        },
        {
            "location": "/#density",
            "text": "The density transformation ( density ) is used to create a conditional distribution model that is used to estimate the density of the distribution at a particular point.",
            "title": "Density"
        },
        {
            "location": "/#simplify",
            "text": "The simplify transformation ( simplify ) is used to improve Hakaru programs by simplifying probabilistic models using computer algebra. This transformation requires the\nuse of Maple.",
            "title": "Simplify"
        },
        {
            "location": "/#metropolis-hastings",
            "text": "The Metropolis Hastings transform ( mh ) is used to convert a Hakaru program into a Metropolis Hastings transition kernel.",
            "title": "Metropolis Hastings"
        },
        {
            "location": "/#internals",
            "text": "The internals section of the manual provides some insight into how Hakaru is implemented and offers guidance into how the system can be extended.   AST  ABT  Datums  Coercions  Transformations  Testing Hakaru modules  Adding a Language Feature",
            "title": "Internals"
        },
        {
            "location": "/#citing-us",
            "text": "When referring to Hakaru, please cite the following  academic paper :  P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic \nProgramming, pp. 62-79, 2016.  @inproceedings{narayanan2016probabilistic,\n    title = {Probabilistic inference by program transformation in Hakaru (system description)},\n    author = {Narayanan, Praveen and Carette, Jacques and Romano, Wren and Shan, Chung{-}chieh and Zinkov, Robert},\n    booktitle = {International Symposium on Functional and Logic Programming - 13th International Symposium, {FLOPS} 2016, Kochi, Japan, March 4-6, 2016, Proceedings},\n    pages = {62--79},\n    year = {2016},\n    organization = {Springer},\n    url = {http://dx.doi.org/10.1007/978-3-319-29604-3_5},\n    doi = {10.1007/978-3-319-29604-3_5},\n}",
            "title": "Citing Us"
        },
        {
            "location": "/intro/probprog/",
            "text": "What is Probabilistic Programming?\n\n\nProbabilistic programming is a software-driven method for creating probabilistic models and then using them to make probabilistic inferences. It \nprovides a means for writing programs which describe probabilistic models such that they can be used to make probabilistic inferences. For example, the \nHakaru program \npoisson(5)\n represents the Poisson distribution with a rate of five. A Probabilistic Programming Language (PPL) is a computer language designed to \ndescribe probabilistic models and distributions such that probabilistic inferences can be made programmatically\n1\n. Hakaru is an example of a PPL. \n\n\nWhy do we need a programming language for describing probability distributions? Consider a machine learning problem. A typical workflow for this type of design is, when \npresented with a problem, to design an inference algorithm for a specific probabilistic distribution and query. The development of a distribution, query, and inference\nalgorithm can be a time consuming task, even for someone that is skilled in these areas. Automating this process via a PPL allows for a broader exploration of the design\nspace without the added effort that is required to using a traditional approach.\n\n\nProbabilistic Models\n\n\nThe world is intrinsically an uncertain place. When you try to predict what will happen in the world given some data you have collected, you are engaging in some\nsort of probabilistic modeling. A distribution or program that describes your data is called the \nmodel\n. In probabilistic modeling, the quantity you wish to predict is \ntreated as a parameter and known data is described as some noisy function of this parameter. This function is called the \nlikelihood\n of the parameter. \n\n\nFor example, you might want to estimate the average time it takes for a bus to arrive at a stop based on actual arrival times. In this situation, you determine that you\ncan represent the likelihood function using a Poisson distribution:\n\n\n\n\n x \\sim \\text{Poisson}(\\lambda) \n\n\n\n\nwhere \n x \n is the actual arrival time, and \n\\lambda\n is the quantity you are using to make the prediction. \n\n\nYou can also represent this likelihood function as a density function which returns how likely it is for \nx\n to be generated for a given choice of \n\\lambda\n:\n\n\n\n\n f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \n\n\n\n\nMethods of Probabilistic Reasoning\n\n\nThere are two main approaches to statistical reasoning: Frequentist and Bayesian. In Frequentist reasoning, the goal is to maximize the likelihood function. In the density\nmodel for our bus arrival times, this would mean finding a value for \n\\lambda\n that maximizes \nf\n. In Bayesian reasoning, an estimation is made using the given \nfunction parameters and a conditioned data set collected for the event. For our density model, we would design an estimation functions using our parameters and given it\nour set of bus arrival times to predict a value for \nf\n. You can use either approach to probabilistic reasoning in your Hakaru programs. \n\n\nExample: Bayesian Tug-of-War\n\n\nTo demonstrate the value of this problem-solving approach, we will write a Hakaru program to represent a simplified version of the \n\ntug-of-war\n example from probmods.org. A completed version of this program can be found\nin the \nHakaru examples directory\n.\n\n\nThree friends, Alice, Bob and Carol, want to know which of them is the strongest. They decide that the winner of a game of tug-of-war must be stronger than their opponent,\nand arrange to take turns playing tug-of-war against each other. The person who wins the most matches will be deemed the strongest of them.\n\n\nYou can write a probabilistic program to represent this scenario. In this simulation, the friends are only interested in knowing who will win the third match (\nmatch3\n) \nwhen they already know who won the first two matches. You can represent this problem using either the frequentist or Bayesian reasoning methods, but this example will use \nthe Bayesian approach.\n\n\nYour program will begin with the description of the probabilistic model. You can assume that each friend\u2019s strength comes from a standard normal distribution and that the \nstrength that they pull with also follows some normal distribution centered around their true strength. You can also trivially assume that the friend that pulled the \nhardest will win the match. You can represent this model in Hakaru by writing:\n\n\ndef pulls(strength real):\n    normal(strength, 1)\n\ndef winner(a real, b real):\n    a_pull <~ pulls(a)\n    b_pull <~ pulls(b)\n    return (a_pull > b_pull)\n\nalice <~ normal(0,1)\nbob   <~ normal(0,1)\ncarol <~ normal(0,1)\n\nmatch1 <~ winner(alice, bob)\nmatch2 <~ winner(bob, carol)\nmatch3 <~ winner(alice, carol)\n\n\n\n\nNote:\n This Hakaru code will not compile yet.\n\n\nNow that you have created your model, you can condition your sample generation based on known data. In the third match, Alice is competing against Carol. She wants to know \nhow likely she is to win the match. She won her match against Bob (\nmatch1\n) and that Carol lost her match to Bob (\nmatch2\n). \n\n\nIn your model, the result of \nmatch1\n is \nTrue\n when Alice wins and the result of \nmatch2\n is \nTrue\n when Carol loses. You can use this knowledge to write a conditions for \nyour scenario that will return the result of \nmatch3\n when Alice wins \nmatch1\n and Carol loses \nmatch2\n. If a simulation is run that does not match this pattern, it is \nrejected. This restriction can be written in Hakaru as:\n\n\nif match1 && match2:\n   return match3\nelse:\n   reject. measure(bool)\n\n\n\n\nYou have now created a Hakaru program that describes a probabilistic model and restricted the accepted samples based on known data. You should save your program as \n\ntugofwar_rejection.hk\n so that you can run Hakaru to infer the outcome of \nmatch3\n. \n\n\nIf you call \nhakaru tugofwar_rejection.hk\n, you will get a continuous stream of Boolean results. You can make the calculations more legible by restricting the number of \nprogram executions and counting how many of each Boolean appears. For example, if you restrict the number of program executions to 10000 and collect the results, you will \nsee that \nTrue\n occurs much more frequently than \nFalse\n. This means that Alice is likely to win \nmatch3\n against Carol.\n\n\nhakaru -w tugofwar_rejection.hk | head -n 10000 | sort | uniq -c\n   3060 false\n   6940 true\n\n\n\n\nSimulation and Inference\n\n\nIdeally, you could collect a sufficient number of samples from your observed population to create your probabilistic model. This could be accomplished with populations that\nonly require a manageable number of samples or that are easy to collect. However, for many experiments this is not possible due to limited resources and time. In cases like\nthis, you could generate samples using a \nsimulation\n. In a simulation, you can select the population\u2019s mean and then generate values around this data point. If you wanted \nto know what a population would look like with a different mean, you simply need to change that value in your model and run the simulation again. \n\n\nWhat about the cases where you do have some samples and you want to know something about it? In this case, you use the data you have to guide the generation of samples in \norder to learn how the data occurred. This approach is called \ninference\n. To be able to make inferences from your known samples, you must add reasoning mechanisms to your \nmodel to gauge the usefulness of a model-generated sample with respect to some data that you have already collected. For example, you might have collected some disease data \nfrom a hospital and want to know how it spread in the affected patients. After creating a probabilistic model of disease transmission, you can use your collected data \nto reason about the samples generated from your model to judge its relevance in the creation of the data that you have collected.\n\n\nIn the tug-of-war example, you used Hakaru to restrict which samples were kept (Alice must have won \nmatch1\n and Bob must have won \nmatch2\n) and which ones were discarded. \nThis inference approach is called \nrejection sampling\n because restricted samples generated from your model are discarded. Would this approach still work if the model were \nchanged? Could we use this same technique to determine if Alice will win her match and by how much?\n\n\nAs you pose more complex questions, creating models as rejection samplers becomes increasingly inefficient because of the number of discarded samples. It would be\nbetter if your model could be transformed such that only observed data points are generated so that computational resources are not wasted on data that will not exist in \nyour data set.\n\n\nHakaru uses \nimportance sampling\n where, instead of being rejected immediately, each sample is assigned a weight so that a sample average can be calculated. As more \nsamples are generated, sample weights are updated to reflect the likelihood of that sample\u2019s rejection. While this works well for model inference when the model has\nonly a few dimensions, there are more powerful tools that can be used for more complex scenarios.\n\n\nThe Metropolis-Hastings Algorithm: A Markov Chain Monte Carlo Method\n\n\nYou might encounter situations where direct sampling from your model is difficult, which is common for multi-dimensional models. In models with high dimensionality, sample \npoints tend to cluster in regions so that when a \u201cgood\u201d sample is found, there is a higher chance of finding other good samples in the same area. This means that we want to \nstay in that region to collect more. In this situation, importance sampling becomes less efficient because it does not consider what other samples it has already found when\ngenerating a new one. Instead, a Markov Chain Monte Carlo (MCMC) method should be used. \n\n\nThe MCMC methods are used to sample probability distributions by constructing a Markov Chain. A Markov Chain is used to make predictions solely based on a process\u2019s current \nstate, so it does not require extensive memory for its calculations. In MCMC, a Markov chain is used to generate the next sample based on the current one, making it more \nlikely to stay in densely packed probability regions. As a model increases in dimensions, MCMC methods become essential for the generation of samples because the task of \nfinding high-value samples becomes more difficult.\n\n\nThe Metropolis-Hastings algorithm\n2\n is an MCMC method for generating a sequence of random samples from a probabilistic distribution. This is useful for approximating a \ndistribution that fits your existing data. The algorithm is included in Hakaru\u2019s transformations as the command tool \nmh\n. This transform converts\nyour probabilistic program into a Markov Chain which can be used for sample generation.\n\n\n\n\n\n\n\n\n\n\nProababilistic programming language (Wikipedia)\n\u00a0\n\u21a9\n\n\n\n\n\n\nD.J.C. MacKay, \u201cIntroduction to Monte Carlo Methods\u201d, Learning in Graphical Models, vol. 89, pp. 175-204, 1998.\u00a0\n\u21a9",
            "title": "What is Probabilistic Programming?"
        },
        {
            "location": "/intro/probprog/#what-is-probabilistic-programming",
            "text": "Probabilistic programming is a software-driven method for creating probabilistic models and then using them to make probabilistic inferences. It \nprovides a means for writing programs which describe probabilistic models such that they can be used to make probabilistic inferences. For example, the \nHakaru program  poisson(5)  represents the Poisson distribution with a rate of five. A Probabilistic Programming Language (PPL) is a computer language designed to \ndescribe probabilistic models and distributions such that probabilistic inferences can be made programmatically 1 . Hakaru is an example of a PPL.   Why do we need a programming language for describing probability distributions? Consider a machine learning problem. A typical workflow for this type of design is, when \npresented with a problem, to design an inference algorithm for a specific probabilistic distribution and query. The development of a distribution, query, and inference\nalgorithm can be a time consuming task, even for someone that is skilled in these areas. Automating this process via a PPL allows for a broader exploration of the design\nspace without the added effort that is required to using a traditional approach.",
            "title": "What is Probabilistic Programming?"
        },
        {
            "location": "/intro/probprog/#probabilistic-models",
            "text": "The world is intrinsically an uncertain place. When you try to predict what will happen in the world given some data you have collected, you are engaging in some\nsort of probabilistic modeling. A distribution or program that describes your data is called the  model . In probabilistic modeling, the quantity you wish to predict is \ntreated as a parameter and known data is described as some noisy function of this parameter. This function is called the  likelihood  of the parameter.   For example, you might want to estimate the average time it takes for a bus to arrive at a stop based on actual arrival times. In this situation, you determine that you\ncan represent the likelihood function using a Poisson distribution:    x \\sim \\text{Poisson}(\\lambda)    where   x   is the actual arrival time, and  \\lambda  is the quantity you are using to make the prediction.   You can also represent this likelihood function as a density function which returns how likely it is for  x  to be generated for a given choice of  \\lambda :    f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}",
            "title": "Probabilistic Models"
        },
        {
            "location": "/intro/probprog/#methods-of-probabilistic-reasoning",
            "text": "There are two main approaches to statistical reasoning: Frequentist and Bayesian. In Frequentist reasoning, the goal is to maximize the likelihood function. In the density\nmodel for our bus arrival times, this would mean finding a value for  \\lambda  that maximizes  f . In Bayesian reasoning, an estimation is made using the given \nfunction parameters and a conditioned data set collected for the event. For our density model, we would design an estimation functions using our parameters and given it\nour set of bus arrival times to predict a value for  f . You can use either approach to probabilistic reasoning in your Hakaru programs.",
            "title": "Methods of Probabilistic Reasoning"
        },
        {
            "location": "/intro/probprog/#example-bayesian-tug-of-war",
            "text": "To demonstrate the value of this problem-solving approach, we will write a Hakaru program to represent a simplified version of the  tug-of-war  example from probmods.org. A completed version of this program can be found\nin the  Hakaru examples directory .  Three friends, Alice, Bob and Carol, want to know which of them is the strongest. They decide that the winner of a game of tug-of-war must be stronger than their opponent,\nand arrange to take turns playing tug-of-war against each other. The person who wins the most matches will be deemed the strongest of them.  You can write a probabilistic program to represent this scenario. In this simulation, the friends are only interested in knowing who will win the third match ( match3 ) \nwhen they already know who won the first two matches. You can represent this problem using either the frequentist or Bayesian reasoning methods, but this example will use \nthe Bayesian approach.  Your program will begin with the description of the probabilistic model. You can assume that each friend\u2019s strength comes from a standard normal distribution and that the \nstrength that they pull with also follows some normal distribution centered around their true strength. You can also trivially assume that the friend that pulled the \nhardest will win the match. You can represent this model in Hakaru by writing:  def pulls(strength real):\n    normal(strength, 1)\n\ndef winner(a real, b real):\n    a_pull <~ pulls(a)\n    b_pull <~ pulls(b)\n    return (a_pull > b_pull)\n\nalice <~ normal(0,1)\nbob   <~ normal(0,1)\ncarol <~ normal(0,1)\n\nmatch1 <~ winner(alice, bob)\nmatch2 <~ winner(bob, carol)\nmatch3 <~ winner(alice, carol)  Note:  This Hakaru code will not compile yet.  Now that you have created your model, you can condition your sample generation based on known data. In the third match, Alice is competing against Carol. She wants to know \nhow likely she is to win the match. She won her match against Bob ( match1 ) and that Carol lost her match to Bob ( match2 ).   In your model, the result of  match1  is  True  when Alice wins and the result of  match2  is  True  when Carol loses. You can use this knowledge to write a conditions for \nyour scenario that will return the result of  match3  when Alice wins  match1  and Carol loses  match2 . If a simulation is run that does not match this pattern, it is \nrejected. This restriction can be written in Hakaru as:  if match1 && match2:\n   return match3\nelse:\n   reject. measure(bool)  You have now created a Hakaru program that describes a probabilistic model and restricted the accepted samples based on known data. You should save your program as  tugofwar_rejection.hk  so that you can run Hakaru to infer the outcome of  match3 .   If you call  hakaru tugofwar_rejection.hk , you will get a continuous stream of Boolean results. You can make the calculations more legible by restricting the number of \nprogram executions and counting how many of each Boolean appears. For example, if you restrict the number of program executions to 10000 and collect the results, you will \nsee that  True  occurs much more frequently than  False . This means that Alice is likely to win  match3  against Carol.  hakaru -w tugofwar_rejection.hk | head -n 10000 | sort | uniq -c\n   3060 false\n   6940 true",
            "title": "Example: Bayesian Tug-of-War"
        },
        {
            "location": "/intro/probprog/#simulation-and-inference",
            "text": "Ideally, you could collect a sufficient number of samples from your observed population to create your probabilistic model. This could be accomplished with populations that\nonly require a manageable number of samples or that are easy to collect. However, for many experiments this is not possible due to limited resources and time. In cases like\nthis, you could generate samples using a  simulation . In a simulation, you can select the population\u2019s mean and then generate values around this data point. If you wanted \nto know what a population would look like with a different mean, you simply need to change that value in your model and run the simulation again.   What about the cases where you do have some samples and you want to know something about it? In this case, you use the data you have to guide the generation of samples in \norder to learn how the data occurred. This approach is called  inference . To be able to make inferences from your known samples, you must add reasoning mechanisms to your \nmodel to gauge the usefulness of a model-generated sample with respect to some data that you have already collected. For example, you might have collected some disease data \nfrom a hospital and want to know how it spread in the affected patients. After creating a probabilistic model of disease transmission, you can use your collected data \nto reason about the samples generated from your model to judge its relevance in the creation of the data that you have collected.  In the tug-of-war example, you used Hakaru to restrict which samples were kept (Alice must have won  match1  and Bob must have won  match2 ) and which ones were discarded. \nThis inference approach is called  rejection sampling  because restricted samples generated from your model are discarded. Would this approach still work if the model were \nchanged? Could we use this same technique to determine if Alice will win her match and by how much?  As you pose more complex questions, creating models as rejection samplers becomes increasingly inefficient because of the number of discarded samples. It would be\nbetter if your model could be transformed such that only observed data points are generated so that computational resources are not wasted on data that will not exist in \nyour data set.  Hakaru uses  importance sampling  where, instead of being rejected immediately, each sample is assigned a weight so that a sample average can be calculated. As more \nsamples are generated, sample weights are updated to reflect the likelihood of that sample\u2019s rejection. While this works well for model inference when the model has\nonly a few dimensions, there are more powerful tools that can be used for more complex scenarios.",
            "title": "Simulation and Inference"
        },
        {
            "location": "/intro/probprog/#the-metropolis-hastings-algorithm-a-markov-chain-monte-carlo-method",
            "text": "You might encounter situations where direct sampling from your model is difficult, which is common for multi-dimensional models. In models with high dimensionality, sample \npoints tend to cluster in regions so that when a \u201cgood\u201d sample is found, there is a higher chance of finding other good samples in the same area. This means that we want to \nstay in that region to collect more. In this situation, importance sampling becomes less efficient because it does not consider what other samples it has already found when\ngenerating a new one. Instead, a Markov Chain Monte Carlo (MCMC) method should be used.   The MCMC methods are used to sample probability distributions by constructing a Markov Chain. A Markov Chain is used to make predictions solely based on a process\u2019s current \nstate, so it does not require extensive memory for its calculations. In MCMC, a Markov chain is used to generate the next sample based on the current one, making it more \nlikely to stay in densely packed probability regions. As a model increases in dimensions, MCMC methods become essential for the generation of samples because the task of \nfinding high-value samples becomes more difficult.  The Metropolis-Hastings algorithm 2  is an MCMC method for generating a sequence of random samples from a probabilistic distribution. This is useful for approximating a \ndistribution that fits your existing data. The algorithm is included in Hakaru\u2019s transformations as the command tool  mh . This transform converts\nyour probabilistic program into a Markov Chain which can be used for sample generation.      Proababilistic programming language (Wikipedia) \u00a0 \u21a9    D.J.C. MacKay, \u201cIntroduction to Monte Carlo Methods\u201d, Learning in Graphical Models, vol. 89, pp. 175-204, 1998.\u00a0 \u21a9",
            "title": "The Metropolis-Hastings Algorithm: A Markov Chain Monte Carlo Method"
        },
        {
            "location": "/intro/installation/",
            "text": "Installing Hakaru\n\n\nYou can download Hakaru by cloning the latest version from our GitHub repository:\n\n\ngit clone git@github.com:hakaru-dev/hakaru\n\n\n\n\nHakaru can be installed by using either \nstack install\n or \ncabal install\n inside the \nhakaru\n directory. One way that you can access these tools is by installing the \nHaskell Platform\n which supports Linux, OSX, and Windows operating systems.\n\n\nIf you are using \nstack install\n, you can install and verify your installation of Hakaru by running the commands:\n\n\nstack install\nstack test\n\n\n\n\nYou can find the output of \nstack test\n in the \n.stack-work/logs/hakaru-0.4.0-test.txt\n file.\n\n\nIf you are using \ncabal install\n, you can install Hakaru by running the commands:\n\n\ncabal update    \ncabal install -j --only-dependencies --enable-tests\ncabal configure --enable-tests\ncabal build\ncabal test\n\n\n\n\nOn Windows systems, you can use the \nstack install\n and \ncabal install\n commands by running them in a Linux shell such as \nCygwin\n or Git Bash.\n\n\nNote:\n If you want to use \ncabal install\n and have installed the Haskell Platform, you might need to add a reference to the directory containing \ncabal.exe\n to the \nPATH\n environment variable.\n\n\nIf you are using GHC 7.10 or earlier on a Windows system and want to use the \ncabal install\n command, you must install the \nlogfloat\n dependency manually after running \ncabal update\n due to a\n\nGHC bug\n:\n\n\ncabal update    \ncabal install -j logfloat -f -useffi\ncabal install -j --only-dependencies --enable-tests\ncabal configure --enable-tests\ncabal build\ncabal test\n\n\n\n\nExtending Hakaru with Maple\n\n\nHakaru uses \nMaple\n to perform computer-algebra guided optimizations. You must have a licensed copy of Maple installed to access this component of the Hakaru language.\n\n\nOn Linux systems, Hakaru can be setup to use Maple by running:\n\n\nexport LOCAL_MAPLE=\"`which maple`\"\ncd hakaru/maple\necho 'libname := \"/path-to-hakaru/hakaru/maple\",libname:' >> ~/.mapleinit\nmaple update-archive.mpl\n\n\n\n\nOn Windows systems, Hakaru can be setup to use Maple by performing the following steps in Administrator mode:\n\n\n\n\n\n\nCreate a User Environment Variable \nLOCAL_MAPLE\n using the Windows command prompt (cmd) by running:\n\n\nSETX LOCAL_MAPLE \"<path to Maple bin directory>\\cmaple.exe\"\n\n\nThis variable can also be created via the Advanced System Properties.\n\n\nNote:\n You might need to restart your computer for the variable to be recognized.\n\n\n\n\n\n\nAdd the path to \ncmaple.exe\n to your PATH system environment variable. This can be done via the Advanced System Properties.\n\n\nNote:\n You might need to restart your computer for the variable to be recognized.    \n\n\n\n\n\n\nIn the Windows command prompt (cmd), create a file \nmaple.ini\n by running:\n\n\necho libname := \"C:\\\\<path to hakaru>\\\\hakaru\\\\maple\",libname: >> \"C:\\<path to maple>\\lib\\maple.ini\"\n\n\n\n\n\n\nIn the Windows command prompt (cmd), Navigate to the \nhakaru\\maple\n directory and run:\n\n\ncmaple update-archive.mpl\n\n\n\n\n\n\nTesting Your Maple Installation with Hakaru\n\n\nIf you have correctly installaed Hakaru\u2019s Maple extension, running \necho \"normal(0,1)\" | simplify -\n in a \nbash\n command line will return \nnormal(0, 1)\n.\n\n\nIf you have not set the \nLOCAL_MAPLE\n environment variable, then \nsimplify\n command might try to locate a \nSSH\n file that might not exist on your machine to try and access a remote installation of Maple.",
            "title": "Installing Hakaru"
        },
        {
            "location": "/intro/installation/#installing-hakaru",
            "text": "You can download Hakaru by cloning the latest version from our GitHub repository:  git clone git@github.com:hakaru-dev/hakaru  Hakaru can be installed by using either  stack install  or  cabal install  inside the  hakaru  directory. One way that you can access these tools is by installing the  Haskell Platform  which supports Linux, OSX, and Windows operating systems.  If you are using  stack install , you can install and verify your installation of Hakaru by running the commands:  stack install\nstack test  You can find the output of  stack test  in the  .stack-work/logs/hakaru-0.4.0-test.txt  file.  If you are using  cabal install , you can install Hakaru by running the commands:  cabal update    \ncabal install -j --only-dependencies --enable-tests\ncabal configure --enable-tests\ncabal build\ncabal test  On Windows systems, you can use the  stack install  and  cabal install  commands by running them in a Linux shell such as  Cygwin  or Git Bash.  Note:  If you want to use  cabal install  and have installed the Haskell Platform, you might need to add a reference to the directory containing  cabal.exe  to the  PATH  environment variable.  If you are using GHC 7.10 or earlier on a Windows system and want to use the  cabal install  command, you must install the  logfloat  dependency manually after running  cabal update  due to a GHC bug :  cabal update    \ncabal install -j logfloat -f -useffi\ncabal install -j --only-dependencies --enable-tests\ncabal configure --enable-tests\ncabal build\ncabal test",
            "title": "Installing Hakaru"
        },
        {
            "location": "/intro/installation/#extending-hakaru-with-maple",
            "text": "Hakaru uses  Maple  to perform computer-algebra guided optimizations. You must have a licensed copy of Maple installed to access this component of the Hakaru language.  On Linux systems, Hakaru can be setup to use Maple by running:  export LOCAL_MAPLE=\"`which maple`\"\ncd hakaru/maple\necho 'libname := \"/path-to-hakaru/hakaru/maple\",libname:' >> ~/.mapleinit\nmaple update-archive.mpl  On Windows systems, Hakaru can be setup to use Maple by performing the following steps in Administrator mode:    Create a User Environment Variable  LOCAL_MAPLE  using the Windows command prompt (cmd) by running:  SETX LOCAL_MAPLE \"<path to Maple bin directory>\\cmaple.exe\"  This variable can also be created via the Advanced System Properties.  Note:  You might need to restart your computer for the variable to be recognized.    Add the path to  cmaple.exe  to your PATH system environment variable. This can be done via the Advanced System Properties.  Note:  You might need to restart your computer for the variable to be recognized.        In the Windows command prompt (cmd), create a file  maple.ini  by running:  echo libname := \"C:\\\\<path to hakaru>\\\\hakaru\\\\maple\",libname: >> \"C:\\<path to maple>\\lib\\maple.ini\"    In the Windows command prompt (cmd), Navigate to the  hakaru\\maple  directory and run:  cmaple update-archive.mpl",
            "title": "Extending Hakaru with Maple"
        },
        {
            "location": "/intro/installation/#testing-your-maple-installation-with-hakaru",
            "text": "If you have correctly installaed Hakaru\u2019s Maple extension, running  echo \"normal(0,1)\" | simplify -  in a  bash  command line will return  normal(0, 1) .  If you have not set the  LOCAL_MAPLE  environment variable, then  simplify  command might try to locate a  SSH  file that might not exist on your machine to try and access a remote installation of Maple.",
            "title": "Testing Your Maple Installation with Hakaru"
        },
        {
            "location": "/intro/samplegen/",
            "text": "Generating Samples from your Hakaru Program\n\n\nThe Hakaru language is built on Monte Carlo methods, which aim to generate individual samples and estimate expectation functions from a given distribution. The first task,\ndrawing samples from a distribution, is often difficult and this can be exhasperated as the dimensionality of the sample space increases. Importance sampling is a Monte\nCarlo method that is used to generate samples by estimating an expectation function for the target distribution instead. The estimated expectation function is then used to \ngenerate samples. To account for the knowledge that the samples were not generated from the target distribution, a weight is assigned so that each sample\u2019s contribution to\nthe estimator is adjusted according to its relevence. However, this method only works well if the distribution proposed by the expectation function is similar to the target\ndistribution. For more complex distributions, a different approach, such as the Metropolis Hastings method should be used\n1\n.\n\n\nThe \nhakaru\n command is used to indefinitely generate samples from a Hakaru program using importance sampling. Each sample is assigned a weight, and a sample\u2019s weight is \ninitialized to \n1.0\n. Weights are changed by Hakaru primitives and processes such as \nweight\n.\n\n\nUsage\n\n\nThe \nhakaru\n command can take up to two Hakaru programs as arguments. If only one program is provided, the \nhakaru\n command generates samples based on the model described in\nthe Hakaru program. In this case, the \nhakaru\n command can be invoked in the command-line by calling:\n\n\nhakaru hakaru_program.hk\n\n\n\n\nIf a second program is given to the \nhakaru\n command, it will treat the two programs as the start of a Markov Chain. This is used when you have created a transition kernel \nusing the \nMetropolis Hastings\n transformation. To invoke the \nhakaru\n command with a transition kernel, you would call:\n\n\nhakaru --transition-kernel transition.hk init.hk\n\n\n\n\nThe first program, \ntransition.hk\n,  is treated as the transition kernel and the second program, \ninit.hk\n, is treated as the initial state of the Markov Chain. When the \n\nhakaru\n command is run, a sample is drawn from \ninit.hk\n. This sample is then passed to \ntransition.hk\n to generate the second sample. After this point, samples generated\nfrom \ntransition.hk\n are passed back into itself to generate further samples.\n\n\nThe Dash (\n-\n) Operator\n\n\nYou might encounter some scenarios where you wish to run a Hakaru command or transformation on a program and then send the resulting output to another command or transform. \nIn these cases, you can take advantage of the dash (\n-\n) command-line notation.\n\n\nThe dash notation is a shortcut used to pass standard inputs and outputs to another command in the same line of script. For example, if you wanted to run the \ndisintegrate\n\nHakaru command followed by the \nsimplify\n command, you would enter:\n\n\ndisintegrate program.hk | simplify -\n\n\n\n\nThis command is equivalent to entering:\n\n\ndisintegrate program.hk > temp.hk\nsimplify temp.hk\n\n\n\n\nNote:\n The \n>\n operator redirects the output from \ndisintegrate program.hk\n to a new file called \ntemp.hk\n.\n\n\nExample\n\n\nThe \u201ctrick coin\u201d is a basic example that is used to introduce the probability or expectation of an outcome. Suppose we are given an unfair coin that follows the distribution:\n\n\nweight(3, return true) <|> weight(1, return false)\n\n\n\n\nIf you save this program as \nbiasedcoin.hk\n, you can generate samples from it by calling:\n\n\n$ hakaru biasedcoin.hk\n4.0     true\n4.0     true\n4.0     true\n4.0     true\n4.0     true\n4.0     true\n4.0     true\n4.0     false\n4.0     false\n...\n\n\n\n\nThe \nhakaru\n command will print a continuous stream of samples drawn from this program. In this example, all sample weights are \n4.0\n. If you wanted to see the ratio of \nweights for a series of coin tosses, you can use an \nawk\n script that tallies the weights for a limited set of samples:\n\n\n$ hakaru biasedcoin.hk | head -n 2000 | awk '{a[$2]+=$1}END{for(i in a) print i, a[i]}'\nfalse 1944\ntrue 6056\n\n\n\n\nIf you were only interested in counting how many times the coin tosses landed on each of HEAD and TAILS, modify the \nawk\n script to be a counter instead:\n\n\n$ hakaru biasedcoin.hk | head -n 2000 | awk '{a[$2]+=1}END{for(i in a) print i, a[i]}'\nfalse 524\ntrue 1476\n\n\n\n\nIn this case, the printing of sample weights might not be important. To suppress the printing of weights during sample generation, you can use the \n--no-weights\n or \n-w\n \noption:\n\n\n$ hakaru --no-weights biasedcoin.hk\nfalse\ntrue\ntrue\ntrue\nfalse\ntrue\ntrue\ntrue\n...\n\n\n\n\nAn example for using the \nhakaru\n command using a transition kernel is available on the \nMetropolis Hastings\n transform page.\n\n\n\n\n\n\n\n\n\n\nD.J.C. MacKay, \u201cIntroduction to Monte Carlo Methods\u201d, Learning in Graphical Models, vol. 89, pp. 175-204, 1998.\u00a0\n\u21a9",
            "title": "Generating Samples from your Hakaru Program"
        },
        {
            "location": "/intro/samplegen/#generating-samples-from-your-hakaru-program",
            "text": "The Hakaru language is built on Monte Carlo methods, which aim to generate individual samples and estimate expectation functions from a given distribution. The first task,\ndrawing samples from a distribution, is often difficult and this can be exhasperated as the dimensionality of the sample space increases. Importance sampling is a Monte\nCarlo method that is used to generate samples by estimating an expectation function for the target distribution instead. The estimated expectation function is then used to \ngenerate samples. To account for the knowledge that the samples were not generated from the target distribution, a weight is assigned so that each sample\u2019s contribution to\nthe estimator is adjusted according to its relevence. However, this method only works well if the distribution proposed by the expectation function is similar to the target\ndistribution. For more complex distributions, a different approach, such as the Metropolis Hastings method should be used 1 .  The  hakaru  command is used to indefinitely generate samples from a Hakaru program using importance sampling. Each sample is assigned a weight, and a sample\u2019s weight is \ninitialized to  1.0 . Weights are changed by Hakaru primitives and processes such as  weight .",
            "title": "Generating Samples from your Hakaru Program"
        },
        {
            "location": "/intro/samplegen/#usage",
            "text": "The  hakaru  command can take up to two Hakaru programs as arguments. If only one program is provided, the  hakaru  command generates samples based on the model described in\nthe Hakaru program. In this case, the  hakaru  command can be invoked in the command-line by calling:  hakaru hakaru_program.hk  If a second program is given to the  hakaru  command, it will treat the two programs as the start of a Markov Chain. This is used when you have created a transition kernel \nusing the  Metropolis Hastings  transformation. To invoke the  hakaru  command with a transition kernel, you would call:  hakaru --transition-kernel transition.hk init.hk  The first program,  transition.hk ,  is treated as the transition kernel and the second program,  init.hk , is treated as the initial state of the Markov Chain. When the  hakaru  command is run, a sample is drawn from  init.hk . This sample is then passed to  transition.hk  to generate the second sample. After this point, samples generated\nfrom  transition.hk  are passed back into itself to generate further samples.",
            "title": "Usage"
        },
        {
            "location": "/intro/samplegen/#the-dash-operator",
            "text": "You might encounter some scenarios where you wish to run a Hakaru command or transformation on a program and then send the resulting output to another command or transform. \nIn these cases, you can take advantage of the dash ( - ) command-line notation.  The dash notation is a shortcut used to pass standard inputs and outputs to another command in the same line of script. For example, if you wanted to run the  disintegrate \nHakaru command followed by the  simplify  command, you would enter:  disintegrate program.hk | simplify -  This command is equivalent to entering:  disintegrate program.hk > temp.hk\nsimplify temp.hk  Note:  The  >  operator redirects the output from  disintegrate program.hk  to a new file called  temp.hk .",
            "title": "The Dash (-) Operator"
        },
        {
            "location": "/intro/samplegen/#example",
            "text": "The \u201ctrick coin\u201d is a basic example that is used to introduce the probability or expectation of an outcome. Suppose we are given an unfair coin that follows the distribution:  weight(3, return true) <|> weight(1, return false)  If you save this program as  biasedcoin.hk , you can generate samples from it by calling:  $ hakaru biasedcoin.hk\n4.0     true\n4.0     true\n4.0     true\n4.0     true\n4.0     true\n4.0     true\n4.0     true\n4.0     false\n4.0     false\n...  The  hakaru  command will print a continuous stream of samples drawn from this program. In this example, all sample weights are  4.0 . If you wanted to see the ratio of \nweights for a series of coin tosses, you can use an  awk  script that tallies the weights for a limited set of samples:  $ hakaru biasedcoin.hk | head -n 2000 | awk '{a[$2]+=$1}END{for(i in a) print i, a[i]}'\nfalse 1944\ntrue 6056  If you were only interested in counting how many times the coin tosses landed on each of HEAD and TAILS, modify the  awk  script to be a counter instead:  $ hakaru biasedcoin.hk | head -n 2000 | awk '{a[$2]+=1}END{for(i in a) print i, a[i]}'\nfalse 524\ntrue 1476  In this case, the printing of sample weights might not be important. To suppress the printing of weights during sample generation, you can use the  --no-weights  or  -w  \noption:  $ hakaru --no-weights biasedcoin.hk\nfalse\ntrue\ntrue\ntrue\nfalse\ntrue\ntrue\ntrue\n...  An example for using the  hakaru  command using a transition kernel is available on the  Metropolis Hastings  transform page.      D.J.C. MacKay, \u201cIntroduction to Monte Carlo Methods\u201d, Learning in Graphical Models, vol. 89, pp. 175-204, 1998.\u00a0 \u21a9",
            "title": "Example"
        },
        {
            "location": "/intro/quickstart/",
            "text": "Quick Start: A Mixture Model Example\n\n\nLet\u2019s start with a simple model of a coin toss experiment so that you can become familiar with some of Hakaru\u2019s data types and functionality. We will assume that a single \ncoin flip can be represented using a Bernoulli distribution. After we have created the Bernoulli model, we will use it to create a mixture model and condition the model\nto estimate what the original coin toss experiment looked like based on the resulting mixture model samples.\n\n\nModeling a Bernoulli Experiment\n\n\nWe will use the \ncategorical\n Hakaru \nRandom Primitive\n to write a Bernoulli distribution\n1\n for our model. The \ncategorical\n primitive requires an \n\narray\n representing the probability of achieving each category in the experiement. Let\u2019s start with a fair experiment and state that each side of the \ncoin has an equal chance of being picked. The result of the coin toss is stored in the variable \nb\n using Hakaru\u2019s notation for \nbind\n:\n\n\nb <~ categorical([0.5, 0.5])\n\n\n\n\nFor data type simplicity, we will map Heads to \ntrue\n and Tails to \nfalse\n. By putting the values of \ntrue\n and \nfalse\n into an array, we can use the value in \nb\n to\nselect which of them to return as the result of the coin toss:\n\n\nreturn [true, false][b]\n\n\n\n\nA characteristic of the Bernoulli distribution is that it assumes that only one experiment is conducted. To collect samples, we need to run this experiment multiple times.\nTo aid in this task, we can rewrite the Bernoulli model as a \nfunction\n. We will call our function \nbern\n: \n\n\ndef bern ():\n    b <~ categorical([0.5, 0.5])\n    return [true, false][b]\n\n\n\n\nNow that we are using functions, we can generalize our model so that we can run experiments on both fair and trick coins. To do this, we should pass in a probability \np\n as \na function argument, which is then used to populate the \ncategorical\n primitive. Hakaru has a specialized data type for probabilities called \nprob\n, which we will use as the\ndata type for our function input:\n\n\ndef bern (p prob):\n    b <~ categorical([p, (1 - p)])\n    return [true, false][b]\n\n\n\n\nIf you we to run this function, we will get a \nType Mismatch\n error. This is because the value \n(1 - p)\n is converted to type \nreal\n as a result of the subtraction operation\nand \ncategorical\n expects all of the values in its array to be of type \nprob\n. One solution would be to manually pass in the value of \n(1 - p)\n as a function argument, which\nwould artificially complicate our function. Instead, we can use Hakaru\u2019s \ncoercions\n to recast \n(1 - p)\n to type \nprob\n:\n\n\ndef bern (p prob):\n    b <~ categorical([p, real2prob(1 - p)])\n    return [true, false][b]\n\n\n\n\nWe can now use our model to run a series of Bernoullli experiments. Let\u2019s set up our program to use a fair coin and save it as \nbernoulli.hk\n:\n\n\ndef bern (p prob):\n    b <~ categorical([p, real2prob(1 - p)])\n    return [true, false][b]\n\nbern(0.5)\n\n\n\n\nRunning this program using \nhakaru bernoulli.hk\n should result in an infinite stream of coin toss trials:\n\n\nfalse\ntrue\nfalse\ntrue\ntrue\ntrue\n...\n\n\n\n\nNow that we have set up our Bernoulli experiment, let\u2019s use it to create a mixture model.\n\n\nCreating a Mixture Model\n\n\nLet\u2019s use our coin flip experiment to create a mixture model by drawing a sample from a normal distribution when the coin is Heads (\ntrue\n) and from a uniform distribution\nwhen it is Tails (\nfalse\n). This is called a \nmixture model\n2\n because we are selecting samples from different distributions. Let\u2019s start by saving a copy of your \nBernoulli function into a new program so that we can use it in our new model. For this example, we will call it \ntwomixture.hk\n.\n\n\nLet\u2019s start by binding the return value of our \nbern\n function to a variable called \ncoin\n to represent the outcome of an experiment:\n\n\ncoin <~ bern(0.5)\n\n\n\n\nNow that we have stored the result of our experiment, let\u2019s use it to generate a sample. Our model has a selection condition where Heads causes a sample to be drawn from \nachieving normal distribution and Tails draws from a uniform distribution. There are two ways of handling this in Hakaru \u2013 \nconditionals\n and \n\npattern matching\n. Since we are working with Booleans, let\u2019s use patern matching so that we can see what it looks like in Hakaru.\n\n\nHakaru pattern matching requires a sequence and a set of possible patterns to compare the sequence to. In our model, our sequence would be \ncoin\n because that is what we are\nusing to select a distribution. Our possible patterns are the possible values that \ncoin\n could have \u2013 \ntrue\n and \nfalse\n. When the pattern is \ntrue\n, we call the normal\ndistribution and when it is \nfalse\n we call the uniform distribution. Both the \nnormal\n and \nuniform\n functions are included in Hakaru\u2019s \nRandom Primitives\n,\nso we do not need to define our own functions for them. The outcome of the pattern match will not be saved unless we bind it to a variable, so let\u2019s bind it to a variable\ncalled \nsample\n:\n\n\nsample <~ match coin:\n    true: normal(0,1)\n    false: uniform(0,1)\n\n\n\n\nNow that we have both the result of our coin toss experiment (\ncoin\n) and our mixture model (\nsample\n), we can return the values:\n\n\nreturn(coin, sample)\n\n\n\n\nWe have completed the mixture model program and can run it using the command \nhakaru twomixture.hk\n to collect samples indefinitely:\n\n\n(true, -0.37622272051934547)\n(false, 4.666320977960081e-2)\n(true, 1.3351978120820147)\n(true, 0.4657111228024136)\n(false, 0.6528078075939211)\n(false, 0.2410145787295287)\n(false, 0.624335005419879)\n(true, -1.5127939371882644)\n(false, 0.15925713370352967)\n(true, 2.2762774663914114e-2)\n...\n\n\n\n\nOf course, Hakaru would not be very interesting if it only provided the means for you to define your model. Let\u2019s try conditioning our model so that we can experiment with\ndifferent values for \nsample\n to estimate what values of \ncoin\n were used.\n\n\nConditioning a Hakaru Program\n\n\nSuppose for our \ntwomixture.hk\n program, we know the value of \nsample\n and want to see what the original values for \ncoin\n were. We can symbolically produce the unnormalized \nconditional distribution from which \ncoin\n samples are taken by using Hakaru\u2019s \ndisintegration\n transform. Before we use \ndisintegrate\n, we \nmust change the line \nreturn (coin, sample)\n to \nreturn (sample, coin)\n. This tells \ndisintegrate\n that we want to create a posterior distribution for \ncoin\n using known \nvalues for \nsample\n. \n\n\nOnce we have setup our model for the \ndisintegrate\n transform, we can transform our model by calling \ndisintegrate twomixture.hk\n. The \ndisintegrate\n transform creates a new \nmodel written as an anonymous function so that it is easier for you to use in other applications. In the model generated by the \ndisintegrate\n transform, our variable \n\nsample\n has been renamed to \nx5\n:\n\n\nfn x5 real: \n bern = fn p prob: \n         b <~ categorical([p, real2prob((1 - prob2real(p)))])\n         return [true, false][b]\n coin <~ bern(1/2)\n (match coin: \n   true: \n    x12 <~ weight((exp((negate(((x5 + 0) ^ 2)) / 2))\n                    / \n                   1\n                    / \n                   sqrt((2 * pi))),\n                  return ())\n    return coin\n   _: reject. measure(bool)) <|> \n bern = fn p prob: \n         b <~ categorical([p, real2prob((1 - prob2real(p)))])\n         return [true, false][b]\n coin <~ bern(1/2)\n (match coin: \n   false: \n    (match (not((x5 < 0)) && not((1 < x5))): \n      true: \n       x12 <~ return ()\n       return coin\n      _: reject. measure(bool))\n   _: reject. measure(bool))\n\n\n\n\nNote:\n The output for \ndisintegrate\n will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling \n\nhakaru disintegrate model1.hk > modelDis.hk\n. For this example, we will call our new program \ntwomixture_D.hk\n. \n\n\nWe can use this program to experiment with different values of \nsample\n(\nx5\n) to see what the original coin toss experiment looked like. To avoid altering the function \ngenerated by \ndisintegrate\n, let\u2019s assign it to a variable \ncoinToss\n so that we can reference it at the end of our program. For our first experiment, let\u2019s try a value of\n\n0.3\n. This means that we are conditioning our model to be more likely to pick samples from the uniform distribution:\n\n\ncoinToss = fn x5 real: \n             bern = fn p prob: \n                     b <~ categorical([p, real2prob((1 - prob2real(p)))])\n                     return [true, false][b]\n             coin <~ bern(1/2)\n             (match coin: \n               true: \n                x12 <~ weight((exp((negate(((x5 + 0) ^ 2)) / 2))\n                                / \n                               1\n                                / \n                               sqrt((2 * pi))),\n                              return ())\n                return coin\n               _: reject. measure(bool)) <|> \n             bern = fn p prob: \n                     b <~ categorical([p, real2prob((1 - prob2real(p)))])\n                     return [true, false][b]\n             coin <~ bern(1/2)\n             (match coin: \n               false: \n                (match (not((x5 < 0)) && not((1 < x5))): \n                  true: \n                   x12 <~ return ()\n                   return coin\n                  _: reject. measure(bool))\n               _: reject. measure(bool))\n\ncoinToss(0.3)\n\n\n\n\nWe can now run the program to estimate what values for \ncoin\n. Let\u2019s use some Unix commands to run the program 1000 times and gather the results into counts:\n\n\nhakaru -w twomixture_D.hk | head -n 1000 | sort | uniq -c\n\n    526 false\n    474 true\n\n\n\n\nAs we can see, when \nx5 = 0.3\n, our coin tosses were more likely to be Tails (\nfalse\n) than Heads (\ntrue\n). Let\u2019s change our argument to \ncoinToss\n to \n3.0\n so that we are\nconditioned to pick values from the normal distribution much more frequently. Running this program shows that our coin tosses must have all been Heads for this value to be\npossible:\n\n\nhakaru -w twomixture_D.hk | head -n 1000 | sort | uniq -c\n\n    1000 true\n\n\n\n\nYou have written a model to represent a Bernoulli experiement and used it to create a mixture model using a normal and uniform distribution. You have also used the \n\ndisintegrate\n transform to generate a new model that can be conditioned with different mixture model results to infer what the original distribution of coin toss \nexperiements might have been. For more Hakaru examples, see the \nExamples\n.\n\n\n\n\n\n\n\n\n\n\nBernoulli distribution (Wikipedia)\n\u00a0\n\u21a9\n\n\n\n\n\n\nMixture Model (Wikipedia)\n\u00a0\n\u21a9",
            "title": "Quick Start: A Mixture Model Example"
        },
        {
            "location": "/intro/quickstart/#quick-start-a-mixture-model-example",
            "text": "Let\u2019s start with a simple model of a coin toss experiment so that you can become familiar with some of Hakaru\u2019s data types and functionality. We will assume that a single \ncoin flip can be represented using a Bernoulli distribution. After we have created the Bernoulli model, we will use it to create a mixture model and condition the model\nto estimate what the original coin toss experiment looked like based on the resulting mixture model samples.",
            "title": "Quick Start: A Mixture Model Example"
        },
        {
            "location": "/intro/quickstart/#modeling-a-bernoulli-experiment",
            "text": "We will use the  categorical  Hakaru  Random Primitive  to write a Bernoulli distribution 1  for our model. The  categorical  primitive requires an  array  representing the probability of achieving each category in the experiement. Let\u2019s start with a fair experiment and state that each side of the \ncoin has an equal chance of being picked. The result of the coin toss is stored in the variable  b  using Hakaru\u2019s notation for  bind :  b <~ categorical([0.5, 0.5])  For data type simplicity, we will map Heads to  true  and Tails to  false . By putting the values of  true  and  false  into an array, we can use the value in  b  to\nselect which of them to return as the result of the coin toss:  return [true, false][b]  A characteristic of the Bernoulli distribution is that it assumes that only one experiment is conducted. To collect samples, we need to run this experiment multiple times.\nTo aid in this task, we can rewrite the Bernoulli model as a  function . We will call our function  bern :   def bern ():\n    b <~ categorical([0.5, 0.5])\n    return [true, false][b]  Now that we are using functions, we can generalize our model so that we can run experiments on both fair and trick coins. To do this, we should pass in a probability  p  as \na function argument, which is then used to populate the  categorical  primitive. Hakaru has a specialized data type for probabilities called  prob , which we will use as the\ndata type for our function input:  def bern (p prob):\n    b <~ categorical([p, (1 - p)])\n    return [true, false][b]  If you we to run this function, we will get a  Type Mismatch  error. This is because the value  (1 - p)  is converted to type  real  as a result of the subtraction operation\nand  categorical  expects all of the values in its array to be of type  prob . One solution would be to manually pass in the value of  (1 - p)  as a function argument, which\nwould artificially complicate our function. Instead, we can use Hakaru\u2019s  coercions  to recast  (1 - p)  to type  prob :  def bern (p prob):\n    b <~ categorical([p, real2prob(1 - p)])\n    return [true, false][b]  We can now use our model to run a series of Bernoullli experiments. Let\u2019s set up our program to use a fair coin and save it as  bernoulli.hk :  def bern (p prob):\n    b <~ categorical([p, real2prob(1 - p)])\n    return [true, false][b]\n\nbern(0.5)  Running this program using  hakaru bernoulli.hk  should result in an infinite stream of coin toss trials:  false\ntrue\nfalse\ntrue\ntrue\ntrue\n...  Now that we have set up our Bernoulli experiment, let\u2019s use it to create a mixture model.",
            "title": "Modeling a Bernoulli Experiment"
        },
        {
            "location": "/intro/quickstart/#creating-a-mixture-model",
            "text": "Let\u2019s use our coin flip experiment to create a mixture model by drawing a sample from a normal distribution when the coin is Heads ( true ) and from a uniform distribution\nwhen it is Tails ( false ). This is called a  mixture model 2  because we are selecting samples from different distributions. Let\u2019s start by saving a copy of your \nBernoulli function into a new program so that we can use it in our new model. For this example, we will call it  twomixture.hk .  Let\u2019s start by binding the return value of our  bern  function to a variable called  coin  to represent the outcome of an experiment:  coin <~ bern(0.5)  Now that we have stored the result of our experiment, let\u2019s use it to generate a sample. Our model has a selection condition where Heads causes a sample to be drawn from \nachieving normal distribution and Tails draws from a uniform distribution. There are two ways of handling this in Hakaru \u2013  conditionals  and  pattern matching . Since we are working with Booleans, let\u2019s use patern matching so that we can see what it looks like in Hakaru.  Hakaru pattern matching requires a sequence and a set of possible patterns to compare the sequence to. In our model, our sequence would be  coin  because that is what we are\nusing to select a distribution. Our possible patterns are the possible values that  coin  could have \u2013  true  and  false . When the pattern is  true , we call the normal\ndistribution and when it is  false  we call the uniform distribution. Both the  normal  and  uniform  functions are included in Hakaru\u2019s  Random Primitives ,\nso we do not need to define our own functions for them. The outcome of the pattern match will not be saved unless we bind it to a variable, so let\u2019s bind it to a variable\ncalled  sample :  sample <~ match coin:\n    true: normal(0,1)\n    false: uniform(0,1)  Now that we have both the result of our coin toss experiment ( coin ) and our mixture model ( sample ), we can return the values:  return(coin, sample)  We have completed the mixture model program and can run it using the command  hakaru twomixture.hk  to collect samples indefinitely:  (true, -0.37622272051934547)\n(false, 4.666320977960081e-2)\n(true, 1.3351978120820147)\n(true, 0.4657111228024136)\n(false, 0.6528078075939211)\n(false, 0.2410145787295287)\n(false, 0.624335005419879)\n(true, -1.5127939371882644)\n(false, 0.15925713370352967)\n(true, 2.2762774663914114e-2)\n...  Of course, Hakaru would not be very interesting if it only provided the means for you to define your model. Let\u2019s try conditioning our model so that we can experiment with\ndifferent values for  sample  to estimate what values of  coin  were used.",
            "title": "Creating a Mixture Model"
        },
        {
            "location": "/intro/quickstart/#conditioning-a-hakaru-program",
            "text": "Suppose for our  twomixture.hk  program, we know the value of  sample  and want to see what the original values for  coin  were. We can symbolically produce the unnormalized \nconditional distribution from which  coin  samples are taken by using Hakaru\u2019s  disintegration  transform. Before we use  disintegrate , we \nmust change the line  return (coin, sample)  to  return (sample, coin) . This tells  disintegrate  that we want to create a posterior distribution for  coin  using known \nvalues for  sample .   Once we have setup our model for the  disintegrate  transform, we can transform our model by calling  disintegrate twomixture.hk . The  disintegrate  transform creates a new \nmodel written as an anonymous function so that it is easier for you to use in other applications. In the model generated by the  disintegrate  transform, our variable  sample  has been renamed to  x5 :  fn x5 real: \n bern = fn p prob: \n         b <~ categorical([p, real2prob((1 - prob2real(p)))])\n         return [true, false][b]\n coin <~ bern(1/2)\n (match coin: \n   true: \n    x12 <~ weight((exp((negate(((x5 + 0) ^ 2)) / 2))\n                    / \n                   1\n                    / \n                   sqrt((2 * pi))),\n                  return ())\n    return coin\n   _: reject. measure(bool)) <|> \n bern = fn p prob: \n         b <~ categorical([p, real2prob((1 - prob2real(p)))])\n         return [true, false][b]\n coin <~ bern(1/2)\n (match coin: \n   false: \n    (match (not((x5 < 0)) && not((1 < x5))): \n      true: \n       x12 <~ return ()\n       return coin\n      _: reject. measure(bool))\n   _: reject. measure(bool))  Note:  The output for  disintegrate  will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling  hakaru disintegrate model1.hk > modelDis.hk . For this example, we will call our new program  twomixture_D.hk .   We can use this program to experiment with different values of  sample ( x5 ) to see what the original coin toss experiment looked like. To avoid altering the function \ngenerated by  disintegrate , let\u2019s assign it to a variable  coinToss  so that we can reference it at the end of our program. For our first experiment, let\u2019s try a value of 0.3 . This means that we are conditioning our model to be more likely to pick samples from the uniform distribution:  coinToss = fn x5 real: \n             bern = fn p prob: \n                     b <~ categorical([p, real2prob((1 - prob2real(p)))])\n                     return [true, false][b]\n             coin <~ bern(1/2)\n             (match coin: \n               true: \n                x12 <~ weight((exp((negate(((x5 + 0) ^ 2)) / 2))\n                                / \n                               1\n                                / \n                               sqrt((2 * pi))),\n                              return ())\n                return coin\n               _: reject. measure(bool)) <|> \n             bern = fn p prob: \n                     b <~ categorical([p, real2prob((1 - prob2real(p)))])\n                     return [true, false][b]\n             coin <~ bern(1/2)\n             (match coin: \n               false: \n                (match (not((x5 < 0)) && not((1 < x5))): \n                  true: \n                   x12 <~ return ()\n                   return coin\n                  _: reject. measure(bool))\n               _: reject. measure(bool))\n\ncoinToss(0.3)  We can now run the program to estimate what values for  coin . Let\u2019s use some Unix commands to run the program 1000 times and gather the results into counts:  hakaru -w twomixture_D.hk | head -n 1000 | sort | uniq -c\n\n    526 false\n    474 true  As we can see, when  x5 = 0.3 , our coin tosses were more likely to be Tails ( false ) than Heads ( true ). Let\u2019s change our argument to  coinToss  to  3.0  so that we are\nconditioned to pick values from the normal distribution much more frequently. Running this program shows that our coin tosses must have all been Heads for this value to be\npossible:  hakaru -w twomixture_D.hk | head -n 1000 | sort | uniq -c\n\n    1000 true  You have written a model to represent a Bernoulli experiement and used it to create a mixture model using a normal and uniform distribution. You have also used the  disintegrate  transform to generate a new model that can be conditioned with different mixture model results to infer what the original distribution of coin toss \nexperiements might have been. For more Hakaru examples, see the  Examples .      Bernoulli distribution (Wikipedia) \u00a0 \u21a9    Mixture Model (Wikipedia) \u00a0 \u21a9",
            "title": "Conditioning a Hakaru Program"
        },
        {
            "location": "/examples/",
            "text": "Examples\n\n\nGaussian Mixture Model\n\n\nBelow is a model for a Gaussian Mixture model. This can be seen\nas a Bayesian version of K-means clustering.\n\n\n# Prelude to define dirichlet\ndef add(a prob, b prob):\n    a + b\n\ndef sum(a array(prob)):\n    reduce(add, 0, a)\n\ndef normalize(x array(prob)):\n    total = sum(x)\n    array i of size(x):\n       x[i] / total\n\ndef dirichlet(as array(prob)):\n    xs <~ plate i of int2nat(size(as)-1):\n            beta(summate j from i+1 to size(as): as[j],\n                 as[i])\n    return array i of size(as):\n             x = product j from 0 to i: xs[j]\n             x * if i+1==size(as): 1 else: real2prob(1-xs[i])\n\n\n# num of clusters\nK = 5\n# num of points\nN = 20\n\n# prior probability of picking cluster K\npi  <~ dirichlet(array _ of K: 1)\n# prior on mean and precision\nmu  <~ plate _ of K:\n         normal(0, 5e-9)\ntau <~ plate _ of K:\n         gamma(2, 0.05)\n# observed data\nx   <~ plate _ of N:\n         i <~ categorical(pi)\n         normal(mu[i], tau[i])\n\nreturn (x, mu). pair(array(real), array(real))\n\n\n\n\nLatent Dirichlet Allocation\n\n\nBelow is the LDA topic model, a form of Bayesian Naives Bayes.\n\n\nK = 2 # number of topics\nM = 3 # number of docs\nV = 7 # size of vocabulary\n\n# number of words in each document\ndoc = [4, 5, 3]\n\ntopic_prior = array _ of K: 1.0\nword_prior  = array _ of V: 1.0\n\nphi <~ plate _ of K:     # word dist for topic k\n         dirichlet(word_prior)\n\n# likelihood\nz   <~ plate m of M:\n         theta <~ dirichlet(topic_prior)\n         plate _ of doc[m]: # topic marker for word n in doc m\n           categorical(theta)\n\nw   <~ plate m of M: # for doc m\n         plate n of doc[m]: # for word n in doc m\n           categorical(phi[z[m][n]])\n\nreturn (w, z)",
            "title": "Examples"
        },
        {
            "location": "/examples/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/examples/#gaussian-mixture-model",
            "text": "Below is a model for a Gaussian Mixture model. This can be seen\nas a Bayesian version of K-means clustering.  # Prelude to define dirichlet\ndef add(a prob, b prob):\n    a + b\n\ndef sum(a array(prob)):\n    reduce(add, 0, a)\n\ndef normalize(x array(prob)):\n    total = sum(x)\n    array i of size(x):\n       x[i] / total\n\ndef dirichlet(as array(prob)):\n    xs <~ plate i of int2nat(size(as)-1):\n            beta(summate j from i+1 to size(as): as[j],\n                 as[i])\n    return array i of size(as):\n             x = product j from 0 to i: xs[j]\n             x * if i+1==size(as): 1 else: real2prob(1-xs[i])\n\n\n# num of clusters\nK = 5\n# num of points\nN = 20\n\n# prior probability of picking cluster K\npi  <~ dirichlet(array _ of K: 1)\n# prior on mean and precision\nmu  <~ plate _ of K:\n         normal(0, 5e-9)\ntau <~ plate _ of K:\n         gamma(2, 0.05)\n# observed data\nx   <~ plate _ of N:\n         i <~ categorical(pi)\n         normal(mu[i], tau[i])\n\nreturn (x, mu). pair(array(real), array(real))",
            "title": "Gaussian Mixture Model"
        },
        {
            "location": "/examples/#latent-dirichlet-allocation",
            "text": "Below is the LDA topic model, a form of Bayesian Naives Bayes.  K = 2 # number of topics\nM = 3 # number of docs\nV = 7 # size of vocabulary\n\n# number of words in each document\ndoc = [4, 5, 3]\n\ntopic_prior = array _ of K: 1.0\nword_prior  = array _ of V: 1.0\n\nphi <~ plate _ of K:     # word dist for topic k\n         dirichlet(word_prior)\n\n# likelihood\nz   <~ plate m of M:\n         theta <~ dirichlet(topic_prior)\n         plate _ of doc[m]: # topic marker for word n in doc m\n           categorical(theta)\n\nw   <~ plate m of M: # for doc m\n         plate n of doc[m]: # for word n in doc m\n           categorical(phi[z[m][n]])\n\nreturn (w, z)",
            "title": "Latent Dirichlet Allocation"
        },
        {
            "location": "/transforms/compile/",
            "text": "Compiling to Haskell\n\n\nHakaru can be compiled to Haskell using the \ncompile\n command.\n\n\nFor example if we wish to compile \nexample.hk\n\n\nx <~ normal(0,1)\ny <~ normal(x,1)\nreturn y\n\n\n\n\nWe call \ncompile example.hk\n, which produces a file \nexample.hs\n.\n\n\ncat example.hs\n\n\n\n\n{-# LANGUAGE DataKinds, NegativeLiterals #-}\nmodule Main where\n\nimport           Prelude                          hiding (product)\nimport           Language.Hakaru.Runtime.Prelude\nimport           Language.Hakaru.Types.Sing\nimport qualified System.Random.MWC                as MWC\nimport           Control.Monad\n\nprog = \n  normal (nat2real (nat_ 0)) (nat2prob (nat_ 1)) >>= \\ x0 ->\n  normal x0 (nat2prob (nat_ 1)) >>= \\ y1 ->\n  dirac y1\n\nmain :: IO ()\nmain = do\n  g <- MWC.createSystemRandom\n  forever $ run g prog\n\n\n\n\nThis is a regular Haskell file, which can then be furthered compiled into\nmachine code.",
            "title": "Compiling to Haskell"
        },
        {
            "location": "/transforms/compile/#compiling-to-haskell",
            "text": "Hakaru can be compiled to Haskell using the  compile  command.  For example if we wish to compile  example.hk  x <~ normal(0,1)\ny <~ normal(x,1)\nreturn y  We call  compile example.hk , which produces a file  example.hs .  cat example.hs  {-# LANGUAGE DataKinds, NegativeLiterals #-}\nmodule Main where\n\nimport           Prelude                          hiding (product)\nimport           Language.Hakaru.Runtime.Prelude\nimport           Language.Hakaru.Types.Sing\nimport qualified System.Random.MWC                as MWC\nimport           Control.Monad\n\nprog = \n  normal (nat2real (nat_ 0)) (nat2prob (nat_ 1)) >>= \\ x0 ->\n  normal x0 (nat2prob (nat_ 1)) >>= \\ y1 ->\n  dirac y1\n\nmain :: IO ()\nmain = do\n  g <- MWC.createSystemRandom\n  forever $ run g prog  This is a regular Haskell file, which can then be furthered compiled into\nmachine code.",
            "title": "Compiling to Haskell"
        },
        {
            "location": "/transforms/hkc/",
            "text": "Compiling to C\n\n\nhkc\n is a command line tool to compiler Hakaru programs to C. HKC was\ncreated with portability and speed in mind. More recently, OpenMP support is\nbeing added to gain more performance on multi-core machines. Basic command line\nusage of HKC is much like other compilers:\n\n\nhkc foo.hk -o foo.c\n\n\n\n\nIt is possible to go straight to an executable with the \n--make ARG\n flag, where\nthe argument is the C compiler you would like to use.\n\n\nType Conversions\n\n\nThe types available in Hakaru programs are the following: \nnat\n, \nint\n, \nreal\n,\n\nprob\n, \narray(<type>)\n, \nmeasure(<type>)\n, and datum like \ntrue\n and \nfalse\n.\n\n\nnat\n and \nint\n have a trivial mapping to the C \nint\n type. \nreal\n becomes a C\n\ndouble\n. The \nprob\n type in Hakaru is stored in the log-domain to avoid\nunderflow. In C this corresponds to a \ndouble\n, but we first take the log of it\nbefore storing it, so we have to take the exp of it to bring it back to the real\nnumbers.\n\n\nArrays become structs that contain the size and a pointer to data stored within.\nThe structs are generated at compile time, but there are only four which are\nnamed after the type they contain. Here they all are:\n\n\nstruct arrayNat {\n  int size; int * data;\n};\n\nstruct arrayInt {\n  int size; int * data;\n};\n\nstruct arrayReal {\n  int size; double * data;\n};\n\nstruct arrayProb {\n  int size; double * data;\n};\n\n\n\n\nMeasures\n\n\nMeasures compile to C functions that take a location for a sample, return the\nweight of the measure and store a sample in the location is was given. A simple\nexample is \nuniform(0,1)\n a measure over type \nreal\n.\n\n\n#include <time.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\ndouble measure(double * s_a)\n {\n    *s_a = ((double)0) + ((double)rand()) / ((double)RAND_MAX) * ((double)1) - ((double)0);\n    return 0;\n }\n\nint main()\n {\n    double sample;\n    while (1)\n    {\n        measure(&sample);\n        printf(\"%.17f\\n\",sample);\n    }\n    return 0;\n }\n\n\n\n\nRecall that weights have type \nprob\n and are stored in the log-domain. This\nexample has a weight of 1.\n\n\nCalling \nhkc\n on a measure will create a function like the one above and also a\nmain function that infinitely takes samples. Using \nhkc -F ARG\n will produce\njust the function with the name of its argument.\n\n\nLambdas\n\n\nLambdas compile to functions in C:\n\n\nfn x array(real):\n  (summate i from 0 to size(x): x[i])\n   *\n  prob2real(recip(nat2prob((size(x) + 1))))\n\n\n\n\n\nBecomes:\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\nstruct arrayReal {\n   int size; double * data;\n };\n\ndouble fn_a(struct arrayReal x_b)\n {\n   unsigned int i_c;\n   double acc_d;\n   double p_e;\n   double _f;\n   double r_g;\n   acc_d = 0;\n   for (i_c = 0; i_c < x_b.size; i_c++)\n   {\n     acc_d += *(x_b.data + i_c);\n   }\n   p_e = log1p(((1 + x_b.size) - 1));\n   _f = -p_e;\n   r_g = (expm1(_f) + 1);\n   return (r_g * acc_d);\n }\n\n\n\n\nUsing the \n-F\n flag will allow the user to add their own name to a function,\notherwise the name is chosen automatically as \nfn_<unique identifier>\n.\n\n\nComputations\n\n\nWhen compiling a computation, HKC just creates a main function to compute the\nvalue and print it. For example:\n\n\nsummate i from 1 to 100000000:\n  nat2real(i) / nat2real(i)\n\n\n\n\nbecomes:\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\nint main()\n {\n    double result;\n    int i_a;\n    double acc_b;\n    double _c;\n    acc_b = 0;\n    for (i_a = 1; i_a < 100000000; i_a++)\n    {\n       _c = (1 / ((double)i_a));\n       acc_b += (_c * ((double)i_a));\n    }\n    result = acc_b;\n    printf(\"%.17f\\n\",result);\n    return 0;\n }\n\n\n\n\nParallel Programs\n\n\nCalling HKC with the \n-j\n flag will generate the code with parallel regions to\ncompute the value. The parallel code uses OpenMP directives. To check if you\u2019re\ncompiler supports OpenMP, check \nhere\n.\n\n\nFor example, GCC requires the \n-fopenmp\n flag for OpenMP support:\n\n\nhkc -j foo.hk -o foo.c\ngcc -lm -fopenmp foo.c -o foo.bin",
            "title": "Compiling to C"
        },
        {
            "location": "/transforms/hkc/#compiling-to-c",
            "text": "hkc  is a command line tool to compiler Hakaru programs to C. HKC was\ncreated with portability and speed in mind. More recently, OpenMP support is\nbeing added to gain more performance on multi-core machines. Basic command line\nusage of HKC is much like other compilers:  hkc foo.hk -o foo.c  It is possible to go straight to an executable with the  --make ARG  flag, where\nthe argument is the C compiler you would like to use.",
            "title": "Compiling to C"
        },
        {
            "location": "/transforms/hkc/#type-conversions",
            "text": "The types available in Hakaru programs are the following:  nat ,  int ,  real , prob ,  array(<type>) ,  measure(<type>) , and datum like  true  and  false .  nat  and  int  have a trivial mapping to the C  int  type.  real  becomes a C double . The  prob  type in Hakaru is stored in the log-domain to avoid\nunderflow. In C this corresponds to a  double , but we first take the log of it\nbefore storing it, so we have to take the exp of it to bring it back to the real\nnumbers.  Arrays become structs that contain the size and a pointer to data stored within.\nThe structs are generated at compile time, but there are only four which are\nnamed after the type they contain. Here they all are:  struct arrayNat {\n  int size; int * data;\n};\n\nstruct arrayInt {\n  int size; int * data;\n};\n\nstruct arrayReal {\n  int size; double * data;\n};\n\nstruct arrayProb {\n  int size; double * data;\n};",
            "title": "Type Conversions"
        },
        {
            "location": "/transforms/hkc/#measures",
            "text": "Measures compile to C functions that take a location for a sample, return the\nweight of the measure and store a sample in the location is was given. A simple\nexample is  uniform(0,1)  a measure over type  real .  #include <time.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\ndouble measure(double * s_a)\n {\n    *s_a = ((double)0) + ((double)rand()) / ((double)RAND_MAX) * ((double)1) - ((double)0);\n    return 0;\n }\n\nint main()\n {\n    double sample;\n    while (1)\n    {\n        measure(&sample);\n        printf(\"%.17f\\n\",sample);\n    }\n    return 0;\n }  Recall that weights have type  prob  and are stored in the log-domain. This\nexample has a weight of 1.  Calling  hkc  on a measure will create a function like the one above and also a\nmain function that infinitely takes samples. Using  hkc -F ARG  will produce\njust the function with the name of its argument.",
            "title": "Measures"
        },
        {
            "location": "/transforms/hkc/#lambdas",
            "text": "Lambdas compile to functions in C:  fn x array(real):\n  (summate i from 0 to size(x): x[i])\n   *\n  prob2real(recip(nat2prob((size(x) + 1))))  Becomes:  #include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\nstruct arrayReal {\n   int size; double * data;\n };\n\ndouble fn_a(struct arrayReal x_b)\n {\n   unsigned int i_c;\n   double acc_d;\n   double p_e;\n   double _f;\n   double r_g;\n   acc_d = 0;\n   for (i_c = 0; i_c < x_b.size; i_c++)\n   {\n     acc_d += *(x_b.data + i_c);\n   }\n   p_e = log1p(((1 + x_b.size) - 1));\n   _f = -p_e;\n   r_g = (expm1(_f) + 1);\n   return (r_g * acc_d);\n }  Using the  -F  flag will allow the user to add their own name to a function,\notherwise the name is chosen automatically as  fn_<unique identifier> .",
            "title": "Lambdas"
        },
        {
            "location": "/transforms/hkc/#computations",
            "text": "When compiling a computation, HKC just creates a main function to compute the\nvalue and print it. For example:  summate i from 1 to 100000000:\n  nat2real(i) / nat2real(i)  becomes:  #include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\nint main()\n {\n    double result;\n    int i_a;\n    double acc_b;\n    double _c;\n    acc_b = 0;\n    for (i_a = 1; i_a < 100000000; i_a++)\n    {\n       _c = (1 / ((double)i_a));\n       acc_b += (_c * ((double)i_a));\n    }\n    result = acc_b;\n    printf(\"%.17f\\n\",result);\n    return 0;\n }",
            "title": "Computations"
        },
        {
            "location": "/transforms/hkc/#parallel-programs",
            "text": "Calling HKC with the  -j  flag will generate the code with parallel regions to\ncompute the value. The parallel code uses OpenMP directives. To check if you\u2019re\ncompiler supports OpenMP, check  here .  For example, GCC requires the  -fopenmp  flag for OpenMP support:  hkc -j foo.hk -o foo.c\ngcc -lm -fopenmp foo.c -o foo.bin",
            "title": "Parallel Programs"
        },
        {
            "location": "/lang/rand/",
            "text": "Primitive Probability Distributions\n\n\nHakaru comes with a small set of primitive probability\ndistributions.\n\n\n\n\n\n\n\n\nnormal(mean. \nreal\n, standard_deviation. \nprob\n): \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\n\n\nunivariate Normal (Gaussian) distribution\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuniform(low. \nreal\n, high. \nreal\n): \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\n\n\nUniform distribution is a continuous univariate distribution defined from low to high\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngamma(shape. \nprob\n, scale. \nprob\n): \nmeasure(prob)\n \n\n\n\n\n\n\n\n\n\n\n\n\nGamma distribution with shape and scale parameterization\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbeta(a. \nprob\n, b. \nprob\n): \nmeasure(prob)\n \n\n\n\n\n\n\n\n\n\n\n\n\nBeta distribution\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npoisson(l. \nprob\n): \nmeasure(nat)\n \n\n\n\n\n\n\n\n\n\n\n\n\nPoisson distribution\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategorical(v. \narray(prob)\n): \nmeasure(nat)\n \n\n\n\n\n\n\n\n\n\n\n\n\nCategorical distribution\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndirac(x. \na\n): \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\n\n\nDirac distribution\n\n\n-\n\n\n\n\n\n\n\n\nThe Dirac distribution appears often enough, that we have given an\nadditional keyword in our language for it: \nreturn\n. The following\nprograms are equivalent.\n\n\ndirac(3)\n\n\n\n\nreturn 3\n\n\n\n\n\n\n\n\n\n\nlebesgue: \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\n\n\nthe distribution constant over the real line\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweight(x. \nprob\n, m. \nmeasure(a)\n): \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\n\n\na \nm\n distribution, reweighted by \nx\n\n\n-\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreject: \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\n\n\nThe distribution over the empty set\n\n\n-\n\n\n\n\n\n\n\n\nFinally, we have a binary choice operator \n<|>\n, which takes two\ndistributions, and returns an unnormalized distribution which returns\none or the other.  For example, to get a distribution which where with\nprobability 0.5 draws from a uniform(0,1), and probability 0.5 draws\nfrom uniform(5,6).\n\n\nweight(0.5, uniform(0,1)) <|>\nweight(0.5, uniform(5,6))",
            "title": "Primitive Probability Distributions"
        },
        {
            "location": "/lang/rand/#primitive-probability-distributions",
            "text": "Hakaru comes with a small set of primitive probability\ndistributions.",
            "title": "Primitive Probability Distributions"
        },
        {
            "location": "/lang/letbind/",
            "text": "Let and Bind\n\n\nIn Hakaru, we can give names for expressions to our programs with \n=\n,\nwhich we call \nLet\n. This gives us the ability to share computation\nthat might be needed in the program.\n\n\nx = 2\nx + 3\n\n\n\n\nWe can use \n=\n to give a name to any expression in our language. The\nname you assign is in scope for the rest of the body it was defined in.\n\n\nBind\n\n\nHakaru also has the operator \n<~\n. This operator, which call \nBind\n\ncan only be used with expressions that denote probability distributions.\nBind allows us to talk about draws from a distribution using a name for\nany particular value that could have come from that distribution.\n\n\n# Bad\nx <~ 2 + 3\nx\n\n\n\n\n# Good\nx <~ normal(0,1)\nreturn x\n\n\n\n\nBecause Bind is about draws from a distribution, the rest of the body\nmust also denote a probability distribution.\n\n\n# Bad\nx <~ normal(0,1)\nx\n\n\n\n\n# Good\nx <~ normal(0,1)\nreturn x\n\n\n\n\nTo help distinguish Let and Bind. Here is a probabilistic program, where we\nlet \nf\n be equal to the normal distribution, and take draws from \nf\n.\n\n\nf = normal(0,1)\nx <~ f\nreturn x*x",
            "title": "Let and Bind"
        },
        {
            "location": "/lang/letbind/#let-and-bind",
            "text": "In Hakaru, we can give names for expressions to our programs with  = ,\nwhich we call  Let . This gives us the ability to share computation\nthat might be needed in the program.  x = 2\nx + 3  We can use  =  to give a name to any expression in our language. The\nname you assign is in scope for the rest of the body it was defined in.",
            "title": "Let and Bind"
        },
        {
            "location": "/lang/letbind/#bind",
            "text": "Hakaru also has the operator  <~ . This operator, which call  Bind \ncan only be used with expressions that denote probability distributions.\nBind allows us to talk about draws from a distribution using a name for\nany particular value that could have come from that distribution.  # Bad\nx <~ 2 + 3\nx  # Good\nx <~ normal(0,1)\nreturn x  Because Bind is about draws from a distribution, the rest of the body\nmust also denote a probability distribution.  # Bad\nx <~ normal(0,1)\nx  # Good\nx <~ normal(0,1)\nreturn x  To help distinguish Let and Bind. Here is a probabilistic program, where we\nlet  f  be equal to the normal distribution, and take draws from  f .  f = normal(0,1)\nx <~ f\nreturn x*x",
            "title": "Bind"
        },
        {
            "location": "/lang/cond/",
            "text": "Conditionals\n\n\nHakaru supports an \nif\n expression. This if must have two\nbodies. There exists no special syntax for \nelse if\n like\nyou might find in Python.\n\n\na  = 4\nb  = 5\nif a > b:\n   a + 1\nelse:\n   b - 2",
            "title": "Conditionals"
        },
        {
            "location": "/lang/cond/#conditionals",
            "text": "Hakaru supports an  if  expression. This if must have two\nbodies. There exists no special syntax for  else if  like\nyou might find in Python.  a  = 4\nb  = 5\nif a > b:\n   a + 1\nelse:\n   b - 2",
            "title": "Conditionals"
        },
        {
            "location": "/lang/functions/",
            "text": "Functions\n\n\nFunctions can be defined using a Python-inspired style syntax. One\nnotable difference is that each argument must be followed by its\ntype.\n\n\ndef add(x real, y real):\n    x + y\n\nadd(4,5)\n\n\n\n\nWe may optionally provide a type for the return value of a function if\nwe wish.\n\n\ndef add(x. real, y. real) real:\n    x + y\n\nadd(4,5)\n\n\n\n\nAnonymous functions\n\n\nIf you don\u2019t wish to name your functions, we also offer a syntax\nfor anonymous functions. These only take on argument and must be\ngiven a type alongside the variable name.\n\n\nfn x real: x + 1\n\n\n\n\nInternally, there are only one argument anonymous functions, and\nlets. The first example is equivalent to the following.\n\n\nadd = fn x real:\n         fn y real:\n            x + y\nadd(4,5)",
            "title": "Functions"
        },
        {
            "location": "/lang/functions/#functions",
            "text": "Functions can be defined using a Python-inspired style syntax. One\nnotable difference is that each argument must be followed by its\ntype.  def add(x real, y real):\n    x + y\n\nadd(4,5)  We may optionally provide a type for the return value of a function if\nwe wish.  def add(x. real, y. real) real:\n    x + y\n\nadd(4,5)",
            "title": "Functions"
        },
        {
            "location": "/lang/functions/#anonymous-functions",
            "text": "If you don\u2019t wish to name your functions, we also offer a syntax\nfor anonymous functions. These only take on argument and must be\ngiven a type alongside the variable name.  fn x real: x + 1  Internally, there are only one argument anonymous functions, and\nlets. The first example is equivalent to the following.  add = fn x real:\n         fn y real:\n            x + y\nadd(4,5)",
            "title": "Anonymous functions"
        },
        {
            "location": "/lang/coercions/",
            "text": "Types and Coercions\n\n\nHakaru is a simply-typed language which has\na few basic types and some more complicated\nones which can be built out of simpler types.\n\n\nTypes\n\n\n\n\nnat is the type for natural numbers. This includes zero.\n\n\nint is the integer type.\n\n\nprob is the type for positive real number. This includes zero.\n\n\nreal is the type for real numbers.\n\n\narray(x) is the type for arrays where each element is type x\n\n\nmeasure(x) is the type for probability distributions whose\n  sample space is type x\n\n\n\n\nCoercions\n\n\nFor the primitive numeric types we also offer coercion functions.\n\n\n\n\nprob2real\n\n\nint2real\n\n\nnat2int\n\n\nreal2prob\n\n\nreal2int\n\n\nint2nat\n\n\n\n\nFor the ones which are always safe to apply such as \nnat2int\n we will\nautomatically insert them if it is required for the program to typecheck.",
            "title": "Types and Coercions"
        },
        {
            "location": "/lang/coercions/#types-and-coercions",
            "text": "Hakaru is a simply-typed language which has\na few basic types and some more complicated\nones which can be built out of simpler types.",
            "title": "Types and Coercions"
        },
        {
            "location": "/lang/coercions/#types",
            "text": "nat is the type for natural numbers. This includes zero.  int is the integer type.  prob is the type for positive real number. This includes zero.  real is the type for real numbers.  array(x) is the type for arrays where each element is type x  measure(x) is the type for probability distributions whose\n  sample space is type x",
            "title": "Types"
        },
        {
            "location": "/lang/coercions/#coercions",
            "text": "For the primitive numeric types we also offer coercion functions.   prob2real  int2real  nat2int  real2prob  real2int  int2nat   For the ones which are always safe to apply such as  nat2int  we will\nautomatically insert them if it is required for the program to typecheck.",
            "title": "Coercions"
        },
        {
            "location": "/lang/datatypes/",
            "text": "Data Types and Match\n\n\nHakaru with several built-in data types.\n\n\n\n\npair\n\n\nunit\n\n\neither\n\n\nbool\n\n\n\n\nMatch\n\n\nWe use \nmatch\n to deconstruct out data types\nand access their elements.\n\n\nmatch left(3). either(int,bool):\n  left(x) : 1\n  right(x): 2\n\n\n\n\nWe do include special syntax for pairs\n\n\nmatch (1,2):\n  (x,y): x + y",
            "title": "Data Types and Match"
        },
        {
            "location": "/lang/datatypes/#data-types-and-match",
            "text": "Hakaru with several built-in data types.   pair  unit  either  bool",
            "title": "Data Types and Match"
        },
        {
            "location": "/lang/datatypes/#match",
            "text": "We use  match  to deconstruct out data types\nand access their elements.  match left(3). either(int,bool):\n  left(x) : 1\n  right(x): 2  We do include special syntax for pairs  match (1,2):\n  (x,y): x + y",
            "title": "Match"
        },
        {
            "location": "/lang/arrays/",
            "text": "Arrays and Plate\n\n\nHakaru provides special syntax for arrays, which\nis distinct from the other data types.\n\n\nArrays\n\n\nTo construct arrays, we provide an index variable, size argument, and\nan expression body. This body is evaluated for each index of the\narray. For example, to construct the array \n[0,1,2,3]\n:\n\n\narray i of 4: i\n\n\n\n\nArray Literals\n\n\nWe can also create arrays using the literal syntax a comma delimited\nlist surrounded by brackets: \n[0,1,2,3]\n\n\nArray size and indexing\n\n\nIf \na\n is an array, then \nsize(a)\n is its number of elements, which is a \nnat\n.\nIf \ni\n is a \nnat\n then \na[i]\n is the element of \na\n at index \ni\n.\nIndices start at zero, so the maximum valid value of \ni\n is \nsize(a)-1\n.\n\n\nPlate\n\n\nBeyond, arrays Hakaru includes special syntax for describing measures\nover arrays called \nplate\n. Plate using the same syntax as \narray\n but\nthe body must have a measure type. It returns a measure over arrays.\nFor example, if we wish to have a distribution over three independent\nnormal distributions we would do so as follows:\n\n\nplate _ of 3: normal(0,1)",
            "title": "Arrays and Plate"
        },
        {
            "location": "/lang/arrays/#arrays-and-plate",
            "text": "Hakaru provides special syntax for arrays, which\nis distinct from the other data types.",
            "title": "Arrays and Plate"
        },
        {
            "location": "/lang/arrays/#arrays",
            "text": "To construct arrays, we provide an index variable, size argument, and\nan expression body. This body is evaluated for each index of the\narray. For example, to construct the array  [0,1,2,3] :  array i of 4: i",
            "title": "Arrays"
        },
        {
            "location": "/lang/arrays/#array-literals",
            "text": "We can also create arrays using the literal syntax a comma delimited\nlist surrounded by brackets:  [0,1,2,3]",
            "title": "Array Literals"
        },
        {
            "location": "/lang/arrays/#array-size-and-indexing",
            "text": "If  a  is an array, then  size(a)  is its number of elements, which is a  nat .\nIf  i  is a  nat  then  a[i]  is the element of  a  at index  i .\nIndices start at zero, so the maximum valid value of  i  is  size(a)-1 .",
            "title": "Array size and indexing"
        },
        {
            "location": "/lang/arrays/#plate",
            "text": "Beyond, arrays Hakaru includes special syntax for describing measures\nover arrays called  plate . Plate using the same syntax as  array  but\nthe body must have a measure type. It returns a measure over arrays.\nFor example, if we wish to have a distribution over three independent\nnormal distributions we would do so as follows:  plate _ of 3: normal(0,1)",
            "title": "Plate"
        },
        {
            "location": "/lang/loops/",
            "text": "Loops\n\n\nWe also express loops that compute sums (\nsummate\n) and products (\nproduct\n).\nThe syntax of these loops begins by declaring an \ninclusive\n lower bound and\nan \nexclusive\n upper bound.  For example, the factorial of \nn\n is not\n\nproduct i from 1 to n: i\n but rather \nproduct i from 1 to n+1: i\n.\nThis convention takes some getting used to but it makes it easy to deal\nwith arrays.  For example, if \na\n is an array of numbers then their sum is\n\nsummate i from 0 to size(a): a[i]\n.",
            "title": "Loops"
        },
        {
            "location": "/lang/loops/#loops",
            "text": "We also express loops that compute sums ( summate ) and products ( product ).\nThe syntax of these loops begins by declaring an  inclusive  lower bound and\nan  exclusive  upper bound.  For example, the factorial of  n  is not product i from 1 to n: i  but rather  product i from 1 to n+1: i .\nThis convention takes some getting used to but it makes it easy to deal\nwith arrays.  For example, if  a  is an array of numbers then their sum is summate i from 0 to size(a): a[i] .",
            "title": "Loops"
        },
        {
            "location": "/transforms/expect/",
            "text": "Expect\n\n\nThe expectation transformation takes a program representing a measure,\nand a function over the sample space, and returns a program computing\nthe expectation over that measure with respect to the given function.\n\n\nUsage\n\n\nExpect can be used inside programs with the \nexpect\n keyword.\n\n\nexpect x uniform(1,3):\n    real2prob(2*x + 1)\n\n\n\n\nThis program computes the expectation of \nuniform(1,3)\n using the\nfunction \n2*x + 1\n. This program expands to the following equivalent\nprogram:\n\n\nintegrate x from 1 to 3: \n recip(real2prob(3 - 1)) * real2prob(2*x + 1)\n\n\n\n\nThis can be optimized by piping by it into the \nsimplify\n program. It\nwill in turn return \n5\n.",
            "title": "Expect"
        },
        {
            "location": "/transforms/expect/#expect",
            "text": "The expectation transformation takes a program representing a measure,\nand a function over the sample space, and returns a program computing\nthe expectation over that measure with respect to the given function.",
            "title": "Expect"
        },
        {
            "location": "/transforms/expect/#usage",
            "text": "Expect can be used inside programs with the  expect  keyword.  expect x uniform(1,3):\n    real2prob(2*x + 1)  This program computes the expectation of  uniform(1,3)  using the\nfunction  2*x + 1 . This program expands to the following equivalent\nprogram:  integrate x from 1 to 3: \n recip(real2prob(3 - 1)) * real2prob(2*x + 1)  This can be optimized by piping by it into the  simplify  program. It\nwill in turn return  5 .",
            "title": "Usage"
        },
        {
            "location": "/transforms/normalize/",
            "text": "Normalize\n\n\nWe also provide a \nnormalize\n command. This command takes as input a\nprogram representing any measure and reweights it into a program\nrepresenting a probability distribution.\n\n\nFor example in a slightly contrived example, we can weight a normal\ndistribution by two. Normalizing it will then remove this weight.\n\n\n> echo \"weight(2, normal(0,1))\" | normalize | simplify -\nnormal(0, 1)",
            "title": "Normalize"
        },
        {
            "location": "/transforms/normalize/#normalize",
            "text": "We also provide a  normalize  command. This command takes as input a\nprogram representing any measure and reweights it into a program\nrepresenting a probability distribution.  For example in a slightly contrived example, we can weight a normal\ndistribution by two. Normalizing it will then remove this weight.  > echo \"weight(2, normal(0,1))\" | normalize | simplify -\nnormal(0, 1)",
            "title": "Normalize"
        },
        {
            "location": "/transforms/disintegrate/",
            "text": "Disintegrate\n\n\nThe \ndisintegrate\n transformation converts a Hakaru program representing a joint probability distribution into a Hakaru program representing a posterior distribution for a \ntarget distribution variable. This transform is equivalent to model conditioning in probability theory, where the known data is provided to the transformed Hakaru model.\n\n\nNote:\n The \ndisintegrate\n transform cannot be used to condition variables of type \nbool\n or expressions containing Boolean operators.\n\n\nUsage\n\n\nBefore you use the \ndisintegrate\n transform, your Hakaru program should contain a \nreturn\n statement containing the variables for your known and unknown data. The order of the\nvariables in the \nreturn\n statement is important. The variable for the known data should appear first, followed by the variable representing the unknown data.\n\n\nYou can use the \ndisintegrate\n transform in the command line by calling:\n\n\ndisintegrate hakaru_program.hk\n\n\n\n\nThis command will return a new Hakaru program that contains an anonymous function representing the transformed program. The function argument represents the variable for which \nyou will test different values for your unknown variable.\n\n\nExample\n\n\nLet\u2019s condition a joint probability distribution of two independent random variables that are each drawn from a normal distribution. You can define this model in Hakaru using\na program such as:\n\n\ny <~ normal(0,1)\nx <~ normal(\u03b8,1)\nreturn (y,x)\n\n\n\n\nIn this program, \nx\n and \ny\n are the independent variables. The statement \nreturn (y,x)\n states that you want to condition your model to create a posterior model for \nx\n using\nknown values for \ny\n. If you save this program as \nhello1.hk\n, you would call the disintegrate transform on it by running:\n\n\ndisintegrate hello1.hk\n\n\n\n\nNote:\n The output for \ndisintegrate\n will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling \n\ndisintegrate model1.hk > model2.hk\n. For this example, we will call our new program \nhello1_D.hk\n.\n\n\nThe resulting program renames the known-value variable \ny\n (here it is renamed to \nx2\n) and creates an anonymous function that, given a value for \ny\n, calculates the \ncorresponding value for \nx\n:\n\n\nfn x2 real: \n x <~ normal(0, 1)\n x7 <~ weight((exp((negate(((x2 - x) ^ 2)) / 2))\n                / \n               1\n                / \n               sqrt((2 * pi))),\n              return ())\n return x",
            "title": "Disintegrate"
        },
        {
            "location": "/transforms/disintegrate/#disintegrate",
            "text": "The  disintegrate  transformation converts a Hakaru program representing a joint probability distribution into a Hakaru program representing a posterior distribution for a \ntarget distribution variable. This transform is equivalent to model conditioning in probability theory, where the known data is provided to the transformed Hakaru model.  Note:  The  disintegrate  transform cannot be used to condition variables of type  bool  or expressions containing Boolean operators.",
            "title": "Disintegrate"
        },
        {
            "location": "/transforms/disintegrate/#usage",
            "text": "Before you use the  disintegrate  transform, your Hakaru program should contain a  return  statement containing the variables for your known and unknown data. The order of the\nvariables in the  return  statement is important. The variable for the known data should appear first, followed by the variable representing the unknown data.  You can use the  disintegrate  transform in the command line by calling:  disintegrate hakaru_program.hk  This command will return a new Hakaru program that contains an anonymous function representing the transformed program. The function argument represents the variable for which \nyou will test different values for your unknown variable.",
            "title": "Usage"
        },
        {
            "location": "/transforms/disintegrate/#example",
            "text": "Let\u2019s condition a joint probability distribution of two independent random variables that are each drawn from a normal distribution. You can define this model in Hakaru using\na program such as:  y <~ normal(0,1)\nx <~ normal(\u03b8,1)\nreturn (y,x)  In this program,  x  and  y  are the independent variables. The statement  return (y,x)  states that you want to condition your model to create a posterior model for  x  using\nknown values for  y . If you save this program as  hello1.hk , you would call the disintegrate transform on it by running:  disintegrate hello1.hk  Note:  The output for  disintegrate  will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling  disintegrate model1.hk > model2.hk . For this example, we will call our new program  hello1_D.hk .  The resulting program renames the known-value variable  y  (here it is renamed to  x2 ) and creates an anonymous function that, given a value for  y , calculates the \ncorresponding value for  x :  fn x2 real: \n x <~ normal(0, 1)\n x7 <~ weight((exp((negate(((x2 - x) ^ 2)) / 2))\n                / \n               1\n                / \n               sqrt((2 * pi))),\n              return ())\n return x",
            "title": "Example"
        },
        {
            "location": "/transforms/density/",
            "text": "Density\n\n\nThe density transform (\ndensity\n) finds the density of a probability distribution at a particular point. This transform is a specialized form of the \n\ndisintegrate\n transform\n that also computes the \nexpectation\n of the probabilistic \ndistribution as part of its work. \n\n\nUsage\n\n\nYour Hakaru program must be a probability distribution (type \nmeasure(x)\n) in order to use the \ndensity\n transform. Most Hakaru programs that end\nwith a \nreturn\n statement do not meet this requirement because they return values instead of functions on values.\n\n\nYou can use the \ndensity\n transform in the command line by calling:\n\n\ndensity hakaru_program.hk\n\n\n\n\nThis transformation will produce a new Hakaru program containing an \nanonymous function\n representing the density function for that model.\n\n\nExample\n\n\nThe normal distribution is a commonly used distribution in probabilistic modeling. A simple Hakaru program modelling the simplest normal distribution\nis:\n\n\nnormal(0,1)\n\n\n\n\nAssuming that this program is named \nnorm.hk\n, we can calculate the density function for this distribution by running:\n\n\ndensity norm.hk\n\n\n\n\nNote:\n The output for \ndensity\n will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling \n\ndensity model1.hk > model2.hk\n. For this example, we will call our new program \nnorm_density.hk\n.\n\n\nWhen you open the new Hakaru program, \nnorm_density.hk\n, you will find an anonymous Hakaru function. You can then assign values to the function\u2019s arguments to test the \nprobabilistic density at a specific point in the model.\n\n\nfn x0 real: \n (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1)\n\n\n\n\nFor example, if you wanted to test the density at \n0.2\n, you could alter \nnorm_density.hk\n to:\n\n\nnormalDensity = fn x0 real: \n (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1)\n\nreturn normalDensity(0.2)\n\n\n\n\nYou could then use the \nhakaru\n command\n to have Hakaru compute the density:\n\n\n$ hakaru norm_density.hk | head -n 1\n0.3910426939754559\n\n\n\n\nNote:\n If the argument \nhead -n 1\n is omitted, the \nhakaru\n command will print out the resulting density value indefinitely.",
            "title": "Density"
        },
        {
            "location": "/transforms/density/#density",
            "text": "The density transform ( density ) finds the density of a probability distribution at a particular point. This transform is a specialized form of the  disintegrate  transform  that also computes the  expectation  of the probabilistic \ndistribution as part of its work.",
            "title": "Density"
        },
        {
            "location": "/transforms/density/#usage",
            "text": "Your Hakaru program must be a probability distribution (type  measure(x) ) in order to use the  density  transform. Most Hakaru programs that end\nwith a  return  statement do not meet this requirement because they return values instead of functions on values.  You can use the  density  transform in the command line by calling:  density hakaru_program.hk  This transformation will produce a new Hakaru program containing an  anonymous function  representing the density function for that model.",
            "title": "Usage"
        },
        {
            "location": "/transforms/density/#example",
            "text": "The normal distribution is a commonly used distribution in probabilistic modeling. A simple Hakaru program modelling the simplest normal distribution\nis:  normal(0,1)  Assuming that this program is named  norm.hk , we can calculate the density function for this distribution by running:  density norm.hk  Note:  The output for  density  will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling  density model1.hk > model2.hk . For this example, we will call our new program  norm_density.hk .  When you open the new Hakaru program,  norm_density.hk , you will find an anonymous Hakaru function. You can then assign values to the function\u2019s arguments to test the \nprobabilistic density at a specific point in the model.  fn x0 real: \n (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1)  For example, if you wanted to test the density at  0.2 , you could alter  norm_density.hk  to:  normalDensity = fn x0 real: \n (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1)\n\nreturn normalDensity(0.2)  You could then use the  hakaru  command  to have Hakaru compute the density:  $ hakaru norm_density.hk | head -n 1\n0.3910426939754559  Note:  If the argument  head -n 1  is omitted, the  hakaru  command will print out the resulting density value indefinitely.",
            "title": "Example"
        },
        {
            "location": "/transforms/simplify/",
            "text": "Hakaru-Maple\n\n\nHakaru uses the computer algebra system Maple to aid in performing program\ntransformations. You can use this functionality of Hakaru if you have Maple\ninstalled locally or can access Maple remotely. \n\n\nMaple can be accessed through the module \nLanguage.Hakaru.Maple\n or through the\nHakaru program \nhk-maple\n.\n\n\nThe \nhk-maple\n command invokes a Maple command on a Hakaru program. Given a\nHakaru program in concrete syntax and a Maple-Hakaru command, typecheck the\nprogram invoke the Maple command on the program and its type pretty print, parse\nand typecheck the program resulting from Maple. See the \n--help\n flag of\n\nhk-maple\n for more information.\n\n\nThe currently available Maple-Hakaru commands (also called subcommands):\n\n\n\n\nSimplify \n\n\nDisintegrate \n\n\nSummarize \n\n\n\n\nNote: calls to Maple may take a very long time. To see if your program is taking\nan appreciable amount of time to parse and typecheck, use the \n--debug\n flag.\n\n\nSubcommands\n\n\nSimplify\n\n\nHakaru programs are interpreted by Maple as linear operators. In this\ninterpretation, many commonly understood (by Maple) and powerful tools for\nsimplification become available. The metric for simplification as understood by\nthis command is sampling efficiency. \nSimplify\n attempts to be as conservative\nas possible in changing the given program. In particular, it should not change\nterms unless an improvement with respect to sampling is performed; in this case,\narbitrary rearrangement may happen, even if an expression more similair to the\noriginal could be produced.\n\n\nSimplify\n is the default subcommand.\n\n\nSimplify\n preserves the semantics of the given program up to normalization of\nweights. If the stronger sense of equivalence is needed, the output of\n\nSimplify\n can be passed to \nnormalize\n. \n\n\nDisintegrate\n\n\nThe Maple disintegrator is an alternative implementation of the program\ntransformation described in [Disintegrate]. Semantically, the Maple\ndisintegrator and Haskell disintegrator implement the same transformation. In\nparticular, their outputs are not (often) identical, but have equivalent\nsampling semantics. In practice, the ouputs may differ, since one may fail where\nthe other succeeds. \n\n\nIf in doubt about which disintegrator to use, consider the following order:\n\n\n\n\ndisintegrate x\n\n\ndisintegrate x | hk-maple -\n\n\nhk-maple --command disintegrate x\n\n\nhk-maple x | disintegrate -\n \n\n\netc\u2026\n\n\n\n\nAll of the above programs should be equivalent as samplers. \n\n\nThe disintegrator internally relies heavily on the \nSimplify\n command, so if the\ngiven problem is an easy disintegration problem but a difficult simplification\nproblem, it is preferred to use the Haskell disintegrator followed by a call\nto \nSimplify\n.\n\n\nThe chance that the Maple disintegrator produces a good program (or any program\nat all) is proportional to the type of program it is given. In addition to\nprograms whose disintegration by Haskell is not efficient as a sampler, the\nfollowing programs are good candidates:\n\n\n\n\nprograms which contain superpositions with complicated conditions\n\n\nprograms which contain complicated rational polynomials \n\n\n\n\nThe Maple disintegrator follows the same conventions as the Haskell\ndisintegrator.\n\n\nLike \nSimplify\n, \nDisintegrate\n preserves the semantics of the given program only up to\nnormalization of weights.\n\n\nSummarize\n\n\n?\n\n\nExample\n\n\nThis program takes in a value of type \nprob\n and returns a measure of type \nreal\n:\n\n\nfn a prob:\n  x <~ normal(a,1)\n  y <~ normal(x,1)\n  z <~ normal(y,1)\n  return z\n\n\n\n\nThe returned value, \nz\n, is generated by passing the last value generated by the function, starting with the original function argument. This indicates that it might be \nreducible to a smaller program. Assuming that we named the program \nsimplify_before.hk\n, we can call the \nsimplify\n transform by running:\n\n\nsimplify simplify_before.hk\n\n\n\n\nNote:\n The output for \nsimplify\n will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling \n\nsimplify model1.hk > model2.hk\n. For this example, we will call our new program \nsimplify_after.hk\n. \n\n\nWhen you open our new program, \nsimplify_after.hk\n, you will see that the original five-line program has been reduced to a single line:\n\n\nfn a prob: normal(prob2real(a), sqrt(3))",
            "title": "Simplify"
        },
        {
            "location": "/transforms/simplify/#hakaru-maple",
            "text": "Hakaru uses the computer algebra system Maple to aid in performing program\ntransformations. You can use this functionality of Hakaru if you have Maple\ninstalled locally or can access Maple remotely.   Maple can be accessed through the module  Language.Hakaru.Maple  or through the\nHakaru program  hk-maple .  The  hk-maple  command invokes a Maple command on a Hakaru program. Given a\nHakaru program in concrete syntax and a Maple-Hakaru command, typecheck the\nprogram invoke the Maple command on the program and its type pretty print, parse\nand typecheck the program resulting from Maple. See the  --help  flag of hk-maple  for more information.  The currently available Maple-Hakaru commands (also called subcommands):   Simplify   Disintegrate   Summarize    Note: calls to Maple may take a very long time. To see if your program is taking\nan appreciable amount of time to parse and typecheck, use the  --debug  flag.",
            "title": "Hakaru-Maple"
        },
        {
            "location": "/transforms/simplify/#subcommands",
            "text": "",
            "title": "Subcommands"
        },
        {
            "location": "/transforms/simplify/#simplify",
            "text": "Hakaru programs are interpreted by Maple as linear operators. In this\ninterpretation, many commonly understood (by Maple) and powerful tools for\nsimplification become available. The metric for simplification as understood by\nthis command is sampling efficiency.  Simplify  attempts to be as conservative\nas possible in changing the given program. In particular, it should not change\nterms unless an improvement with respect to sampling is performed; in this case,\narbitrary rearrangement may happen, even if an expression more similair to the\noriginal could be produced.  Simplify  is the default subcommand.  Simplify  preserves the semantics of the given program up to normalization of\nweights. If the stronger sense of equivalence is needed, the output of Simplify  can be passed to  normalize .",
            "title": "Simplify"
        },
        {
            "location": "/transforms/simplify/#disintegrate",
            "text": "The Maple disintegrator is an alternative implementation of the program\ntransformation described in [Disintegrate]. Semantically, the Maple\ndisintegrator and Haskell disintegrator implement the same transformation. In\nparticular, their outputs are not (often) identical, but have equivalent\nsampling semantics. In practice, the ouputs may differ, since one may fail where\nthe other succeeds.   If in doubt about which disintegrator to use, consider the following order:   disintegrate x  disintegrate x | hk-maple -  hk-maple --command disintegrate x  hk-maple x | disintegrate -    etc\u2026   All of the above programs should be equivalent as samplers.   The disintegrator internally relies heavily on the  Simplify  command, so if the\ngiven problem is an easy disintegration problem but a difficult simplification\nproblem, it is preferred to use the Haskell disintegrator followed by a call\nto  Simplify .  The chance that the Maple disintegrator produces a good program (or any program\nat all) is proportional to the type of program it is given. In addition to\nprograms whose disintegration by Haskell is not efficient as a sampler, the\nfollowing programs are good candidates:   programs which contain superpositions with complicated conditions  programs which contain complicated rational polynomials    The Maple disintegrator follows the same conventions as the Haskell\ndisintegrator.  Like  Simplify ,  Disintegrate  preserves the semantics of the given program only up to\nnormalization of weights.",
            "title": "Disintegrate"
        },
        {
            "location": "/transforms/simplify/#summarize",
            "text": "?",
            "title": "Summarize"
        },
        {
            "location": "/transforms/simplify/#example",
            "text": "This program takes in a value of type  prob  and returns a measure of type  real :  fn a prob:\n  x <~ normal(a,1)\n  y <~ normal(x,1)\n  z <~ normal(y,1)\n  return z  The returned value,  z , is generated by passing the last value generated by the function, starting with the original function argument. This indicates that it might be \nreducible to a smaller program. Assuming that we named the program  simplify_before.hk , we can call the  simplify  transform by running:  simplify simplify_before.hk  Note:  The output for  simplify  will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling  simplify model1.hk > model2.hk . For this example, we will call our new program  simplify_after.hk .   When you open our new program,  simplify_after.hk , you will see that the original five-line program has been reduced to a single line:  fn a prob: normal(prob2real(a), sqrt(3))",
            "title": "Example"
        },
        {
            "location": "/transforms/mh/",
            "text": "Metropolis Hastings\n\n\nIn Hakaru, all inference algorithms are represented as program\ntransformations. In particular, the Metropolis-Hastings transform\ntakes as input a probabilistic program representing the target\ndistribution, and a probabilistic program representing the proposal\ndistribution and returns a probabilistic program representing the MH\ntransition kernel.\n\n\nmh command\n\n\nYou can access this functionality using the \nmh\n command. It takes\ntwo files as input representing the target distribution and proposal\nkernel.\n\n\nFor example, suppose we would like to make a Markov Chain for the\nnormal distribution, where the proposal distribution is a random walk.\n\n\nTarget\n\n\n# target.hk\nnormal(0,1)\n\n\n\n\nProposal\n\n\n# proposal.hk\nfn x real: normal(x, 0.04)\n\n\n\n\nWe can use \nmh\n to create a transition kernel.\n\n\nmh target.hk proposal.hk\n\nx5 = x2 = fn x0 real: \n           (exp((negate(((x0 - nat2real(0)) ^ 2))\n                  / \n                 prob2real((2 * (nat2prob(1) ^ 2)))))\n             / \n            nat2prob(1)\n             / \n            sqrt((2 * pi))\n             / \n            1)\n     fn x1 real: \n      x0 <~ normal(x1, (1/25))\n      return (x0, (x2(x0) / x2(x1)))\nfn x4 real: \n x3 <~ x5(x4)\n (match x3: \n   (x1, x2): \n    x0 <~ weight(min(1, x2), return true) <|> \n          weight(real2prob((prob2real(1) - prob2real(min(1, x2)))),\n                 return false)\n    return (match x0: \n             true: x1\n             false: x4))\n\n\n\n\n\nThis can then be simplified.\n\n\nmh target.hk proposal.hk | simplify -\n\nfn x4 real: \n x03 <~ normal(x4, (1/25))\n weight(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))),\n        return x03) <|> \n weight(real2prob((1\n                    + \n                   (prob2real(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))))\n                     * \n                    (-1)))),\n        return x4)\n\n\n\n\nThis can then be run using \nhakaru\n. Hakaru when run with two\narguments will assume that the first file is a transition kernel,\nand the second file represents a measure to initialize from.\n\n\nmh target.hk proposal.hk | simplify - | hakaru --transition-kernel - target.hk | head\n\n-0.6133542972818671\n-0.6111567543723275\n-0.5963756142974966\n-0.5661156231637984\n-0.6280335079595971\n-0.616432866701967\n-0.6053631512209712\n-0.5964839795872353\n-0.6020821843203473\n-0.6535246137595148",
            "title": "Metropolis Hastings"
        },
        {
            "location": "/transforms/mh/#metropolis-hastings",
            "text": "In Hakaru, all inference algorithms are represented as program\ntransformations. In particular, the Metropolis-Hastings transform\ntakes as input a probabilistic program representing the target\ndistribution, and a probabilistic program representing the proposal\ndistribution and returns a probabilistic program representing the MH\ntransition kernel.",
            "title": "Metropolis Hastings"
        },
        {
            "location": "/transforms/mh/#mh-command",
            "text": "You can access this functionality using the  mh  command. It takes\ntwo files as input representing the target distribution and proposal\nkernel.  For example, suppose we would like to make a Markov Chain for the\nnormal distribution, where the proposal distribution is a random walk.",
            "title": "mh command"
        },
        {
            "location": "/transforms/mh/#target",
            "text": "# target.hk\nnormal(0,1)",
            "title": "Target"
        },
        {
            "location": "/transforms/mh/#proposal",
            "text": "# proposal.hk\nfn x real: normal(x, 0.04)  We can use  mh  to create a transition kernel.  mh target.hk proposal.hk\n\nx5 = x2 = fn x0 real: \n           (exp((negate(((x0 - nat2real(0)) ^ 2))\n                  / \n                 prob2real((2 * (nat2prob(1) ^ 2)))))\n             / \n            nat2prob(1)\n             / \n            sqrt((2 * pi))\n             / \n            1)\n     fn x1 real: \n      x0 <~ normal(x1, (1/25))\n      return (x0, (x2(x0) / x2(x1)))\nfn x4 real: \n x3 <~ x5(x4)\n (match x3: \n   (x1, x2): \n    x0 <~ weight(min(1, x2), return true) <|> \n          weight(real2prob((prob2real(1) - prob2real(min(1, x2)))),\n                 return false)\n    return (match x0: \n             true: x1\n             false: x4))  This can then be simplified.  mh target.hk proposal.hk | simplify -\n\nfn x4 real: \n x03 <~ normal(x4, (1/25))\n weight(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))),\n        return x03) <|> \n weight(real2prob((1\n                    + \n                   (prob2real(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))))\n                     * \n                    (-1)))),\n        return x4)  This can then be run using  hakaru . Hakaru when run with two\narguments will assume that the first file is a transition kernel,\nand the second file represents a measure to initialize from.  mh target.hk proposal.hk | simplify - | hakaru --transition-kernel - target.hk | head\n\n-0.6133542972818671\n-0.6111567543723275\n-0.5963756142974966\n-0.5661156231637984\n-0.6280335079595971\n-0.616432866701967\n-0.6053631512209712\n-0.5964839795872353\n-0.6020821843203473\n-0.6535246137595148",
            "title": "Proposal"
        },
        {
            "location": "/internals/ast/",
            "text": "Internal Representation of Hakaru terms\n\n\nThe Hakaru AST can be found defined in\n\nhaskell/Language/Hakaru/Syntax/AST.hs\n. It is made up of several parts which this section and the next one will explain.\n\n\nWe should note, this datatype makes use of\n\nAbstract Binding Trees\n\nwhich we discuss in more detail in the next\n\nsection\n. ABTs can be understood as a way to abstract\nthe use of variables in the AST. The advantage of this is it allows\nall variable substitution and manipulation logic to live in one place\nand not be specific to a particular AST.\n\n\nDatakind\n\n\nThe AST is typed using the Hakaru kind, defined in \nhaskell/Language/Types/DataKind.hs\n. All Hakaru types are defined in terms of\nthe primitives in this datakind.\n\n\n-- | The universe\\/kind of Hakaru types.\ndata Hakaru\n    = HNat -- ^ The natural numbers; aka, the non-negative integers.\n\n    -- | The integers.\n    | HInt\n\n    -- | Non-negative real numbers. Unlike what you might expect,\n    -- this is /not/ restructed to the @[0,1]@ interval!\n    | HProb\n\n    -- | The affinely extended real number line. That is, the real\n    -- numbers extended with positive and negative infinities.\n    | HReal\n\n    -- | The measure monad\n    | HMeasure !Hakaru\n\n    -- | The built-in type for uniform arrays.\n    | HArray !Hakaru\n\n    -- | The type of Hakaru functions.\n    | !Hakaru :-> !Hakaru\n\n    -- | A user-defined polynomial datatype. Each such type is\n    -- specified by a \\\"tag\\\" (the @HakaruCon@) which names the type, and a sum-of-product representation of the type itself.\n    | HData !HakaruCon [[HakaruFun]]\n\n\n\n\n\nPlease read Datakind.hs for more details.\n\n\nTerm\n\n\nThe Term datatype includes all the syntactic constructions for the Hakaru language.\nFor all those where we know the number of arguments we expect that language construct\nto get, we define the \n(:$)\n constructor, which takes \nSCons\n and \nSArgs\n datatypes\nas arguments.\n\n\n-- | The generating functor for Hakaru ASTs. This type is given in\n-- open-recursive form, where the first type argument gives the\n-- recursive form. The recursive form @abt@ does not have exactly\n-- the same kind as @Term abt@ because every 'Term' represents a\n-- locally-closed term whereas the underlying @abt@ may bind some\n-- variables.\ndata Term :: ([Hakaru] -> Hakaru -> *) -> Hakaru -> * where\n    -- Simple syntactic forms (i.e., generalized quantifiers)\n    (:$) :: !(SCon args a) -> !(SArgs abt args) -> Term abt a\n\n    -- N-ary operators\n    NaryOp_ :: !(NaryOp a) -> !(Seq (abt '[] a)) -> Term abt a\n\n    -- Literal\\/Constant values\n    Literal_ :: !(Literal a) -> Term abt a\n\n    Empty_ :: !(Sing ('HArray a)) -> Term abt ('HArray a)\n    Array_\n        :: !(abt '[] 'HNat)\n        -> !(abt '[ 'HNat ] a)\n        -> Term abt ('HArray a)\n\n    -- -- User-defined data types\n    -- A data constructor applied to some expressions. N.B., this\n    -- definition only accounts for data constructors which are\n    -- fully saturated. Unsaturated constructors will need to be\n    -- eta-expanded.\n    Datum_ :: !(Datum (abt '[]) (HData' t)) -> Term abt (HData' t)\n\n    -- Generic case-analysis (via ABTs and Structural Focalization).\n    Case_ :: !(abt '[] a) -> [Branch a abt b] -> Term abt b\n\n    -- Linear combinations of measures.\n    Superpose_\n        :: L.NonEmpty (abt '[] 'HProb, abt '[] ('HMeasure a))\n        -> Term abt ('HMeasure a)\n\n    Reject_ :: !(Sing ('HMeasure a)) -> Term abt ('HMeasure a)\n\n\n\n\nSCons and SArgs\n\n\nWhen using \n(:$)\n we have a way to describe primitives where we\nknow the number of arguments they should get. In that regard,\nSArgs is a typed list of abt terms indexed by its size.\n\n\n-- | The arguments to a @(':$')@ node in the 'Term'; that is, a list\n-- of ASTs, where the whole list is indexed by a (type-level) list\n-- of the indices of each element.\ndata SArgs :: ([Hakaru] -> Hakaru -> *) -> [([Hakaru], Hakaru)] -> *\n    where\n    End :: SArgs abt '[]\n    (:*) :: !(abt vars a)\n        -> !(SArgs abt args)\n        -> SArgs abt ( '(vars, a) ': args)\n\n\n\n\nThese are combined with SCons which describes the constructor, and\nthe types it expects for its arguments. For example suppose we had\nan AST for a function \nf\n and it\u2019s argument \nx\n, we could construct\na Term for applying \nf\n to \nx\n by writing \nApp_:$ f :* x :* End\n.\n\n\n-- | The constructor of a @(':$')@ node in the 'Term'. Each of these\n-- constructors denotes a \\\"normal\\/standard\\/basic\\\" syntactic\n-- form (i.e., a generalized quantifier). In the literature, these\n-- syntactic forms are sometimes called \\\"operators\\\", but we avoid\n-- calling them that so as not to introduce confusion vs 'PrimOp'\n-- etc. Instead we use the term \\\"operator\\\" to refer to any primitive\n-- function or constant; that is, non-binding syntactic forms. Also\n-- in the literature, the 'SCon' type itself is usually called the\n-- \\\"signature\\\" of the term language. However, we avoid calling\n-- it that since our 'Term' has constructors other than just @(:$)@,\n-- so 'SCon' does not give a complete signature for our terms.\n--\n-- The main reason for breaking this type out and using it in\n-- conjunction with @(':$')@ and 'SArgs' is so that we can easily\n-- pattern match on /fully saturated/ nodes. For example, we want\n-- to be able to match @MeasureOp_ Uniform :$ lo :* hi :* End@\n-- without needing to deal with 'App_' nodes nor 'viewABT'.\ndata SCon :: [([Hakaru], Hakaru)] -> Hakaru -> * where\n    Lam_ :: SCon '[ '( '[ a ], b ) ] (a ':-> b)\n    App_ :: SCon '[ LC (a ':-> b ), LC a ] b\n    Let_ :: SCon '[ LC a, '( '[ a ], b ) ] b\n\n    CoerceTo_   :: !(Coercion a b) -> SCon '[ LC a ] b\n    UnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a\n\n    PrimOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(PrimOp typs a) -> SCon args a\n    ArrayOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(ArrayOp typs a) -> SCon args a\n    MeasureOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(MeasureOp typs a) -> SCon args ('HMeasure a)\n\n    Dirac :: SCon '[ LC a ] ('HMeasure a)\n\n    MBind :: SCon\n        '[ LC ('HMeasure a)\n        ,  '( '[ a ], 'HMeasure b)\n        ] ('HMeasure b)\n\n    Plate :: SCon\n        '[ LC 'HNat\n        , '( '[ 'HNat ], 'HMeasure a)\n        ] ('HMeasure ('HArray a))\n\n    Chain :: SCon\n        '[ LC 'HNat, LC s\n        , '( '[ s ],  'HMeasure (HPair a s))\n        ] ('HMeasure (HPair ('HArray a) s))\n\n    Integrate\n        :: SCon '[ LC 'HReal, LC 'HReal, '( '[ 'HReal ], 'HProb) ] 'HProb\n\n    Summate\n        :: HDiscrete a\n        -> HSemiring b\n        -> SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Product\n        :: HDiscrete a\n        -> HSemiring b\n        -> SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Expect :: SCon '[ LC ('HMeasure a), '( '[ a ], 'HProb) ] 'HProb\n\n    Observe :: SCon '[ LC ('HMeasure a), LC a ] ('HMeasure a)\n\n\n\n\nYou\u2019ll notice in \nSCon\n there are definitions for PrimOp, MeasureOp, and ArrayOp\nthese are done more organizational purposes and have constructions for the\ndifferent categories of primitives.\n\n\nMeasureOp\n\n\nPrimitives of type measure are defined in MeasureOp.\n\n\n-- | Primitive operators to produce, consume, or transform\n-- distributions\\/measures. This corresponds to the old @Mochastic@\n-- class, except that 'MBind' and 'Superpose_' are handled elsewhere\n-- since they are not simple operators. (Also 'Dirac' is handled\n-- elsewhere since it naturally fits with 'MBind', even though it\n-- is a siple operator.)\ndata MeasureOp :: [Hakaru] -> Hakaru -> * where\n    Lebesgue    :: MeasureOp '[]                 'HReal\n    Counting    :: MeasureOp '[]                 'HInt\n    Categorical :: MeasureOp '[ 'HArray 'HProb ] 'HNat\n    Uniform     :: MeasureOp '[ 'HReal, 'HReal ] 'HReal\n    Normal      :: MeasureOp '[ 'HReal, 'HProb ] 'HReal\n    Poisson     :: MeasureOp '[ 'HProb         ] 'HNat\n    Gamma       :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n    Beta        :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n\n\n\n\nArrayOp\n\n\nPrimitives that involve manipulating value of type array,\nend up in ArrayOp.\n\n\n-- | Primitive operators for consuming or transforming arrays.\ndata ArrayOp :: [Hakaru] -> Hakaru -> * where\n    Index  :: !(Sing a) -> ArrayOp '[ 'HArray a, 'HNat ] a\n    Size   :: !(Sing a) -> ArrayOp '[ 'HArray a ] 'HNat\n    Reduce :: !(Sing a) -> ArrayOp '[ a ':-> a ':-> a, a, 'HArray a ] a\n\n\n\n\nPrimOp\n\n\nAll primitive operations which don\u2019t return something\nof type array or measure are placed in PrimOp\n\n\n-- | Simple primitive functions, and constants.\ndata PrimOp :: [Hakaru] -> Hakaru -> * where\n\n    -- -- -- Here we have /monomorphic/ operators\n    -- -- The Boolean operators\n    Not  :: PrimOp '[ HBool ] HBool\n    -- And, Or, Xor, Iff\n    Impl :: PrimOp '[ HBool, HBool ] HBool\n    -- Impl x y == Or (Not x) y\n    Diff :: PrimOp '[ HBool, HBool ] HBool\n    -- Diff x y == Not (Impl x y)\n    Nand :: PrimOp '[ HBool, HBool ] HBool\n    -- Nand aka Alternative Denial, Sheffer stroke\n    Nor  :: PrimOp '[ HBool, HBool ] HBool\n    -- Nor aka Joint Denial, aka Quine dagger, aka Pierce arrow\n\n    -- -- Trigonometry operators\n    Pi    :: PrimOp '[] 'HProb\n    Sin   :: PrimOp '[ 'HReal ] 'HReal\n    Cos   :: PrimOp '[ 'HReal ] 'HReal\n    Tan   :: PrimOp '[ 'HReal ] 'HReal\n    Asin  :: PrimOp '[ 'HReal ] 'HReal\n    Acos  :: PrimOp '[ 'HReal ] 'HReal\n    Atan  :: PrimOp '[ 'HReal ] 'HReal\n    Sinh  :: PrimOp '[ 'HReal ] 'HReal\n    Cosh  :: PrimOp '[ 'HReal ] 'HReal\n    Tanh  :: PrimOp '[ 'HReal ] 'HReal\n    Asinh :: PrimOp '[ 'HReal ] 'HReal\n    Acosh :: PrimOp '[ 'HReal ] 'HReal\n    Atanh :: PrimOp '[ 'HReal ] 'HReal\n\n    -- -- Other Real\\/Prob-valued operators\n    RealPow   :: PrimOp '[ 'HProb, 'HReal ] 'HProb\n    Exp       :: PrimOp '[ 'HReal ] 'HProb\n    Log       :: PrimOp '[ 'HProb ] 'HReal\n    Infinity  :: HIntegrable a -> PrimOp '[] a\n    GammaFunc :: PrimOp '[ 'HReal ] 'HProb\n    BetaFunc  :: PrimOp '[ 'HProb, 'HProb ] 'HProb\n\n    -- -- -- Here we have the /polymorphic/ operators\n    -- -- HEq and HOrd operators\n    Equal :: !(HEq  a) -> PrimOp '[ a, a ] HBool\n    Less  :: !(HOrd a) -> PrimOp '[ a, a ] HBool\n\n    -- -- HSemiring operators (the non-n-ary ones)\n    NatPow :: !(HSemiring a) -> PrimOp '[ a, 'HNat ] a\n\n    -- -- HRing operators\n    Negate :: !(HRing a) -> PrimOp '[ a ] a\n    Abs    :: !(HRing a) -> PrimOp '[ a ] (NonNegative a)\n    Signum :: !(HRing a) -> PrimOp '[ a ] a\n\n    -- -- HFractional operators\n    Recip :: !(HFractional a) -> PrimOp '[ a ] a\n\n    -- -- HRadical operators\n    NatRoot :: !(HRadical a) -> PrimOp '[ a, 'HNat ] a\n\n    -- -- HContinuous operators\n    Erf :: !(HContinuous a) -> PrimOp '[ a ] a",
            "title": "AST and Hakaru Datakind"
        },
        {
            "location": "/internals/ast/#internal-representation-of-hakaru-terms",
            "text": "The Hakaru AST can be found defined in haskell/Language/Hakaru/Syntax/AST.hs . It is made up of several parts which this section and the next one will explain.  We should note, this datatype makes use of Abstract Binding Trees \nwhich we discuss in more detail in the next section . ABTs can be understood as a way to abstract\nthe use of variables in the AST. The advantage of this is it allows\nall variable substitution and manipulation logic to live in one place\nand not be specific to a particular AST.",
            "title": "Internal Representation of Hakaru terms"
        },
        {
            "location": "/internals/ast/#datakind",
            "text": "The AST is typed using the Hakaru kind, defined in  haskell/Language/Types/DataKind.hs . All Hakaru types are defined in terms of\nthe primitives in this datakind.  -- | The universe\\/kind of Hakaru types.\ndata Hakaru\n    = HNat -- ^ The natural numbers; aka, the non-negative integers.\n\n    -- | The integers.\n    | HInt\n\n    -- | Non-negative real numbers. Unlike what you might expect,\n    -- this is /not/ restructed to the @[0,1]@ interval!\n    | HProb\n\n    -- | The affinely extended real number line. That is, the real\n    -- numbers extended with positive and negative infinities.\n    | HReal\n\n    -- | The measure monad\n    | HMeasure !Hakaru\n\n    -- | The built-in type for uniform arrays.\n    | HArray !Hakaru\n\n    -- | The type of Hakaru functions.\n    | !Hakaru :-> !Hakaru\n\n    -- | A user-defined polynomial datatype. Each such type is\n    -- specified by a \\\"tag\\\" (the @HakaruCon@) which names the type, and a sum-of-product representation of the type itself.\n    | HData !HakaruCon [[HakaruFun]]  Please read Datakind.hs for more details.",
            "title": "Datakind"
        },
        {
            "location": "/internals/ast/#term",
            "text": "The Term datatype includes all the syntactic constructions for the Hakaru language.\nFor all those where we know the number of arguments we expect that language construct\nto get, we define the  (:$)  constructor, which takes  SCons  and  SArgs  datatypes\nas arguments.  -- | The generating functor for Hakaru ASTs. This type is given in\n-- open-recursive form, where the first type argument gives the\n-- recursive form. The recursive form @abt@ does not have exactly\n-- the same kind as @Term abt@ because every 'Term' represents a\n-- locally-closed term whereas the underlying @abt@ may bind some\n-- variables.\ndata Term :: ([Hakaru] -> Hakaru -> *) -> Hakaru -> * where\n    -- Simple syntactic forms (i.e., generalized quantifiers)\n    (:$) :: !(SCon args a) -> !(SArgs abt args) -> Term abt a\n\n    -- N-ary operators\n    NaryOp_ :: !(NaryOp a) -> !(Seq (abt '[] a)) -> Term abt a\n\n    -- Literal\\/Constant values\n    Literal_ :: !(Literal a) -> Term abt a\n\n    Empty_ :: !(Sing ('HArray a)) -> Term abt ('HArray a)\n    Array_\n        :: !(abt '[] 'HNat)\n        -> !(abt '[ 'HNat ] a)\n        -> Term abt ('HArray a)\n\n    -- -- User-defined data types\n    -- A data constructor applied to some expressions. N.B., this\n    -- definition only accounts for data constructors which are\n    -- fully saturated. Unsaturated constructors will need to be\n    -- eta-expanded.\n    Datum_ :: !(Datum (abt '[]) (HData' t)) -> Term abt (HData' t)\n\n    -- Generic case-analysis (via ABTs and Structural Focalization).\n    Case_ :: !(abt '[] a) -> [Branch a abt b] -> Term abt b\n\n    -- Linear combinations of measures.\n    Superpose_\n        :: L.NonEmpty (abt '[] 'HProb, abt '[] ('HMeasure a))\n        -> Term abt ('HMeasure a)\n\n    Reject_ :: !(Sing ('HMeasure a)) -> Term abt ('HMeasure a)",
            "title": "Term"
        },
        {
            "location": "/internals/ast/#scons-and-sargs",
            "text": "When using  (:$)  we have a way to describe primitives where we\nknow the number of arguments they should get. In that regard,\nSArgs is a typed list of abt terms indexed by its size.  -- | The arguments to a @(':$')@ node in the 'Term'; that is, a list\n-- of ASTs, where the whole list is indexed by a (type-level) list\n-- of the indices of each element.\ndata SArgs :: ([Hakaru] -> Hakaru -> *) -> [([Hakaru], Hakaru)] -> *\n    where\n    End :: SArgs abt '[]\n    (:*) :: !(abt vars a)\n        -> !(SArgs abt args)\n        -> SArgs abt ( '(vars, a) ': args)  These are combined with SCons which describes the constructor, and\nthe types it expects for its arguments. For example suppose we had\nan AST for a function  f  and it\u2019s argument  x , we could construct\na Term for applying  f  to  x  by writing  App_:$ f :* x :* End .  -- | The constructor of a @(':$')@ node in the 'Term'. Each of these\n-- constructors denotes a \\\"normal\\/standard\\/basic\\\" syntactic\n-- form (i.e., a generalized quantifier). In the literature, these\n-- syntactic forms are sometimes called \\\"operators\\\", but we avoid\n-- calling them that so as not to introduce confusion vs 'PrimOp'\n-- etc. Instead we use the term \\\"operator\\\" to refer to any primitive\n-- function or constant; that is, non-binding syntactic forms. Also\n-- in the literature, the 'SCon' type itself is usually called the\n-- \\\"signature\\\" of the term language. However, we avoid calling\n-- it that since our 'Term' has constructors other than just @(:$)@,\n-- so 'SCon' does not give a complete signature for our terms.\n--\n-- The main reason for breaking this type out and using it in\n-- conjunction with @(':$')@ and 'SArgs' is so that we can easily\n-- pattern match on /fully saturated/ nodes. For example, we want\n-- to be able to match @MeasureOp_ Uniform :$ lo :* hi :* End@\n-- without needing to deal with 'App_' nodes nor 'viewABT'.\ndata SCon :: [([Hakaru], Hakaru)] -> Hakaru -> * where\n    Lam_ :: SCon '[ '( '[ a ], b ) ] (a ':-> b)\n    App_ :: SCon '[ LC (a ':-> b ), LC a ] b\n    Let_ :: SCon '[ LC a, '( '[ a ], b ) ] b\n\n    CoerceTo_   :: !(Coercion a b) -> SCon '[ LC a ] b\n    UnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a\n\n    PrimOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(PrimOp typs a) -> SCon args a\n    ArrayOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(ArrayOp typs a) -> SCon args a\n    MeasureOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(MeasureOp typs a) -> SCon args ('HMeasure a)\n\n    Dirac :: SCon '[ LC a ] ('HMeasure a)\n\n    MBind :: SCon\n        '[ LC ('HMeasure a)\n        ,  '( '[ a ], 'HMeasure b)\n        ] ('HMeasure b)\n\n    Plate :: SCon\n        '[ LC 'HNat\n        , '( '[ 'HNat ], 'HMeasure a)\n        ] ('HMeasure ('HArray a))\n\n    Chain :: SCon\n        '[ LC 'HNat, LC s\n        , '( '[ s ],  'HMeasure (HPair a s))\n        ] ('HMeasure (HPair ('HArray a) s))\n\n    Integrate\n        :: SCon '[ LC 'HReal, LC 'HReal, '( '[ 'HReal ], 'HProb) ] 'HProb\n\n    Summate\n        :: HDiscrete a\n        -> HSemiring b\n        -> SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Product\n        :: HDiscrete a\n        -> HSemiring b\n        -> SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Expect :: SCon '[ LC ('HMeasure a), '( '[ a ], 'HProb) ] 'HProb\n\n    Observe :: SCon '[ LC ('HMeasure a), LC a ] ('HMeasure a)  You\u2019ll notice in  SCon  there are definitions for PrimOp, MeasureOp, and ArrayOp\nthese are done more organizational purposes and have constructions for the\ndifferent categories of primitives.",
            "title": "SCons and SArgs"
        },
        {
            "location": "/internals/ast/#measureop",
            "text": "Primitives of type measure are defined in MeasureOp.  -- | Primitive operators to produce, consume, or transform\n-- distributions\\/measures. This corresponds to the old @Mochastic@\n-- class, except that 'MBind' and 'Superpose_' are handled elsewhere\n-- since they are not simple operators. (Also 'Dirac' is handled\n-- elsewhere since it naturally fits with 'MBind', even though it\n-- is a siple operator.)\ndata MeasureOp :: [Hakaru] -> Hakaru -> * where\n    Lebesgue    :: MeasureOp '[]                 'HReal\n    Counting    :: MeasureOp '[]                 'HInt\n    Categorical :: MeasureOp '[ 'HArray 'HProb ] 'HNat\n    Uniform     :: MeasureOp '[ 'HReal, 'HReal ] 'HReal\n    Normal      :: MeasureOp '[ 'HReal, 'HProb ] 'HReal\n    Poisson     :: MeasureOp '[ 'HProb         ] 'HNat\n    Gamma       :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n    Beta        :: MeasureOp '[ 'HProb, 'HProb ] 'HProb",
            "title": "MeasureOp"
        },
        {
            "location": "/internals/ast/#arrayop",
            "text": "Primitives that involve manipulating value of type array,\nend up in ArrayOp.  -- | Primitive operators for consuming or transforming arrays.\ndata ArrayOp :: [Hakaru] -> Hakaru -> * where\n    Index  :: !(Sing a) -> ArrayOp '[ 'HArray a, 'HNat ] a\n    Size   :: !(Sing a) -> ArrayOp '[ 'HArray a ] 'HNat\n    Reduce :: !(Sing a) -> ArrayOp '[ a ':-> a ':-> a, a, 'HArray a ] a",
            "title": "ArrayOp"
        },
        {
            "location": "/internals/ast/#primop",
            "text": "All primitive operations which don\u2019t return something\nof type array or measure are placed in PrimOp  -- | Simple primitive functions, and constants.\ndata PrimOp :: [Hakaru] -> Hakaru -> * where\n\n    -- -- -- Here we have /monomorphic/ operators\n    -- -- The Boolean operators\n    Not  :: PrimOp '[ HBool ] HBool\n    -- And, Or, Xor, Iff\n    Impl :: PrimOp '[ HBool, HBool ] HBool\n    -- Impl x y == Or (Not x) y\n    Diff :: PrimOp '[ HBool, HBool ] HBool\n    -- Diff x y == Not (Impl x y)\n    Nand :: PrimOp '[ HBool, HBool ] HBool\n    -- Nand aka Alternative Denial, Sheffer stroke\n    Nor  :: PrimOp '[ HBool, HBool ] HBool\n    -- Nor aka Joint Denial, aka Quine dagger, aka Pierce arrow\n\n    -- -- Trigonometry operators\n    Pi    :: PrimOp '[] 'HProb\n    Sin   :: PrimOp '[ 'HReal ] 'HReal\n    Cos   :: PrimOp '[ 'HReal ] 'HReal\n    Tan   :: PrimOp '[ 'HReal ] 'HReal\n    Asin  :: PrimOp '[ 'HReal ] 'HReal\n    Acos  :: PrimOp '[ 'HReal ] 'HReal\n    Atan  :: PrimOp '[ 'HReal ] 'HReal\n    Sinh  :: PrimOp '[ 'HReal ] 'HReal\n    Cosh  :: PrimOp '[ 'HReal ] 'HReal\n    Tanh  :: PrimOp '[ 'HReal ] 'HReal\n    Asinh :: PrimOp '[ 'HReal ] 'HReal\n    Acosh :: PrimOp '[ 'HReal ] 'HReal\n    Atanh :: PrimOp '[ 'HReal ] 'HReal\n\n    -- -- Other Real\\/Prob-valued operators\n    RealPow   :: PrimOp '[ 'HProb, 'HReal ] 'HProb\n    Exp       :: PrimOp '[ 'HReal ] 'HProb\n    Log       :: PrimOp '[ 'HProb ] 'HReal\n    Infinity  :: HIntegrable a -> PrimOp '[] a\n    GammaFunc :: PrimOp '[ 'HReal ] 'HProb\n    BetaFunc  :: PrimOp '[ 'HProb, 'HProb ] 'HProb\n\n    -- -- -- Here we have the /polymorphic/ operators\n    -- -- HEq and HOrd operators\n    Equal :: !(HEq  a) -> PrimOp '[ a, a ] HBool\n    Less  :: !(HOrd a) -> PrimOp '[ a, a ] HBool\n\n    -- -- HSemiring operators (the non-n-ary ones)\n    NatPow :: !(HSemiring a) -> PrimOp '[ a, 'HNat ] a\n\n    -- -- HRing operators\n    Negate :: !(HRing a) -> PrimOp '[ a ] a\n    Abs    :: !(HRing a) -> PrimOp '[ a ] (NonNegative a)\n    Signum :: !(HRing a) -> PrimOp '[ a ] a\n\n    -- -- HFractional operators\n    Recip :: !(HFractional a) -> PrimOp '[ a ] a\n\n    -- -- HRadical operators\n    NatRoot :: !(HRadical a) -> PrimOp '[ a, 'HNat ] a\n\n    -- -- HContinuous operators\n    Erf :: !(HContinuous a) -> PrimOp '[ a ] a",
            "title": "PrimOp"
        },
        {
            "location": "/internals/abt/",
            "text": "Abstract Binding Trees\n\n\nHakaru makes use of many program transformations in its codebase.\nBecause of this, a special mechanism is included for handing\nvariable bindings and substitutions. We abstract this into its\nown typeclass called \nABT\n. This can be found in \nLanguage.Hakaru.Syntax.ABT\n.\n\n\nBelow is an excerpt of this typeclass\n\n\nclass ABT (syn :: ([k] -> k -> *) -> k -> *) (abt :: [k] -> k -> *) | abt -> syn where\n    -- Smart constructors for building a 'View' and then injecting it into the @abt@.\n    syn  :: syn abt  a -> abt '[] a\n    var  :: Variable a -> abt '[] a\n    bind :: Variable a -> abt xs b -> abt (a ': xs) b\n    caseBind :: abt (x ': xs) a -> (Variable x -> abt xs a -> r) -> r\n    ...\n\n\n\n\nThe advantage of having this typeclass is that we think about variable binding\nindependently of the AST for our language. For example, we can define variable\nsubstitution once and for all.",
            "title": "ABT"
        },
        {
            "location": "/internals/abt/#abstract-binding-trees",
            "text": "Hakaru makes use of many program transformations in its codebase.\nBecause of this, a special mechanism is included for handing\nvariable bindings and substitutions. We abstract this into its\nown typeclass called  ABT . This can be found in  Language.Hakaru.Syntax.ABT .  Below is an excerpt of this typeclass  class ABT (syn :: ([k] -> k -> *) -> k -> *) (abt :: [k] -> k -> *) | abt -> syn where\n    -- Smart constructors for building a 'View' and then injecting it into the @abt@.\n    syn  :: syn abt  a -> abt '[] a\n    var  :: Variable a -> abt '[] a\n    bind :: Variable a -> abt xs b -> abt (a ': xs) b\n    caseBind :: abt (x ': xs) a -> (Variable x -> abt xs a -> r) -> r\n    ...  The advantage of having this typeclass is that we think about variable binding\nindependently of the AST for our language. For example, we can define variable\nsubstitution once and for all.",
            "title": "Abstract Binding Trees"
        },
        {
            "location": "/internals/datums/",
            "text": "Data representation\n\n\nData types are stored using a sum of product representation.\nThey can be found in \nLanguage.Hakaru.Syntax.Datum\n.\n\n\n-- The first component is a hint for what the data constructor\n-- should be called when pretty-printing, giving error messages,\n-- etc. Like the hints for variable names, its value is not actually\n-- used to decide which constructor is meant or which pattern\n-- matches.\ndata Datum :: (Hakaru -> *) -> Hakaru -> * where\n    Datum\n        :: {-# UNPACK #-} !Text\n        -> !(Sing (HData' t))\n        -> !(DatumCode (Code t) ast (HData' t))\n        -> Datum ast (HData' t)\n\n-- | The intermediate components of a data constructor. The intuition\n-- behind the two indices is that the @[[HakaruFun]]@ is a functor\n-- applied to the Hakaru type. Initially the @[[HakaruFun]]@ functor\n-- will be the 'Code' associated with the Hakaru type; hence it's\n-- the one-step unrolling of the fixed point for our recursive\n-- datatypes. But as we go along, we'll be doing induction on the\n-- @[[HakaruFun]]@ functor.\ndata DatumCode :: [[HakaruFun]] -> (Hakaru -> *) -> Hakaru -> * where\n    -- Skip rightwards along the sum.\n    Inr :: !(DatumCode  xss abt a) -> DatumCode (xs ': xss) abt a\n    -- Inject into the sum.\n    Inl :: !(DatumStruct xs abt a) -> DatumCode (xs ': xss) abt a\n\ndata DatumStruct :: [HakaruFun] -> (Hakaru -> *) -> Hakaru -> * where\n    -- BUG: haddock doesn't like annotations on GADT constructors\n    -- <https://github.com/hakaru-dev/hakaru/issues/6>\n\n    -- Combine components of the product. (\\\"et\\\" means \\\"and\\\" in Latin)\n    Et  :: !(DatumFun    x         abt a)\n        -> !(DatumStruct xs        abt a)\n        ->   DatumStruct (x ': xs) abt a\n\n    -- Close off the product.\n    Done :: DatumStruct '[] abt a\n\ndata DatumFun :: HakaruFun -> (Hakaru -> *) -> Hakaru -> * where\n    -- Hit a leaf which isn't a recursive component of the datatype.\n    Konst :: !(ast b) -> DatumFun ('K b) ast a\n    -- Hit a leaf which is a recursive component of the datatype.\n    Ident :: !(ast a) -> DatumFun 'I     ast a\n\n\n\n\nIn Hakaru we have implemented Bool, Pair, Either, Maybe, and List.",
            "title": "Datums"
        },
        {
            "location": "/internals/datums/#data-representation",
            "text": "Data types are stored using a sum of product representation.\nThey can be found in  Language.Hakaru.Syntax.Datum .  -- The first component is a hint for what the data constructor\n-- should be called when pretty-printing, giving error messages,\n-- etc. Like the hints for variable names, its value is not actually\n-- used to decide which constructor is meant or which pattern\n-- matches.\ndata Datum :: (Hakaru -> *) -> Hakaru -> * where\n    Datum\n        :: {-# UNPACK #-} !Text\n        -> !(Sing (HData' t))\n        -> !(DatumCode (Code t) ast (HData' t))\n        -> Datum ast (HData' t)\n\n-- | The intermediate components of a data constructor. The intuition\n-- behind the two indices is that the @[[HakaruFun]]@ is a functor\n-- applied to the Hakaru type. Initially the @[[HakaruFun]]@ functor\n-- will be the 'Code' associated with the Hakaru type; hence it's\n-- the one-step unrolling of the fixed point for our recursive\n-- datatypes. But as we go along, we'll be doing induction on the\n-- @[[HakaruFun]]@ functor.\ndata DatumCode :: [[HakaruFun]] -> (Hakaru -> *) -> Hakaru -> * where\n    -- Skip rightwards along the sum.\n    Inr :: !(DatumCode  xss abt a) -> DatumCode (xs ': xss) abt a\n    -- Inject into the sum.\n    Inl :: !(DatumStruct xs abt a) -> DatumCode (xs ': xss) abt a\n\ndata DatumStruct :: [HakaruFun] -> (Hakaru -> *) -> Hakaru -> * where\n    -- BUG: haddock doesn't like annotations on GADT constructors\n    -- <https://github.com/hakaru-dev/hakaru/issues/6>\n\n    -- Combine components of the product. (\\\"et\\\" means \\\"and\\\" in Latin)\n    Et  :: !(DatumFun    x         abt a)\n        -> !(DatumStruct xs        abt a)\n        ->   DatumStruct (x ': xs) abt a\n\n    -- Close off the product.\n    Done :: DatumStruct '[] abt a\n\ndata DatumFun :: HakaruFun -> (Hakaru -> *) -> Hakaru -> * where\n    -- Hit a leaf which isn't a recursive component of the datatype.\n    Konst :: !(ast b) -> DatumFun ('K b) ast a\n    -- Hit a leaf which is a recursive component of the datatype.\n    Ident :: !(ast a) -> DatumFun 'I     ast a  In Hakaru we have implemented Bool, Pair, Either, Maybe, and List.",
            "title": "Data representation"
        },
        {
            "location": "/internals/coercions/",
            "text": "Coercions\n\n\nFor convenience, Hakaru offers functions to convert between the four\ndifferent numeric types in the language. These types are\n\n\n\n\nnat - Natural numbers\n\n\nint - Integers\n\n\nprob - Positive real numbers\n\n\nreal - Real numbers\n\n\n\n\nAmongst these types there are a collection of safe and unsafe\ncoercions. A safe coercion is one which is always guaranteed to\nbe valid. For example, converting a \nnat\n to an \nint\n is always\nsafe. Converting an \nint\n to a \nnat\n is unsafe as the value can\nnegative, and lead to runtime errors.\n\n\nThese are represented in the AST using the \nCoerceTo\n and \nUnsafeFrom\n\nconstructors. Note that coercions are always defined in terms of the\nsafe direction to go to.\n\n\nCoerceTo_   :: !(Coercion a b) -> SCon '[ LC a ] b\nUnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a\n\n\n\n\nInternally, coercions are specified using the \nCoercion\n datatype. This\ndatatype states that each coercion is made up of a series of primitive\ncoercions.\n\n\ndata Coercion :: Hakaru -> Hakaru -> * where\n    CNil :: Coercion a a\n    CCons :: !(PrimCoercion a b) -> !(Coercion b c) -> Coercion a c\n\n\n\n\nThese primitive coercions can either involve loosening a restriction\non the sign of the value, or changing the numeric value to be over\na continuous value. For example, to coerce from int to real, we would\nhave a single \nCoercion\n with a \nPrimCoercion\n in it with the Continuous\ndata constructor.\n\n\ndata PrimCoercion :: Hakaru -> Hakaru -> * where\n    Signed     :: !(HRing a)       -> PrimCoercion (NonNegative a) a\n    Continuous :: !(HContinuous a) -> PrimCoercion (HIntegral   a) a",
            "title": "Coercions"
        },
        {
            "location": "/internals/coercions/#coercions",
            "text": "For convenience, Hakaru offers functions to convert between the four\ndifferent numeric types in the language. These types are   nat - Natural numbers  int - Integers  prob - Positive real numbers  real - Real numbers   Amongst these types there are a collection of safe and unsafe\ncoercions. A safe coercion is one which is always guaranteed to\nbe valid. For example, converting a  nat  to an  int  is always\nsafe. Converting an  int  to a  nat  is unsafe as the value can\nnegative, and lead to runtime errors.  These are represented in the AST using the  CoerceTo  and  UnsafeFrom \nconstructors. Note that coercions are always defined in terms of the\nsafe direction to go to.  CoerceTo_   :: !(Coercion a b) -> SCon '[ LC a ] b\nUnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a  Internally, coercions are specified using the  Coercion  datatype. This\ndatatype states that each coercion is made up of a series of primitive\ncoercions.  data Coercion :: Hakaru -> Hakaru -> * where\n    CNil :: Coercion a a\n    CCons :: !(PrimCoercion a b) -> !(Coercion b c) -> Coercion a c  These primitive coercions can either involve loosening a restriction\non the sign of the value, or changing the numeric value to be over\na continuous value. For example, to coerce from int to real, we would\nhave a single  Coercion  with a  PrimCoercion  in it with the Continuous\ndata constructor.  data PrimCoercion :: Hakaru -> Hakaru -> * where\n    Signed     :: !(HRing a)       -> PrimCoercion (NonNegative a) a\n    Continuous :: !(HContinuous a) -> PrimCoercion (HIntegral   a) a",
            "title": "Coercions"
        },
        {
            "location": "/internals/transforms/",
            "text": "Program transformations in Hakaru\n\n\nCoalesce\n\n\nCoalesce is an internal transformation that works on the untyped Hakaru AST. It\ntakes recursive \nNAryOp\n terms that have the same type and combines them into\na single term. For instance:\n\n\n3.0 + 1.5 + 0.3\n\n\n\n\nis parser as:\n\n\nNaryOp Sum [3.0, NaryOp Sum [1.5, NaryOp Sum [0.3]]]\n\n\n\n\nwhich when coalesced becomes:\n\n\nNaryOp Sum [3.0,1.5,0.3]\n\n\n\n\nOptimizations\n\n\nThe Hakaru AST has a suite of standard compiler optimizations which have\na substantial effect on the runtime of the resulting program.\nThe current pipeline is described by the \noptimizations\n variable in\n\nLanguage.Hakaru.Syntax.Transforms\n.\nIn order, the optimizations performed are:\n\n\n\n\nA-normalization\n\n\nUniquification of variables (needed for let-floating)\n\n\nLet-floating\n\n\nCommon subexpression elimination\n\n\nPruning of dead binders\n\n\nUniquification of variables (for the C backend)\n\n\nConstant Propagation\n\n\n\n\nEach pass is described in more detail below.\n\n\nA-normalization\n\n\nFound in \nLanguage.Hakaru.Syntax.ANF\n\n\nSee \nThe Essence of Compiling with Continuations by Flannigan, Sabry, Duba, and\nFelleisen\n\n\nA-normalization converts expressions into \nadministrative normal form\n (ANF).\nThis ensures that all intermediate values are named and all arguments to\nfunctions or primitive operations are either literals or variables.\nANF is a common program representation for functional language compilers which\ncan simplify some compiler passes and make others more effective.\nAs an example, consider\n\n\n(add1 (let ([x (f y)]) 5))\n\n\n\n\nThis expression in ANF looks like the following\n\n\n(let ([x (f y)]) (add1 5))\n\n\n\n\nwhich opens up the opportunity for constant folding to eliminate the \n(add1 5)\n\nexpression.\nThis pass exists mostly to simplify the implementation of CSE, but is useful for\nother passes as well.\n\n\nUniquification\n\n\nFound in \nLanguage.Hakaru.Syntax.Uniquify\n\n\nEnsures all variables in the program have unique variable identifiers.\nThis is not strictly necessary, but simplifies the implementation of other\npasses, several of which rely on this property.\n\n\nLet-floating\n\n\nFound in \nLanguage.Hakaru.Syntax.Hoist\n\n\nSee \nLet-Floating: Moving Bindings to Give Faster Programs (1996)\nby Simon Peyton Jones , Will Partain , Andr\u00e9 Santos\n\n\nLet-floating alters the bindings structure of the program in order to improve\nperformance.\nTypically, this entails moving definitions into or out of lambda expressions.\nWhen a lambda expression encodes a loop, this effectively accomplishes\nloop invariant code motion.\nThis pass only moves definitions upward in the AST.\nFor the most part, we are only interested in looping constructs like \nsummate\n and\n\nproduct\n, and moving \nsummate\n expressions out of other \nsummate\n or \nproduct\n\nexpressions when they do not depend on the index.\nThis can radically alter the asymptotics of the resulting program, as nested\nloops are converted into sequentially executed loops.\n\n\nThe only assumption this pass makes about the input AST is that all variable\nidentifiers are unique.\nThis is to handle the case where two branches of a match statement introduce the\nsame variable.\nIf both binders are hoisted out of the match statement, they one binding will\nshadow the other.\n\n\nThis pass, as implemented, unconditionally floats expression to where their data\ndependencies are fulfilled.\nThis is not safe in a general purpose language, and we may need to layer some\nheuristics on top of this pass to make it less aggressive if we end up\nintroducing performance regressions.\n\n\nCommon Subexpression Elimination\n\n\nFound in \nLanguage.Hakaru.Syntax.CSE\n\n\nCommon subexpression elimination eliminates redundant computation by reusing\nresults for equivalent expressions.\nThe current implementation of this pass relies on the program being in ANF.\n\n\nANF simplifies the implementation of CSE greatly by ensuring all expressions are\nnamed and that if two expressions may be shared, one of them is let-bound so\nthat it dominates the other.\nIn short, ANF simplifies the program to a simple top-down traversal of the AST.\nConsider the example\n\n\n(+ (add1 z) (add1 z))\n\n\n\n\nEliminating the common expression \n(add1 z)\n requires us to traverse the\nexpression in evaluation order, track expression which have already been\nevaluated, recognize when an expression is duplicated, and introduce it\nwith a new name that dominates all use sites of that expression.\nHowever, an expression in ANF allows us to perform CSE simply by keeping track\nof let-bound expressions and propagating those expressions downward into the\nAST.\nConsider the example in ANF\n\n\n(let ([t1 (add1 z)])\n  (let ([t2 (add1 z)])\n    (+ t1 t2)))\n\n\n\n\nTo remove the common subexpression, we simply have to note that the \n(add1 z)\n\nbound to \nt2\n is equivalent to the expression bound to \nt1\n and replace it with\nthe variable \nt1\n.\n\n\n(let ([t1 (add1 z)])\n  (let ([t2 t1])\n    (+ t1 t2)))\n\n\n\n\nTrivial bindings can then be eliminated, if desired, giving\n\n\n(let ([t1 (add1 z)])\n  (+ t1 t1)))\n\n\n\n\nA major goal of CSE is to cleanup any work which is duplicated by the\nlet-floating pass.\n\n\nPruning\n\n\nFound in \nLanguage.Hakaru.Syntax.Prune\n\n\nThis is essentially a limited form of dead code elimination.\nIf an expression is bound to a variable which is never referenced, then that\nexpression need never be executed, as the code language has no side effects.\nThis pass serves to clean up some of the junk introduced by other passes.\n\n\nCases which are handled\n\n\n\n\n(let ([x e1]) e2) => e2 if x not in fv(e2)\n\n\n(let ([x e1]) x)  => e1\n\n\n\n\nConstant Propagation\n\n\nFound in \nLanguage.Hakaru.Evalutation.ConstantPropagation\n\n\nPerforms simple constant propagation and constant folding.\nThe current implementation does not do that much work, mostly just evaluating\nprimitive operations when their arguments are constant.\n\n\nUnused Passes\n\n\nLoop Peeling\n\n\nFound in \nLanguage.Hakaru.Syntax.Unroll\n\n\nLoop peeling was an initial attempt at performing loop invariant code motion by\nleveraging CSE to do most of the heavy lifting.\nPeeling is a common strategy to make other optimization passes \u201cloop-aware\u201d.\nThe idea is to peel off one iteration of a loop and then apply the existing\nsuite of optimizations.\nConsider the following \nsummate\n whose body  \ne\n is some loop-invariant\ncomputation.\n\n\n(summate lo hi (\u03bb x -> e))\n\n\n\n\nAfter peeling we obtain\n\n\n(if (= lo hi)\n    0\n    (let ([x lo])\n      (let ([t1 e])\n        (let ([t2 (summate (+ lo 1) hi (\u03bb x -> e))])\n          (+ t1 t2)))))\n\n\n\n\nAfter applying CSE, the loop invariant body is simply reused on each iteration\n\n\n(if (= lo hi)\n    0\n    (let ([x lo])\n      (let ([t1 e])\n        (let ([t2 (summate (+ lo 1) hi (\u03bb x -> t1))])\n          (+ t1 t2)))))\n\n\n\n\nANF ensures that all subexpression in the \ne\n bound to \nt1\n are shareable with\nthe copy of \ne\n used in the body of the \nsummate\n, allowing us to hoist out\nsubexpressions of \ne\n and not just the entire \nsummate\n body.\n\n\nThis pass is currently disabled in favor of the let-floating pass, which does\na better job without causing an exponential blow up in code size.\nSome of Hakaru\u2019s looping constructs, such as \narray\n, cannot be peeled, so we\ncannot move loop invariant operations out of \narray\n statements.",
            "title": "Transformaitons"
        },
        {
            "location": "/internals/transforms/#program-transformations-in-hakaru",
            "text": "",
            "title": "Program transformations in Hakaru"
        },
        {
            "location": "/internals/transforms/#coalesce",
            "text": "Coalesce is an internal transformation that works on the untyped Hakaru AST. It\ntakes recursive  NAryOp  terms that have the same type and combines them into\na single term. For instance:  3.0 + 1.5 + 0.3  is parser as:  NaryOp Sum [3.0, NaryOp Sum [1.5, NaryOp Sum [0.3]]]  which when coalesced becomes:  NaryOp Sum [3.0,1.5,0.3]",
            "title": "Coalesce"
        },
        {
            "location": "/internals/transforms/#optimizations",
            "text": "The Hakaru AST has a suite of standard compiler optimizations which have\na substantial effect on the runtime of the resulting program.\nThe current pipeline is described by the  optimizations  variable in Language.Hakaru.Syntax.Transforms .\nIn order, the optimizations performed are:   A-normalization  Uniquification of variables (needed for let-floating)  Let-floating  Common subexpression elimination  Pruning of dead binders  Uniquification of variables (for the C backend)  Constant Propagation   Each pass is described in more detail below.",
            "title": "Optimizations"
        },
        {
            "location": "/internals/transforms/#a-normalization",
            "text": "Found in  Language.Hakaru.Syntax.ANF  See  The Essence of Compiling with Continuations by Flannigan, Sabry, Duba, and\nFelleisen  A-normalization converts expressions into  administrative normal form  (ANF).\nThis ensures that all intermediate values are named and all arguments to\nfunctions or primitive operations are either literals or variables.\nANF is a common program representation for functional language compilers which\ncan simplify some compiler passes and make others more effective.\nAs an example, consider  (add1 (let ([x (f y)]) 5))  This expression in ANF looks like the following  (let ([x (f y)]) (add1 5))  which opens up the opportunity for constant folding to eliminate the  (add1 5) \nexpression.\nThis pass exists mostly to simplify the implementation of CSE, but is useful for\nother passes as well.",
            "title": "A-normalization"
        },
        {
            "location": "/internals/transforms/#uniquification",
            "text": "Found in  Language.Hakaru.Syntax.Uniquify  Ensures all variables in the program have unique variable identifiers.\nThis is not strictly necessary, but simplifies the implementation of other\npasses, several of which rely on this property.",
            "title": "Uniquification"
        },
        {
            "location": "/internals/transforms/#let-floating",
            "text": "Found in  Language.Hakaru.Syntax.Hoist  See  Let-Floating: Moving Bindings to Give Faster Programs (1996)\nby Simon Peyton Jones , Will Partain , Andr\u00e9 Santos  Let-floating alters the bindings structure of the program in order to improve\nperformance.\nTypically, this entails moving definitions into or out of lambda expressions.\nWhen a lambda expression encodes a loop, this effectively accomplishes\nloop invariant code motion.\nThis pass only moves definitions upward in the AST.\nFor the most part, we are only interested in looping constructs like  summate  and product , and moving  summate  expressions out of other  summate  or  product \nexpressions when they do not depend on the index.\nThis can radically alter the asymptotics of the resulting program, as nested\nloops are converted into sequentially executed loops.  The only assumption this pass makes about the input AST is that all variable\nidentifiers are unique.\nThis is to handle the case where two branches of a match statement introduce the\nsame variable.\nIf both binders are hoisted out of the match statement, they one binding will\nshadow the other.  This pass, as implemented, unconditionally floats expression to where their data\ndependencies are fulfilled.\nThis is not safe in a general purpose language, and we may need to layer some\nheuristics on top of this pass to make it less aggressive if we end up\nintroducing performance regressions.",
            "title": "Let-floating"
        },
        {
            "location": "/internals/transforms/#common-subexpression-elimination",
            "text": "Found in  Language.Hakaru.Syntax.CSE  Common subexpression elimination eliminates redundant computation by reusing\nresults for equivalent expressions.\nThe current implementation of this pass relies on the program being in ANF.  ANF simplifies the implementation of CSE greatly by ensuring all expressions are\nnamed and that if two expressions may be shared, one of them is let-bound so\nthat it dominates the other.\nIn short, ANF simplifies the program to a simple top-down traversal of the AST.\nConsider the example  (+ (add1 z) (add1 z))  Eliminating the common expression  (add1 z)  requires us to traverse the\nexpression in evaluation order, track expression which have already been\nevaluated, recognize when an expression is duplicated, and introduce it\nwith a new name that dominates all use sites of that expression.\nHowever, an expression in ANF allows us to perform CSE simply by keeping track\nof let-bound expressions and propagating those expressions downward into the\nAST.\nConsider the example in ANF  (let ([t1 (add1 z)])\n  (let ([t2 (add1 z)])\n    (+ t1 t2)))  To remove the common subexpression, we simply have to note that the  (add1 z) \nbound to  t2  is equivalent to the expression bound to  t1  and replace it with\nthe variable  t1 .  (let ([t1 (add1 z)])\n  (let ([t2 t1])\n    (+ t1 t2)))  Trivial bindings can then be eliminated, if desired, giving  (let ([t1 (add1 z)])\n  (+ t1 t1)))  A major goal of CSE is to cleanup any work which is duplicated by the\nlet-floating pass.",
            "title": "Common Subexpression Elimination"
        },
        {
            "location": "/internals/transforms/#pruning",
            "text": "Found in  Language.Hakaru.Syntax.Prune  This is essentially a limited form of dead code elimination.\nIf an expression is bound to a variable which is never referenced, then that\nexpression need never be executed, as the code language has no side effects.\nThis pass serves to clean up some of the junk introduced by other passes.  Cases which are handled   (let ([x e1]) e2) => e2 if x not in fv(e2)  (let ([x e1]) x)  => e1",
            "title": "Pruning"
        },
        {
            "location": "/internals/transforms/#constant-propagation",
            "text": "Found in  Language.Hakaru.Evalutation.ConstantPropagation  Performs simple constant propagation and constant folding.\nThe current implementation does not do that much work, mostly just evaluating\nprimitive operations when their arguments are constant.",
            "title": "Constant Propagation"
        },
        {
            "location": "/internals/transforms/#unused-passes",
            "text": "",
            "title": "Unused Passes"
        },
        {
            "location": "/internals/transforms/#loop-peeling",
            "text": "Found in  Language.Hakaru.Syntax.Unroll  Loop peeling was an initial attempt at performing loop invariant code motion by\nleveraging CSE to do most of the heavy lifting.\nPeeling is a common strategy to make other optimization passes \u201cloop-aware\u201d.\nThe idea is to peel off one iteration of a loop and then apply the existing\nsuite of optimizations.\nConsider the following  summate  whose body   e  is some loop-invariant\ncomputation.  (summate lo hi (\u03bb x -> e))  After peeling we obtain  (if (= lo hi)\n    0\n    (let ([x lo])\n      (let ([t1 e])\n        (let ([t2 (summate (+ lo 1) hi (\u03bb x -> e))])\n          (+ t1 t2)))))  After applying CSE, the loop invariant body is simply reused on each iteration  (if (= lo hi)\n    0\n    (let ([x lo])\n      (let ([t1 e])\n        (let ([t2 (summate (+ lo 1) hi (\u03bb x -> t1))])\n          (+ t1 t2)))))  ANF ensures that all subexpression in the  e  bound to  t1  are shareable with\nthe copy of  e  used in the body of the  summate , allowing us to hoist out\nsubexpressions of  e  and not just the entire  summate  body.  This pass is currently disabled in favor of the let-floating pass, which does\na better job without causing an exponential blow up in code size.\nSome of Hakaru\u2019s looping constructs, such as  array , cannot be peeled, so we\ncannot move loop invariant operations out of  array  statements.",
            "title": "Loop Peeling"
        },
        {
            "location": "/internals/testing/",
            "text": "Testing infrastructure in Hakaru\n\n\nUnit testing has been created for Hakaru\u2019s main functions. The test suite is managed by Cabal and written in Haskell.\n\n\nTests written to test programs written using Hakaru are located in the \ntests/\n subdirectory at the root of the \nproject. Tests written in Haskell to check Hakaru functionality can be found at \nhaskell/Tests/\n.\n\n\nRunning Tests\n\n\nHakaru can be tested by running \ncabal test\n or \nstack test\n from the root directory of the project.\n\n\nNote:\n Tests that require Maple, such as \nsimplify\n tests, are only run if a local installation of Maple is detected.\n\n\nCreating New Tests\n\n\nHakaru testing is managed in the \nhakaru.cabal\n file found in the root directory. The main file for using the test suite is \nTestSuite.hs\n, which can be\nfound in \nhaskell\\tests\n.\n\n\nFor all tests, two programs are required \u2013 the program to test and a program representing the expected result.\n\n\nWriting Hakaru Tests\n\n\nWhen creating Hakaru tests, your two programs must be saved as seperate files.\n\n\nYou can add a test of your Hakaru programs to a Haskell program by using the \ntestConcreteFiles\n function from \nhaskell/Tests/TestTools.hs\n. This function \ntakes two Hakaru programs as arguments. It first runs \nsimplify\n on the first file and then asserts if it is equivilant to the second file.\n\n\nWriting Haskell Tests\n\n\nHaskell tests are created using the \nHUnit\n testing framework. Most existing Haskell tests use the \ntestSStriv\n function from \nhaskell/Tests/TestTools.hs\n.\nThis function asserts that the first Haskell function passed to it simplifies to the same function provided by the second function argument. The \nsimplification performed specifically calls the \nsimplify\n Hakaru transform.",
            "title": "Testing"
        },
        {
            "location": "/internals/testing/#testing-infrastructure-in-hakaru",
            "text": "Unit testing has been created for Hakaru\u2019s main functions. The test suite is managed by Cabal and written in Haskell.  Tests written to test programs written using Hakaru are located in the  tests/  subdirectory at the root of the \nproject. Tests written in Haskell to check Hakaru functionality can be found at  haskell/Tests/ .",
            "title": "Testing infrastructure in Hakaru"
        },
        {
            "location": "/internals/testing/#running-tests",
            "text": "Hakaru can be tested by running  cabal test  or  stack test  from the root directory of the project.  Note:  Tests that require Maple, such as  simplify  tests, are only run if a local installation of Maple is detected.",
            "title": "Running Tests"
        },
        {
            "location": "/internals/testing/#creating-new-tests",
            "text": "Hakaru testing is managed in the  hakaru.cabal  file found in the root directory. The main file for using the test suite is  TestSuite.hs , which can be\nfound in  haskell\\tests .  For all tests, two programs are required \u2013 the program to test and a program representing the expected result.",
            "title": "Creating New Tests"
        },
        {
            "location": "/internals/testing/#writing-hakaru-tests",
            "text": "When creating Hakaru tests, your two programs must be saved as seperate files.  You can add a test of your Hakaru programs to a Haskell program by using the  testConcreteFiles  function from  haskell/Tests/TestTools.hs . This function \ntakes two Hakaru programs as arguments. It first runs  simplify  on the first file and then asserts if it is equivilant to the second file.",
            "title": "Writing Hakaru Tests"
        },
        {
            "location": "/internals/testing/#writing-haskell-tests",
            "text": "Haskell tests are created using the  HUnit  testing framework. Most existing Haskell tests use the  testSStriv  function from  haskell/Tests/TestTools.hs .\nThis function asserts that the first Haskell function passed to it simplifies to the same function provided by the second function argument. The \nsimplification performed specifically calls the  simplify  Hakaru transform.",
            "title": "Writing Haskell Tests"
        },
        {
            "location": "/internals/newfeature/",
            "text": "Adding a feature to the Hakaru language\n\n\nTo add a feature to the Hakaru language you must\n\n\n\n\nAdd an entry to the AST\n\n\nUpdate symbol resolution and optionally the parser to recognize this construct\n\n\nUpdate the pretty printers if this is something exposed to users\n\n\nUpdate the typechecker to handle it\n\n\nUpdate all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it\n\n\nUpdate the sampler if this primitive is intended to exist at runtime\n\n\nUpdate the compilers to emit the right code for this symbol\n\n\n\n\nTODO: We give an example of what this looks like by adding \ndouble\n to the language.\n\n\nDocumenting a Hakaru Feature\n\n\nIf you add a new feature to Hakaru, you should write accompanying documentation so that others can learn how to use it. The Hakaru documentation is written using \n\nMkDocs\n, which uses MarkDown to format source files. In order to download MkDocs, it is recommended that you use the \nPython\n \npackage manager \npip\n. The \npip\n package manager is bundled with Python starting in versions 2.7.9 and 3.4, so it will be installed alongside Python. If you are using an \nearlier version of Python, you will need to install \npip\n manually. \n\n\n\n\nOn Windows, you must download \nget-pip.py\n and run it in the command prompr using \npython get-pip.py\n\n\nOn Linux, you can install \npip\n from the command line using \nsudo apt-get install python-pip\n\n\nOn OSX, you can install \npip\n from the command line using \nsudo easy-install pip\n\n\n\n\nOnce you have installed \npip\n, you can download the required packages for the Hakaru documentation:\n\n\npip install mkdocs\npip install python-markdown-math\npip install mkdocs-extensions\npip install mkdocs-bootswatch",
            "title": "Adding a Language Feature"
        },
        {
            "location": "/internals/newfeature/#adding-a-feature-to-the-hakaru-language",
            "text": "To add a feature to the Hakaru language you must   Add an entry to the AST  Update symbol resolution and optionally the parser to recognize this construct  Update the pretty printers if this is something exposed to users  Update the typechecker to handle it  Update all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it  Update the sampler if this primitive is intended to exist at runtime  Update the compilers to emit the right code for this symbol   TODO: We give an example of what this looks like by adding  double  to the language.",
            "title": "Adding a feature to the Hakaru language"
        },
        {
            "location": "/internals/newfeature/#documenting-a-hakaru-feature",
            "text": "If you add a new feature to Hakaru, you should write accompanying documentation so that others can learn how to use it. The Hakaru documentation is written using  MkDocs , which uses MarkDown to format source files. In order to download MkDocs, it is recommended that you use the  Python  \npackage manager  pip . The  pip  package manager is bundled with Python starting in versions 2.7.9 and 3.4, so it will be installed alongside Python. If you are using an \nearlier version of Python, you will need to install  pip  manually.    On Windows, you must download  get-pip.py  and run it in the command prompr using  python get-pip.py  On Linux, you can install  pip  from the command line using  sudo apt-get install python-pip  On OSX, you can install  pip  from the command line using  sudo easy-install pip   Once you have installed  pip , you can download the required packages for the Hakaru documentation:  pip install mkdocs\npip install python-markdown-math\npip install mkdocs-extensions\npip install mkdocs-bootswatch",
            "title": "Documenting a Hakaru Feature"
        }
    ]
}