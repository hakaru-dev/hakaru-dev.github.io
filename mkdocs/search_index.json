{
    "docs": [
        {
            "location": "/", 
            "text": "Hakaru\n\n\n\n\n\nHakaru is a probabilistic programming language. A probabilistic programming\nlanguage is a language specifically designed for manipulating probability\ndistributions. These sorts of languages are great for machine learning and\nstochastic modeling.\n\n\nOverview\n\n\nThis manual provides a guide for how to use Hakaru.\n\n\nIntroduction\n\n\nWhat is Probabilistic Programming\n\n\nProbabilistic programming systems allow us to write programs which\ndescribe probability distributions, and provide mechanisms to\nsample and condition the distributions they represent on data. In\nthis page, we give a sense of the sorts of problems Hakaru is\ngreat at solving, and how you would describe them in Hakaru.\n\n\nInstallation\n\n\nLearn how to install Hakaru\n\n\nQuickstart\n\n\nGet started with this quickstart page. Where we show\nhow to sample and condition from a small Hakaru program.\n\n\nExamples\n\n\nHere we go through several more involved examples of the kinds of\nproblems Hakaru is uniquely well-suited to solve.\n\n\nIn particular, we describe a model for Gaussian Mixture Models and\nusing a form of Bayesian Naives Bayes as applied to document\nclassification.\n\n\nLanguage Guide\n\n\nThe language section provides an overview of the syntax of Hakaru as\nwell as some of the primitives in the language.\n\n\nRandom Primitives\n\n\nThese are the built-in probability distributions.\n\n\nLet and Bind\n\n\nThis is how we can give names to subexpressions and a\ndraw from a probability distribution.\n\n\nConditionals\n\n\nHakaru supports a restricted \nif\n expression\n\n\nTypes and Coercions\n\n\nHakaru is a simply-typed language. This section\ndescribes the types available and functions for\nmoving between them.\n\n\nFunctions\n\n\nDefining and using functions\n\n\nDatatypes and match\n\n\nHakaru supports a few built-in datatypes, and offers functionality for\ntaking them apart and reconstructing them.\n\n\nArrays and loops\n\n\nWe offer special support for arrays, and for probability\ndistributions over arrays.\nWe also express loops that compute sums and products.\n\n\nTransformations\n\n\nHakaru implements its inference algorithms predominately as\nprogram transformations. The following are the major ones\nour system provides.\n\n\nExpect\n\n\nDisintegrate\n\n\nSimplify\n\n\nMetropolis Hastings\n\n\nCompiling to Haskell\n\n\nCompiling to C\n\n\nInternals\n\n\nThe internals section of the manual provides some insight into how\nHakaru is implemented and offers guidance into how the system can\nbe extended.\n\n\nAST\n\n\nABT\n\n\nDatums\n\n\nCoercions\n\n\nTransformations\n\n\nTesting\n\n\nAdding a Language Feature", 
            "title": "Home"
        }, 
        {
            "location": "/#overview", 
            "text": "This manual provides a guide for how to use Hakaru.", 
            "title": "Overview"
        }, 
        {
            "location": "/#introduction", 
            "text": "", 
            "title": "Introduction"
        }, 
        {
            "location": "/#what-is-probabilistic-programming", 
            "text": "Probabilistic programming systems allow us to write programs which\ndescribe probability distributions, and provide mechanisms to\nsample and condition the distributions they represent on data. In\nthis page, we give a sense of the sorts of problems Hakaru is\ngreat at solving, and how you would describe them in Hakaru.", 
            "title": "What is Probabilistic Programming"
        }, 
        {
            "location": "/#installation", 
            "text": "Learn how to install Hakaru", 
            "title": "Installation"
        }, 
        {
            "location": "/#quickstart", 
            "text": "Get started with this quickstart page. Where we show\nhow to sample and condition from a small Hakaru program.", 
            "title": "Quickstart"
        }, 
        {
            "location": "/#examples", 
            "text": "Here we go through several more involved examples of the kinds of\nproblems Hakaru is uniquely well-suited to solve.  In particular, we describe a model for Gaussian Mixture Models and\nusing a form of Bayesian Naives Bayes as applied to document\nclassification.", 
            "title": "Examples"
        }, 
        {
            "location": "/#language-guide", 
            "text": "The language section provides an overview of the syntax of Hakaru as\nwell as some of the primitives in the language.", 
            "title": "Language Guide"
        }, 
        {
            "location": "/#random-primitives", 
            "text": "These are the built-in probability distributions.", 
            "title": "Random Primitives"
        }, 
        {
            "location": "/#let-and-bind", 
            "text": "This is how we can give names to subexpressions and a\ndraw from a probability distribution.", 
            "title": "Let and Bind"
        }, 
        {
            "location": "/#conditionals", 
            "text": "Hakaru supports a restricted  if  expression", 
            "title": "Conditionals"
        }, 
        {
            "location": "/#types-and-coercions", 
            "text": "Hakaru is a simply-typed language. This section\ndescribes the types available and functions for\nmoving between them.", 
            "title": "Types and Coercions"
        }, 
        {
            "location": "/#functions", 
            "text": "Defining and using functions", 
            "title": "Functions"
        }, 
        {
            "location": "/#datatypes-and-match", 
            "text": "Hakaru supports a few built-in datatypes, and offers functionality for\ntaking them apart and reconstructing them.", 
            "title": "Datatypes and match"
        }, 
        {
            "location": "/#arrays-and-loops", 
            "text": "We offer special support for arrays, and for probability\ndistributions over arrays.\nWe also express loops that compute sums and products.", 
            "title": "Arrays and loops"
        }, 
        {
            "location": "/#transformations", 
            "text": "Hakaru implements its inference algorithms predominately as\nprogram transformations. The following are the major ones\nour system provides.  Expect  Disintegrate  Simplify  Metropolis Hastings  Compiling to Haskell  Compiling to C", 
            "title": "Transformations"
        }, 
        {
            "location": "/#internals", 
            "text": "The internals section of the manual provides some insight into how\nHakaru is implemented and offers guidance into how the system can\nbe extended.  AST  ABT  Datums  Coercions  Transformations  Testing  Adding a Language Feature", 
            "title": "Internals"
        }, 
        {
            "location": "/intro/probprog/", 
            "text": "What is Probabilistic Programming?\n\n\nProbabilistic programs are programs which represent probability\ndistributions. For example, the program \npoisson(5)\n represents the\npoisson distribution with a rate of five. Why do we need a\nlanguage for describing probability distributions?\n\n\nThe world is intrinsically an uncertain place. When we try to predict\nwhat will happen in the world given some data we have collected, we\nare inherently engaging in some sort of probabilistic modeling. In\nprobabilistic modeling, we treat the quantity we wish to predict as a\nparameter, and then describe our data as some noisy function of this\nparameter. This function is called \nlikelihood\n, and depending on which\nstatistical regime you use can be used in predominately two ways.\n\n\nFor instance, we might want to estimate the average time it takes for\na bus to arrive at a stop, based on actual arrival times. In this situation,\nthe likelihood function would be:\n\n\n$$ x \\sim \\text{Poisson}(\\lambda) $$\n\n\nwhere $x$ is the actual arrival time, and $\\lambda$ is the quantity we\nwish to predict. In other words, this likelihood says our data is a\nnoisy measurement of the average waiting time which follows a Poisson\ndistribution. We can also represent this likelihood function as a\ndensity function which for a given choice of $\\lambda$ returns how\nlikely it is for $x$ to be generated under that parameter.\n\n\n$$ f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} $$\n\n\nUnder a frequentist regime we perform maximum likelihood, where we find\nthe best parameter by finding the $\\lambda$ which maximizes $f$.\n\n\nUnder a Bayesian regime, we don\nt estimate a single best value for the\nparameter. Instead we place a prior distribution on the parameters and\nestimate that posterior distribution conditioned on our data.\n\n\nIn Hakaru, it is possible to use either regime for solving your\nproblems.  We will call the distribution or program which describes\nour data the \nmodel\n.\n\n\nTug of War\n\n\nWe demonstrate the value of this problem-solving approach using a\nsimplified version of the\n\ntug of war\n\nexample from probmods.org. For this problem we will take a Bayesian\napproach to prediction.\n\n\nFor this problem, we have three friends, Alice, Bob and Carol who take\nturns playing a tug of war against each other and we\nd like to know\nwhich of them is the strongest. We can pose this problem as a\nprobabilistic program. In particular, we will try to predict who will\nwin match3 given we have observed who won the first two matches.\n\n\nWe can start by assuming each player\ns strength comes from a standard\nnormal distribution. Then we assume the strength they pull with some\nnormal distribution centered around their true strength, and the\nperson who pulled harder wins.\n\n\ndef pulls(strength real):\n    normal(strength, 1)\n\ndef winner(a real, b real):\n    a_pull \n~ pulls(a)\n    b_pull \n~ pulls(b)\n    return (a_pull \n b_pull)\n\nalice \n~ normal(0,1)\nbob   \n~ normal(0,1)\ncarol \n~ normal(0,1)\n\nmatch1 \n~ winner(alice, bob)\nmatch2 \n~ winner(bob, carol)\nmatch3 \n~ winner(alice, carol)\n\n\n\n\nWe then restrict the set of events to only those where Alice won the\nfirst match and Bob won the second, and return the results of the\nthird match.\n\n\nif match1 \n match2:\n   return match3\nelse:\n   reject. measure(bool)\n\n\n\n\nWe can then run the above model using hakaru, which shows that Alice\nis likely to win her match against Carol.\n\n\nhakaru tugofwar.hk | head -n 10000 | sort | uniq -c\n   3060 false\n   6940 true\n\n\n\n\nSimulation vs Inference\n\n\nOf course, in the above program we performed inference, by taking\nour model and throwing out all events that didn\nt agree with\nthe data we had. How well would this work if we changed our\nmodel slightly? Suppose our data wasn\nt boolean values, but instead\nthe difference of strengths, and we want to not just whether Alice\nwill win, but by how much.\n\n\nAs we pose more complex questions, posing our models as rejection\nsamplers becomes increasing inefficient. Instead we would like to\ndirectly transform our models into those which only generate the\ndata we observed and don\nt waste any computation simulating data\nwhich we know will never exist.\n\n\n\n    \n\n        \nTODO\n\n    \n\n    \n\n        Explain simplify and mh", 
            "title": "What is Probabilistic Programming"
        }, 
        {
            "location": "/intro/probprog/#what-is-probabilistic-programming", 
            "text": "Probabilistic programs are programs which represent probability\ndistributions. For example, the program  poisson(5)  represents the\npoisson distribution with a rate of five. Why do we need a\nlanguage for describing probability distributions?  The world is intrinsically an uncertain place. When we try to predict\nwhat will happen in the world given some data we have collected, we\nare inherently engaging in some sort of probabilistic modeling. In\nprobabilistic modeling, we treat the quantity we wish to predict as a\nparameter, and then describe our data as some noisy function of this\nparameter. This function is called  likelihood , and depending on which\nstatistical regime you use can be used in predominately two ways.  For instance, we might want to estimate the average time it takes for\na bus to arrive at a stop, based on actual arrival times. In this situation,\nthe likelihood function would be:  $$ x \\sim \\text{Poisson}(\\lambda) $$  where $x$ is the actual arrival time, and $\\lambda$ is the quantity we\nwish to predict. In other words, this likelihood says our data is a\nnoisy measurement of the average waiting time which follows a Poisson\ndistribution. We can also represent this likelihood function as a\ndensity function which for a given choice of $\\lambda$ returns how\nlikely it is for $x$ to be generated under that parameter.  $$ f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} $$  Under a frequentist regime we perform maximum likelihood, where we find\nthe best parameter by finding the $\\lambda$ which maximizes $f$.  Under a Bayesian regime, we don t estimate a single best value for the\nparameter. Instead we place a prior distribution on the parameters and\nestimate that posterior distribution conditioned on our data.  In Hakaru, it is possible to use either regime for solving your\nproblems.  We will call the distribution or program which describes\nour data the  model .", 
            "title": "What is Probabilistic Programming?"
        }, 
        {
            "location": "/intro/probprog/#tug-of-war", 
            "text": "We demonstrate the value of this problem-solving approach using a\nsimplified version of the tug of war \nexample from probmods.org. For this problem we will take a Bayesian\napproach to prediction.  For this problem, we have three friends, Alice, Bob and Carol who take\nturns playing a tug of war against each other and we d like to know\nwhich of them is the strongest. We can pose this problem as a\nprobabilistic program. In particular, we will try to predict who will\nwin match3 given we have observed who won the first two matches.  We can start by assuming each player s strength comes from a standard\nnormal distribution. Then we assume the strength they pull with some\nnormal distribution centered around their true strength, and the\nperson who pulled harder wins.  def pulls(strength real):\n    normal(strength, 1)\n\ndef winner(a real, b real):\n    a_pull  ~ pulls(a)\n    b_pull  ~ pulls(b)\n    return (a_pull   b_pull)\n\nalice  ~ normal(0,1)\nbob    ~ normal(0,1)\ncarol  ~ normal(0,1)\n\nmatch1  ~ winner(alice, bob)\nmatch2  ~ winner(bob, carol)\nmatch3  ~ winner(alice, carol)  We then restrict the set of events to only those where Alice won the\nfirst match and Bob won the second, and return the results of the\nthird match.  if match1   match2:\n   return match3\nelse:\n   reject. measure(bool)  We can then run the above model using hakaru, which shows that Alice\nis likely to win her match against Carol.  hakaru tugofwar.hk | head -n 10000 | sort | uniq -c\n   3060 false\n   6940 true", 
            "title": "Tug of War"
        }, 
        {
            "location": "/intro/probprog/#simulation-vs-inference", 
            "text": "Of course, in the above program we performed inference, by taking\nour model and throwing out all events that didn t agree with\nthe data we had. How well would this work if we changed our\nmodel slightly? Suppose our data wasn t boolean values, but instead\nthe difference of strengths, and we want to not just whether Alice\nwill win, but by how much.  As we pose more complex questions, posing our models as rejection\nsamplers becomes increasing inefficient. Instead we would like to\ndirectly transform our models into those which only generate the\ndata we observed and don t waste any computation simulating data\nwhich we know will never exist.", 
            "title": "Simulation vs Inference"
        }, 
        {
            "location": "/intro/installation/", 
            "text": "Installation\n\n\nInstall Hakaru by cloning the latest version from our Github repo\n\n\ngit clone https://github.com/hakaru-dev/hakaru\ncd hakaru\n\n\n\n\nHakaru can then be installed either with \ncabal install\n or \nstack install\n\n\nDue to a \nghc bug\n, Windows users\nusing GHC 7.10 and below need to install the logfloat library separately\n\n\ncabal install -j logfloat -f -useffi\ncd hakaru\ncabal install\n\n\n\n\nMaple extension\n\n\nWithin Hakaru, we use \nMaple\n to perform\ncomputer-algebra guided optimizations. To get access to these optimizations\nyou must have a licensed copy of Maple installed.\n\n\nIn addition to this, we must autoload some Maple libraries that come\nwith the system to access this functionality\n\n\nexport LOCAL_MAPLE=\n`which maple`\n\ncd hakaru/maple\nmaple update-archive.mpl\necho 'libname := \n/path-to-hakaru/hakaru/maple\n,libname:' \n ~/.mapleinit\n\n\n\n\nUnder Windows the instructions become\n\n\nSETX LOCAL_MAPLE \npath to Maple bin directory\n\\cmaple.exe\n\ncd hakaru\\maple \ncmaple update-archive.mpl\necho 'libname := \nC:\\\\\npath to hakaru\n\\\\hakaru\\\\maple\n,libname:' \n \nC:\\\npath to maple\n\\lib\\maple.ini\n\n\n\n\n\nIf the Maple extension has been properly installed running\n\n\necho \nnormal(0,1)\n | simplify -\n\n\n\n\nshould return\n\n\nnormal(0, 1)\n\n\n\n\nIf the \nLOCAL_MAPLE\n environment variable is not set, then \nsimplify\n\ndefaults to invoking \nssh\n to access a remote installation of Maple.\nThe invocation is\n\n\n$MAPLE_SSH\n -l \n$MAPLE_USER\n \n$MAPLE_SERVER\n \n$MAPLE_COMMAND -q -t\n\n\n\n\n\nand defaults to\n\n\n/usr/bin/ssh -l ppaml karst.uits.iu.edu \nmaple -q -t", 
            "title": "Installation"
        }, 
        {
            "location": "/intro/installation/#installation", 
            "text": "Install Hakaru by cloning the latest version from our Github repo  git clone https://github.com/hakaru-dev/hakaru\ncd hakaru  Hakaru can then be installed either with  cabal install  or  stack install  Due to a  ghc bug , Windows users\nusing GHC 7.10 and below need to install the logfloat library separately  cabal install -j logfloat -f -useffi\ncd hakaru\ncabal install", 
            "title": "Installation"
        }, 
        {
            "location": "/intro/installation/#maple-extension", 
            "text": "Within Hakaru, we use  Maple  to perform\ncomputer-algebra guided optimizations. To get access to these optimizations\nyou must have a licensed copy of Maple installed.  In addition to this, we must autoload some Maple libraries that come\nwith the system to access this functionality  export LOCAL_MAPLE= `which maple` \ncd hakaru/maple\nmaple update-archive.mpl\necho 'libname :=  /path-to-hakaru/hakaru/maple ,libname:'   ~/.mapleinit  Under Windows the instructions become  SETX LOCAL_MAPLE  path to Maple bin directory \\cmaple.exe \ncd hakaru\\maple \ncmaple update-archive.mpl\necho 'libname :=  C:\\\\ path to hakaru \\\\hakaru\\\\maple ,libname:'    C:\\ path to maple \\lib\\maple.ini   If the Maple extension has been properly installed running  echo  normal(0,1)  | simplify -  should return  normal(0, 1)  If the  LOCAL_MAPLE  environment variable is not set, then  simplify \ndefaults to invoking  ssh  to access a remote installation of Maple.\nThe invocation is  $MAPLE_SSH  -l  $MAPLE_USER   $MAPLE_SERVER   $MAPLE_COMMAND -q -t   and defaults to  /usr/bin/ssh -l ppaml karst.uits.iu.edu  maple -q -t", 
            "title": "Maple extension"
        }, 
        {
            "location": "/intro/quickstart/", 
            "text": "Quickstart\n\n\nAssuming you have Hakaru \ninstalled\n, let\ns\nsample a simple a model.\n\n\nx \n~ bern(0.5)\ny \n~ match x:\n      true:  normal(0,1)\n      false: uniform(0,1)\nreturn (y,x)\n\n\n\n\nThe generative model here has us flip a coin with bias 0.5, and then\nhave \nx\n be a draw from that distribution. We then check if \nx\n is\ntrue or false. Based on that we either have \ny\n be a draw from\na normal or uniform distribution, and then we return both \nx\n and \ny\n.\nBecause we are choosing between a normal and a uniform distribution,\nprograms like these are sometimes called \nmixture\n models.\n\n\nAssuming we save this file to \ntwomixture.hk\n we can sample from it by\npassing it as an argument to the \nhakaru\n command. \n\n\nhakaru twomixture.hk\n\n\n\n\nHakaru will then produce an infinite stream of samples from the\ndistribution this program represents.\n\n\n(0.8614855008328531, false)\n(0.27145378737815007, false)\n(6.137461559047042e-4, false)\n(0.9699201771404777, true)\n(1.2904529857533733, true)\n(8.605226081336681e-2, false)\n(-0.7713069511457459, true)\n(0.18162205213257607, true)\n(-1.143049106224509, true)\n(0.3667084406816875, false)\n...\n\n\n\n\nOf course, Hakaru wouldn\nt be very interesting if that was all it\ndid. Often what we wish to do is condition a distribution on\ndata. Suppose for \ntwomixture.hk\n we knew \ny\n, and would like to\nsample \nx\n conditioned on this information. We can symbolically\nproduce the unnormalized conditional distribution, which we call the\n\ndisintegration\n of the program.\n\n\ndisintegrate twomixture.hk\n\n\n\n\nThis returns\n\n\nfn x2 real: \n weight(0.5,\n        weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n                1.0 * \n                recip(natroot((2.0 * pi), 2))),\n               x = true\n               return x)) \n|\n \n weight(0.5,\n        match (not((x2 \n 0.0)) \n not((1.0 \n x2))): \n         true: \n          x = false\n          return x\n         false: reject. measure(bool))\n\n\n\n\nDisintegrate returns a function, to make it easier to sample\nfrom, we\nll give a value for x2. We\nll call this file\n\ntwomixture2.hk\n\n\nx2 = 0.3\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x)) \n|\n \nweight(0.5,\n    match (not((x2 \n 0.0)) \n not((1.0 \n x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))\n\n\n\n\nWhich we can run through some unix commands to get a sense of\nthe distribution\n\n\nhakaru twomixture2.hk | head -n 1000 | sort | uniq -c\n\n    526 false\n    474 true\n\n\n\n\nAs we can see, when x2 = 0.3, the uniform distribution is slightly more\nlikely. If we change x2 to be 3.0\n\n\nx2 = 3.0\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x)) \n|\n \nweight(0.5,\n    match (not((x2 \n 0.0)) \n not((1.0 \n x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))\n\n\n\n\nThis reflects that only the normal case is possible.\n\n\nhakaru twomixture3.hk | head -n 1000 | sort | uniq -c\n\n    1000 true", 
            "title": "Quick Example"
        }, 
        {
            "location": "/intro/quickstart/#quickstart", 
            "text": "Assuming you have Hakaru  installed , let s\nsample a simple a model.  x  ~ bern(0.5)\ny  ~ match x:\n      true:  normal(0,1)\n      false: uniform(0,1)\nreturn (y,x)  The generative model here has us flip a coin with bias 0.5, and then\nhave  x  be a draw from that distribution. We then check if  x  is\ntrue or false. Based on that we either have  y  be a draw from\na normal or uniform distribution, and then we return both  x  and  y .\nBecause we are choosing between a normal and a uniform distribution,\nprograms like these are sometimes called  mixture  models.  Assuming we save this file to  twomixture.hk  we can sample from it by\npassing it as an argument to the  hakaru  command.   hakaru twomixture.hk  Hakaru will then produce an infinite stream of samples from the\ndistribution this program represents.  (0.8614855008328531, false)\n(0.27145378737815007, false)\n(6.137461559047042e-4, false)\n(0.9699201771404777, true)\n(1.2904529857533733, true)\n(8.605226081336681e-2, false)\n(-0.7713069511457459, true)\n(0.18162205213257607, true)\n(-1.143049106224509, true)\n(0.3667084406816875, false)\n...  Of course, Hakaru wouldn t be very interesting if that was all it\ndid. Often what we wish to do is condition a distribution on\ndata. Suppose for  twomixture.hk  we knew  y , and would like to\nsample  x  conditioned on this information. We can symbolically\nproduce the unnormalized conditional distribution, which we call the disintegration  of the program.  disintegrate twomixture.hk  This returns  fn x2 real: \n weight(0.5,\n        weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n                1.0 * \n                recip(natroot((2.0 * pi), 2))),\n               x = true\n               return x))  |  \n weight(0.5,\n        match (not((x2   0.0))   not((1.0   x2))): \n         true: \n          x = false\n          return x\n         false: reject. measure(bool))  Disintegrate returns a function, to make it easier to sample\nfrom, we ll give a value for x2. We ll call this file twomixture2.hk  x2 = 0.3\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x))  |  \nweight(0.5,\n    match (not((x2   0.0))   not((1.0   x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))  Which we can run through some unix commands to get a sense of\nthe distribution  hakaru twomixture2.hk | head -n 1000 | sort | uniq -c\n\n    526 false\n    474 true  As we can see, when x2 = 0.3, the uniform distribution is slightly more\nlikely. If we change x2 to be 3.0  x2 = 3.0\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x))  |  \nweight(0.5,\n    match (not((x2   0.0))   not((1.0   x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))  This reflects that only the normal case is possible.  hakaru twomixture3.hk | head -n 1000 | sort | uniq -c\n\n    1000 true", 
            "title": "Quickstart"
        }, 
        {
            "location": "/lang/rand/", 
            "text": "Primitive Probability Distributions\n\n\nHakaru comes with a small set of primitive probability\ndistributions.\n\n\n\n\n\n\n\n\nnormal(mean. \nreal\n, standard_deviation. \nprob\n): \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nunivariate Normal (Gaussian) distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuniform(low. \nreal\n, high. \nreal\n): \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nUniform distribution is a continuous univariate distribution defined from low to high\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngamma(shape. \nprob\n, scale. \nprob\n): \nmeasure(prob)\n \n\n\n\n\n\n\n\n\n\n\nGamma distribution with shape and scale parameterization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbeta(a. \nprob\n, b. \nprob\n): \nmeasure(prob)\n \n\n\n\n\n\n\n\n\n\n\nBeta distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npoisson(l. \nprob\n): \nmeasure(nat)\n \n\n\n\n\n\n\n\n\n\n\nPoisson distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategorical(v. \narray(prob)\n): \nmeasure(nat)\n \n\n\n\n\n\n\n\n\n\n\nCategorical distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndirac(x. \na\n): \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\nDirac distribution\n\n\n\n\n\n\n\n\nThe Dirac distribution appears often enough, that we have given an\nadditional keyword in our language for it: \nreturn\n. The following\nprograms are equivalent.\n\n\ndirac(3)\n\n\n\n\nreturn 3\n\n\n\n\n\n\n\n\n\n\nlebesgue: \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nthe distribution constant over the real line\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweight(x. \nprob\n, m. \nmeasure(a)\n): \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\na \nm\n distribution, reweighted by \nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreject: \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\nThe distribution over the empty set\n\n\n\n\n\n\n\n\nFinally, we have a binary choice operator \n|\n, which takes two\ndistributions, and returns an unnormalized distribution which returns\none or the other.  For example, to get a distribution which where with\nprobability 0.5 draws from a uniform(0,1), and probability 0.5 draws\nfrom uniform(5,6).\n\n\nweight(0.5, uniform(0,1)) \n|\n\nweight(0.5, uniform(5,6))", 
            "title": "Random Primitives"
        }, 
        {
            "location": "/lang/rand/#primitive-probability-distributions", 
            "text": "Hakaru comes with a small set of primitive probability\ndistributions.", 
            "title": "Primitive Probability Distributions"
        }, 
        {
            "location": "/lang/letbind/", 
            "text": "Let and Bind\n\n\nIn Hakaru, we can give names for expressions to our programs with \n=\n,\nwhich we call \nLet\n. This gives us the ability to share computation\nthat might be needed in the program.\n\n\nx = 2\nx + 3\n\n\n\n\nWe can use \n=\n to give a name to any expression in our language. The\nname you assign is in scope for the rest of the body it was defined in.\n\n\nBind\n\n\nHakaru also has the operator \n~\n. This operator, which call \nBind\n\ncan only be used with expressions that denote probability distributions.\nBind allows us to talk about draws from a distribution using a name for\nany particular value that could have come from that distribution.\n\n\n# Bad\nx \n~ 2 + 3\nx\n\n\n\n\n# Good\nx \n~ normal(0,1)\nreturn x\n\n\n\n\nBecause Bind is about draws from a distribution, the rest of the body\nmust also denote a probability distribution.\n\n\n# Bad\nx \n~ normal(0,1)\nx\n\n\n\n\n# Good\nx \n~ normal(0,1)\nreturn x\n\n\n\n\nTo help distinguish Let and Bind. Here is a probabilistic program, where we\nlet \nf\n be equal to the normal distribution, and take draws from \nf\n.\n\n\nf = normal(0,1)\nx \n~ f\nreturn x*x", 
            "title": "Let and Bind"
        }, 
        {
            "location": "/lang/letbind/#let-and-bind", 
            "text": "In Hakaru, we can give names for expressions to our programs with  = ,\nwhich we call  Let . This gives us the ability to share computation\nthat might be needed in the program.  x = 2\nx + 3  We can use  =  to give a name to any expression in our language. The\nname you assign is in scope for the rest of the body it was defined in.", 
            "title": "Let and Bind"
        }, 
        {
            "location": "/lang/letbind/#bind", 
            "text": "Hakaru also has the operator  ~ . This operator, which call  Bind \ncan only be used with expressions that denote probability distributions.\nBind allows us to talk about draws from a distribution using a name for\nany particular value that could have come from that distribution.  # Bad\nx  ~ 2 + 3\nx  # Good\nx  ~ normal(0,1)\nreturn x  Because Bind is about draws from a distribution, the rest of the body\nmust also denote a probability distribution.  # Bad\nx  ~ normal(0,1)\nx  # Good\nx  ~ normal(0,1)\nreturn x  To help distinguish Let and Bind. Here is a probabilistic program, where we\nlet  f  be equal to the normal distribution, and take draws from  f .  f = normal(0,1)\nx  ~ f\nreturn x*x", 
            "title": "Bind"
        }, 
        {
            "location": "/lang/cond/", 
            "text": "Conditionals\n\n\nHakaru supports an \nif\n expression. This if must have two\nbodies. There exists no special syntax for \nelse if\n like\nyou might find in Python.\n\n\na  = 4\nb  = 5\nif a \n b:\n   a + 1\nelse:\n   b - 2", 
            "title": "Conditionals"
        }, 
        {
            "location": "/lang/cond/#conditionals", 
            "text": "Hakaru supports an  if  expression. This if must have two\nbodies. There exists no special syntax for  else if  like\nyou might find in Python.  a  = 4\nb  = 5\nif a   b:\n   a + 1\nelse:\n   b - 2", 
            "title": "Conditionals"
        }, 
        {
            "location": "/lang/coercions/", 
            "text": "Types and Coercions\n\n\nHakaru is a simply-typed language which has\na few basic types and some more complicated\nones which can be built out of simpler types.\n\n\nTypes\n\n\n\n\nnat is the type for natural numbers. This includes zero.\n\n\nint is the integer type.\n\n\nprob is the type for positive real number. This includes zero.\n\n\nreal is the type for real numbers.\n\n\narray(x) is the type for arrays where each element is type x\n\n\nmeasure(x) is the type for probability distributions whose\n  sample space is type x\n\n\n\n\nCoercions\n\n\nFor the primitive numeric types we also offer coercion functions.\n\n\n\n\nprob2real\n\n\nint2real\n\n\nnat2int\n\n\nreal2prob\n\n\nreal2int\n\n\nint2nat\n\n\n\n\nFor the ones which are always safe to apply such as \nnat2int\n we will\nautomatically insert them if it is required for the program to typecheck.", 
            "title": "Coercions"
        }, 
        {
            "location": "/lang/coercions/#types-and-coercions", 
            "text": "Hakaru is a simply-typed language which has\na few basic types and some more complicated\nones which can be built out of simpler types.", 
            "title": "Types and Coercions"
        }, 
        {
            "location": "/lang/coercions/#types", 
            "text": "nat is the type for natural numbers. This includes zero.  int is the integer type.  prob is the type for positive real number. This includes zero.  real is the type for real numbers.  array(x) is the type for arrays where each element is type x  measure(x) is the type for probability distributions whose\n  sample space is type x", 
            "title": "Types"
        }, 
        {
            "location": "/lang/coercions/#coercions", 
            "text": "For the primitive numeric types we also offer coercion functions.   prob2real  int2real  nat2int  real2prob  real2int  int2nat   For the ones which are always safe to apply such as  nat2int  we will\nautomatically insert them if it is required for the program to typecheck.", 
            "title": "Coercions"
        }, 
        {
            "location": "/lang/functions/", 
            "text": "Functions\n\n\nFunctions can be defined using a Python-inspired style syntax. One\nnotable difference is that each argument must be followed by its\ntype.\n\n\ndef add(x real, y real):\n    x + y\n\nadd(4,5)\n\n\n\n\nWe may optionally provide a type for the return value of a function if\nwe wish.\n\n\ndef add(x. real, y. real) real:\n    x + y\n\nadd(4,5)\n\n\n\n\nAnonymous functions\n\n\nIf you don\nt wish to name your functions, we also offer a syntax\nfor anonymous functions. These only take on argument and must be\ngiven a type alongside the variable name.\n\n\nfn x real: x + 1\n\n\n\n\nInternally, there are only one argument anonymous functions, and\nlets. The first example is equivalent to the following.\n\n\nadd = fn x real:\n         fn y real:\n            x + y\nadd(4,5)", 
            "title": "Functions and Let"
        }, 
        {
            "location": "/lang/functions/#functions", 
            "text": "Functions can be defined using a Python-inspired style syntax. One\nnotable difference is that each argument must be followed by its\ntype.  def add(x real, y real):\n    x + y\n\nadd(4,5)  We may optionally provide a type for the return value of a function if\nwe wish.  def add(x. real, y. real) real:\n    x + y\n\nadd(4,5)", 
            "title": "Functions"
        }, 
        {
            "location": "/lang/functions/#anonymous-functions", 
            "text": "If you don t wish to name your functions, we also offer a syntax\nfor anonymous functions. These only take on argument and must be\ngiven a type alongside the variable name.  fn x real: x + 1  Internally, there are only one argument anonymous functions, and\nlets. The first example is equivalent to the following.  add = fn x real:\n         fn y real:\n            x + y\nadd(4,5)", 
            "title": "Anonymous functions"
        }, 
        {
            "location": "/lang/datatypes/", 
            "text": "Data types and Match\n\n\nHakaru with several built-in data types.\n\n\n\n\npair\n\n\nunit\n\n\neither\n\n\nbool\n\n\n\n\nMatch\n\n\nWe use \nmatch\n to deconstruct out data types\nand access their elements.\n\n\nmatch left(3). either(int,bool):\n  left(x) : 1\n  right(x): 2\n\n\n\n\nWe do include special syntax for pairs\n\n\nmatch (1,2):\n  (x,y): x + y", 
            "title": "Datatypes and match"
        }, 
        {
            "location": "/lang/datatypes/#data-types-and-match", 
            "text": "Hakaru with several built-in data types.   pair  unit  either  bool", 
            "title": "Data types and Match"
        }, 
        {
            "location": "/lang/datatypes/#match", 
            "text": "We use  match  to deconstruct out data types\nand access their elements.  match left(3). either(int,bool):\n  left(x) : 1\n  right(x): 2  We do include special syntax for pairs  match (1,2):\n  (x,y): x + y", 
            "title": "Match"
        }, 
        {
            "location": "/lang/arrays/", 
            "text": "Arrays and Plate\n\n\nHakaru provides special syntax for arrays, which\nis distinct from the other data types.\n\n\nArrays\n\n\nTo construct arrays, we provide an index variable, size argument, and\nan expression body. This body is evaluated for each index of the\narray. For example, to construct the array \n[0,1,2,3]\n:\n\n\narray i of 4: i\n\n\n\n\nArray Literals\n\n\nWe can also create arrays using the literal syntax a comma delimited\nlist surrounded by brackets: \n[0,1,2,3]\n\n\nPlate\n\n\nBeyond, arrays Hakaru includes special syntax for describing measures\nover arrays called \nplate\n. Plate using the same syntax as \narray\n but\nthe body must have a measure type. It returns a measure over arrays.\nFor example, if we wish to have a distribution over three independent\nnormal distributions we would do so as follows:\n\n\nplate _ of 3: normal(0,1)\n\n\n\n\nArray size and indexing\n\n\nIf \na\n is an array, then \nsize(a)\n is its number of elements, which is a \nnat\n.\nIf \ni\n is a \nnat\n then \na[i]\n is the element of \na\n at index \ni\n.\nIndices start at zero, so the maximum valid value of \ni\n is \nsize(a)-1\n.\n\n\nLoops\n\n\nWe also express loops that compute sums (\nsummate\n) and products (\nproduct\n).\nThe syntax of these loops begins by declaring an \ninclusive\n lower bound and\nan \nexclusive\n upper bound.  For example, the factorial of \nn\n is not\n\nproduct i from 1 to n: i\n but rather \nproduct i from 1 to n+1: i\n.\nThis convention takes some getting used to but it makes it easy to deal\nwith arrays.  For example, if \na\n is an array of numbers then their sum is\n\nsummate i from 0 to size(a): a[i]\n.", 
            "title": "Arrays"
        }, 
        {
            "location": "/lang/arrays/#arrays-and-plate", 
            "text": "Hakaru provides special syntax for arrays, which\nis distinct from the other data types.", 
            "title": "Arrays and Plate"
        }, 
        {
            "location": "/lang/arrays/#arrays", 
            "text": "To construct arrays, we provide an index variable, size argument, and\nan expression body. This body is evaluated for each index of the\narray. For example, to construct the array  [0,1,2,3] :  array i of 4: i", 
            "title": "Arrays"
        }, 
        {
            "location": "/lang/arrays/#array-literals", 
            "text": "We can also create arrays using the literal syntax a comma delimited\nlist surrounded by brackets:  [0,1,2,3]", 
            "title": "Array Literals"
        }, 
        {
            "location": "/lang/arrays/#plate", 
            "text": "Beyond, arrays Hakaru includes special syntax for describing measures\nover arrays called  plate . Plate using the same syntax as  array  but\nthe body must have a measure type. It returns a measure over arrays.\nFor example, if we wish to have a distribution over three independent\nnormal distributions we would do so as follows:  plate _ of 3: normal(0,1)", 
            "title": "Plate"
        }, 
        {
            "location": "/lang/arrays/#array-size-and-indexing", 
            "text": "If  a  is an array, then  size(a)  is its number of elements, which is a  nat .\nIf  i  is a  nat  then  a[i]  is the element of  a  at index  i .\nIndices start at zero, so the maximum valid value of  i  is  size(a)-1 .", 
            "title": "Array size and indexing"
        }, 
        {
            "location": "/lang/arrays/#loops", 
            "text": "We also express loops that compute sums ( summate ) and products ( product ).\nThe syntax of these loops begins by declaring an  inclusive  lower bound and\nan  exclusive  upper bound.  For example, the factorial of  n  is not product i from 1 to n: i  but rather  product i from 1 to n+1: i .\nThis convention takes some getting used to but it makes it easy to deal\nwith arrays.  For example, if  a  is an array of numbers then their sum is summate i from 0 to size(a): a[i] .", 
            "title": "Loops"
        }, 
        {
            "location": "/transforms/expect/", 
            "text": "Expectation transformation\n\n\nThe expectation transformation takes a program representing a measure,\nand a function over the sample space, and returns a program computing\nthe expectation over that measure with respect to the given function.\n\n\nExpect\n\n\nExpect can be used inside programs with the \nexpect\n keyword.\n\n\nexpect x uniform(1,3):\n    real2prob(2*x + 1)\n\n\n\n\nThis program computes the expectation of \nuniform(1,3)\n using the\nfunction \n2*x + 1\n. This program expands to the following:\n\n\nintegrate x from 1 to 3: \n recip(real2prob(3 - 1)) * real2prob(2*x + 1)\n\n\n\n\nThis can be optimized by \nsimplify\n into \n5\n.\n\n\nNormalize\n\n\nWe also provide a \nnormalize\n command. This command takes as input a\nprogram representing any measure and reweights it into a program\nrepresenting a probability distribution.\n\n\nFor example in a slightly contrived example, we can weight a normal\ndistribution by two. Normalizing it will then remove this weight.\n\n\n echo \nweight(2, normal(0,1))\n | normalize | simplify -\nnormal(0, 1)", 
            "title": "Expect"
        }, 
        {
            "location": "/transforms/expect/#expectation-transformation", 
            "text": "The expectation transformation takes a program representing a measure,\nand a function over the sample space, and returns a program computing\nthe expectation over that measure with respect to the given function.", 
            "title": "Expectation transformation"
        }, 
        {
            "location": "/transforms/expect/#expect", 
            "text": "Expect can be used inside programs with the  expect  keyword.  expect x uniform(1,3):\n    real2prob(2*x + 1)  This program computes the expectation of  uniform(1,3)  using the\nfunction  2*x + 1 . This program expands to the following:  integrate x from 1 to 3: \n recip(real2prob(3 - 1)) * real2prob(2*x + 1)  This can be optimized by  simplify  into  5 .", 
            "title": "Expect"
        }, 
        {
            "location": "/transforms/expect/#normalize", 
            "text": "We also provide a  normalize  command. This command takes as input a\nprogram representing any measure and reweights it into a program\nrepresenting a probability distribution.  For example in a slightly contrived example, we can weight a normal\ndistribution by two. Normalizing it will then remove this weight.   echo  weight(2, normal(0,1))  | normalize | simplify -\nnormal(0, 1)", 
            "title": "Normalize"
        }, 
        {
            "location": "/transforms/disintegrate/", 
            "text": "Disintegrations transformation\n\n\nThe disintegration transformation takes as input a program\nrepresenting a joint probability distribution, and returns\na program which represents an posterior distribution.\n\n\nFor example, if we have the following joint distribution \nhello.hk\n\n\n\u03b8 \n~ normal(0,1)\nx \n~ normal(\u03b8,1)\nreturn (x,\u03b8)\n\n\n\n\nWhen we call \ndisintegrate hello.hk\n we obtain:\n\n\nfn x2 real: \n \u03b8 \n~ normal(0, 1)\n x7 \n~ weight((exp((negate(((x2 - \u03b8) ^ 2)) / 2))\n                / \n               1\n                / \n               sqrt((2 * pi))),\n              return ())\n return \u03b8\n\n\n\n\nThis represents the posterior on \n\u03b8\n given a value of \nx\n which\nhas been renamed \nx2\n.\n\n\nDensity\n\n\nFinding the density of a probability distribution at a particular\npoint is actually a special-case of disintegrate and is\ndefined in terms of it and the expectation transformation.\n\n\nWe also have a command \ndensity\n for doing so.\n\n\necho \nnormal(0,1)\n | density -\n\nfn x0 real: \n (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1)", 
            "title": "Disintegrate"
        }, 
        {
            "location": "/transforms/disintegrate/#disintegrations-transformation", 
            "text": "The disintegration transformation takes as input a program\nrepresenting a joint probability distribution, and returns\na program which represents an posterior distribution.  For example, if we have the following joint distribution  hello.hk  \u03b8  ~ normal(0,1)\nx  ~ normal(\u03b8,1)\nreturn (x,\u03b8)  When we call  disintegrate hello.hk  we obtain:  fn x2 real: \n \u03b8  ~ normal(0, 1)\n x7  ~ weight((exp((negate(((x2 - \u03b8) ^ 2)) / 2))\n                / \n               1\n                / \n               sqrt((2 * pi))),\n              return ())\n return \u03b8  This represents the posterior on  \u03b8  given a value of  x  which\nhas been renamed  x2 .", 
            "title": "Disintegrations transformation"
        }, 
        {
            "location": "/transforms/disintegrate/#density", 
            "text": "Finding the density of a probability distribution at a particular\npoint is actually a special-case of disintegrate and is\ndefined in terms of it and the expectation transformation.  We also have a command  density  for doing so.  echo  normal(0,1)  | density -\n\nfn x0 real: \n (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1)", 
            "title": "Density"
        }, 
        {
            "location": "/transforms/simplify/", 
            "text": "Simplify transformation\n\n\nThe simplify transformation provides a way to automaticaly improve our\nprograms. Simplify works by turning our programs into their expectation\nrepresentation and sending to Maple to be algebraically-simplified.\n\n\nFor example, the following represents a program from values of type\n\nprob\n to a measure of real numbers.\n\n\nfn a prob:\n  x \n~ normal(a,1)\n  y \n~ normal(x,1)\n  z \n~ normal(y,1)\n  return z\n\n\n\n\nAnd it will simplify to the following equivalent program.\n\n\nfn a prob: normal(prob2real(a), sqrt(3))", 
            "title": "Simplify"
        }, 
        {
            "location": "/transforms/simplify/#simplify-transformation", 
            "text": "The simplify transformation provides a way to automaticaly improve our\nprograms. Simplify works by turning our programs into their expectation\nrepresentation and sending to Maple to be algebraically-simplified.  For example, the following represents a program from values of type prob  to a measure of real numbers.  fn a prob:\n  x  ~ normal(a,1)\n  y  ~ normal(x,1)\n  z  ~ normal(y,1)\n  return z  And it will simplify to the following equivalent program.  fn a prob: normal(prob2real(a), sqrt(3))", 
            "title": "Simplify transformation"
        }, 
        {
            "location": "/transforms/mh/", 
            "text": "Metropolis Hastings transform\n\n\nIn Hakaru, all inference algorithms are represented as program\ntransformations. In particular, the Metropolis-Hastings transform\ntakes as input a probabilistic program representing the target\ndistribution, and a probabilistic program representing the proposal\ndistribution and returns a probabilistic program representing the MH\ntransition kernel.\n\n\nmh command\n\n\nYou can access this functionality using the \nmh\n command. It takes\ntwo files as input representing the target distribution and proposal\nkernel.\n\n\nFor example, suppose we would like to make a Markov Chain for the\nnormal distribution, where the proposal distribution is a random walk.\n\n\nTarget\n\n\n# target.hk\nnormal(0,1)\n\n\n\n\nProposal\n\n\n# proposal.hk\nfn x real: normal(x, 0.04)\n\n\n\n\nWe can use \nmh\n to create a transition kernel.\n\n\nmh target.hk proposal.hk\n\nx5 = x2 = fn x0 real: \n           (exp((negate(((x0 - nat2real(0)) ^ 2))\n                  / \n                 prob2real((2 * (nat2prob(1) ^ 2)))))\n             / \n            nat2prob(1)\n             / \n            sqrt((2 * pi))\n             / \n            1)\n     fn x1 real: \n      x0 \n~ normal(x1, (1/25))\n      return (x0, (x2(x0) / x2(x1)))\nfn x4 real: \n x3 \n~ x5(x4)\n (match x3: \n   (x1, x2): \n    x0 \n~ weight(min(1, x2), return true) \n|\n \n          weight(real2prob((prob2real(1) - prob2real(min(1, x2)))),\n                 return false)\n    return (match x0: \n             true: x1\n             false: x4))\n\n\n\n\n\nThis can then be simplified.\n\n\nmh target.hk proposal.hk | simplify -\n\nfn x4 real: \n x03 \n~ normal(x4, (1/25))\n weight(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))),\n        return x03) \n|\n \n weight(real2prob((1\n                    + \n                   (prob2real(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))))\n                     * \n                    (-1)))),\n        return x4)", 
            "title": "Metropolis Hastings"
        }, 
        {
            "location": "/transforms/mh/#metropolis-hastings-transform", 
            "text": "In Hakaru, all inference algorithms are represented as program\ntransformations. In particular, the Metropolis-Hastings transform\ntakes as input a probabilistic program representing the target\ndistribution, and a probabilistic program representing the proposal\ndistribution and returns a probabilistic program representing the MH\ntransition kernel.", 
            "title": "Metropolis Hastings transform"
        }, 
        {
            "location": "/transforms/mh/#mh-command", 
            "text": "You can access this functionality using the  mh  command. It takes\ntwo files as input representing the target distribution and proposal\nkernel.  For example, suppose we would like to make a Markov Chain for the\nnormal distribution, where the proposal distribution is a random walk.", 
            "title": "mh command"
        }, 
        {
            "location": "/transforms/mh/#target", 
            "text": "# target.hk\nnormal(0,1)", 
            "title": "Target"
        }, 
        {
            "location": "/transforms/mh/#proposal", 
            "text": "# proposal.hk\nfn x real: normal(x, 0.04)  We can use  mh  to create a transition kernel.  mh target.hk proposal.hk\n\nx5 = x2 = fn x0 real: \n           (exp((negate(((x0 - nat2real(0)) ^ 2))\n                  / \n                 prob2real((2 * (nat2prob(1) ^ 2)))))\n             / \n            nat2prob(1)\n             / \n            sqrt((2 * pi))\n             / \n            1)\n     fn x1 real: \n      x0  ~ normal(x1, (1/25))\n      return (x0, (x2(x0) / x2(x1)))\nfn x4 real: \n x3  ~ x5(x4)\n (match x3: \n   (x1, x2): \n    x0  ~ weight(min(1, x2), return true)  |  \n          weight(real2prob((prob2real(1) - prob2real(min(1, x2)))),\n                 return false)\n    return (match x0: \n             true: x1\n             false: x4))  This can then be simplified.  mh target.hk proposal.hk | simplify -\n\nfn x4 real: \n x03  ~ normal(x4, (1/25))\n weight(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))),\n        return x03)  |  \n weight(real2prob((1\n                    + \n                   (prob2real(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))))\n                     * \n                    (-1)))),\n        return x4)", 
            "title": "Proposal"
        }, 
        {
            "location": "/transforms/compile/", 
            "text": "Compiling to Haskell\n\n\nHakaru can be compiled to Haskell using the \ncompile\n command.\n\n\nFor example if we wish to compile \nexample.hk\n\n\nx \n~ normal(0,1)\ny \n~ normal(x,1)\nreturn y\n\n\n\n\nWe call \ncompile example.hk\n, which produces a file \nexample.hs\n.\n\n\nThis is a regular Haskell file, which can then be furthered compiled into\nmachine code.\n\n\ncat example.hs\n\n\n\n\n{-# LANGUAGE DataKinds, NegativeLiterals #-}\nmodule Main where\n\nimport           Prelude                          hiding (product)\nimport           Language.Hakaru.Runtime.Prelude\nimport           Language.Hakaru.Types.Sing\nimport qualified System.Random.MWC                as MWC\nimport           Control.Monad\n\nprog = \n  normal (nat2real (nat_ 0)) (nat2prob (nat_ 1)) \n= \\ x0 -\n\n  normal x0 (nat2prob (nat_ 1)) \n= \\ y1 -\n\n  dirac y1\n\nmain :: IO ()\nmain = do\n  g \n- MWC.createSystemRandom\n  forever $ run g prog", 
            "title": "Compiling to Haskell"
        }, 
        {
            "location": "/transforms/compile/#compiling-to-haskell", 
            "text": "Hakaru can be compiled to Haskell using the  compile  command.  For example if we wish to compile  example.hk  x  ~ normal(0,1)\ny  ~ normal(x,1)\nreturn y  We call  compile example.hk , which produces a file  example.hs .  This is a regular Haskell file, which can then be furthered compiled into\nmachine code.  cat example.hs  {-# LANGUAGE DataKinds, NegativeLiterals #-}\nmodule Main where\n\nimport           Prelude                          hiding (product)\nimport           Language.Hakaru.Runtime.Prelude\nimport           Language.Hakaru.Types.Sing\nimport qualified System.Random.MWC                as MWC\nimport           Control.Monad\n\nprog = \n  normal (nat2real (nat_ 0)) (nat2prob (nat_ 1))  = \\ x0 - \n  normal x0 (nat2prob (nat_ 1))  = \\ y1 - \n  dirac y1\n\nmain :: IO ()\nmain = do\n  g  - MWC.createSystemRandom\n  forever $ run g prog", 
            "title": "Compiling to Haskell"
        }, 
        {
            "location": "/transforms/hkc/", 
            "text": "HKC Compilation\n\n\nhkc\n is a command line tool to compiler Hakaru programs to C. HKC was\ncreated with portability and speed in mind. More recently, OpenMP support is\nbeing added to gain more performance on multi-core machines. Basic command line\nusage of HKC is much like other compilers:\n\n\nhkc foo.hk -o foo.c\n\n\n\n\nIt is possible to go straight to an executable with the \n--make ARG\n flag, where\nthe argument is the C compiler you would like to use.\n\n\nType Conversions\n\n\nThe types available in Hakaru programs are the following: \nnat\n, \nint\n, \nreal\n,\n\nprob\n, \narray(\ntype\n)\n, \nmeasure(\ntype\n)\n, and datum like \ntrue\n and \nfalse\n.\n\n\nnat\n and \nint\n have a trivial mapping to the C \nint\n type. \nreal\n becomes a C\n\ndouble\n. The \nprob\n type in Hakaru is stored in the log-domain to avoid\nunderflow. In C this corresponds to a \ndouble\n, but we first take the log of it\nbefore storing it, so we have to take the exp of it to bring it back to the real\nnumbers.\n\n\nArrays become structs that contain the size and a pointer to data stored within.\nThe structs are generated at compile time, but there are only four which are\nnamed after the type they contain. Here they all are:\n\n\nstruct arrayNat {\n  int size; int * data;\n};\n\nstruct arrayInt {\n  int size; int * data;\n};\n\nstruct arrayReal {\n  int size; double * data;\n};\n\nstruct arrayProb {\n  int size; double * data;\n};\n\n\n\n\nMeasures\n\n\nMeasures compile to C functions that take a location for a sample, return the\nweight of the measure and store a sample in the location is was given. A simple\nexample is \nuniform(0,1)\n a measure over type \nreal\n.\n\n\n#include \ntime.h\n\n#include \nstdlib.h\n\n#include \nstdio.h\n\n#include \nmath.h\n\n\ndouble measure(double * s_a)\n {\n    *s_a = ((double)0) + ((double)rand()) / ((double)RAND_MAX) * ((double)1) - ((double)0);\n    return 0;\n }\n\nint main()\n {\n    double sample;\n    while (1)\n    {\n        measure(\nsample);\n        printf(\n%.17f\\n\n,sample);\n    }\n    return 0;\n }\n\n\n\n\nRecall that weights have type \nprob\n and are stored in the log-domain. This\nexample has a weight of 1.\n\n\nCalling \nhkc\n on a measure will create a function like the one above and also a\nmain function that infinitely takes samples. Using \nhkc -F ARG\n will produce\njust the function with the name of its argument.\n\n\nLambdas\n\n\nLambdas compile to functions in C:\n\n\nfn x array(real):\n  (summate i from 0 to size(x): x[i])\n   *\n  prob2real(recip(nat2prob((size(x) + 1))))\n\n\n\n\n\nBecomes:\n\n\n#include \nstdlib.h\n\n#include \nstdio.h\n\n#include \nmath.h\n\n\nstruct arrayReal {\n   int size; double * data;\n };\n\ndouble fn_a(struct arrayReal x_b)\n {\n   unsigned int i_c;\n   double acc_d;\n   double p_e;\n   double _f;\n   double r_g;\n   acc_d = 0;\n   for (i_c = 0; i_c \n x_b.size; i_c++)\n   {\n     acc_d += *(x_b.data + i_c);\n   }\n   p_e = log1p(((1 + x_b.size) - 1));\n   _f = -p_e;\n   r_g = (expm1(_f) + 1);\n   return (r_g * acc_d);\n }\n\n\n\n\nUsing the \n-F\n flag will allow the user to add their own name to a function,\notherwise the name is chosen automatically as \nfn_\nunique identifier\n.\n\n\nComputations\n\n\nWhen compiling a computation, HKC just creates a main function to compute the\nvalue and print it. For example:\n\n\nsummate i from 1 to 100000000:\n  nat2real(i) / nat2real(i)\n\n\n\n\nbecomes:\n\n\n#include \nstdlib.h\n\n#include \nstdio.h\n\n#include \nmath.h\n\n\nint main()\n {\n    double result;\n    int i_a;\n    double acc_b;\n    double _c;\n    acc_b = 0;\n    for (i_a = 1; i_a \n 100000000; i_a++)\n    {\n       _c = (1 / ((double)i_a));\n       acc_b += (_c * ((double)i_a));\n    }\n    result = acc_b;\n    printf(\n%.17f\\n\n,result);\n    return 0;\n }\n\n\n\n\nParallel Programs\n\n\nCalling HKC with the \n-j\n flag will generate the code with parallel regions to\ncompute the value. The parallel code uses OpenMP directives. To check if you\nre\ncompiler supports OpenMP, check \nhere\n.\n\n\nFor example, GCC requires the \n-fopenmp\n flag for OpenMP support:\n\n\nhkc -j foo.hk -o foo.c\ngcc -lm -fopenmp foo.c -o foo.bin", 
            "title": "Compiling to C"
        }, 
        {
            "location": "/transforms/hkc/#hkc-compilation", 
            "text": "hkc  is a command line tool to compiler Hakaru programs to C. HKC was\ncreated with portability and speed in mind. More recently, OpenMP support is\nbeing added to gain more performance on multi-core machines. Basic command line\nusage of HKC is much like other compilers:  hkc foo.hk -o foo.c  It is possible to go straight to an executable with the  --make ARG  flag, where\nthe argument is the C compiler you would like to use.", 
            "title": "HKC Compilation"
        }, 
        {
            "location": "/transforms/hkc/#type-conversions", 
            "text": "The types available in Hakaru programs are the following:  nat ,  int ,  real , prob ,  array( type ) ,  measure( type ) , and datum like  true  and  false .  nat  and  int  have a trivial mapping to the C  int  type.  real  becomes a C double . The  prob  type in Hakaru is stored in the log-domain to avoid\nunderflow. In C this corresponds to a  double , but we first take the log of it\nbefore storing it, so we have to take the exp of it to bring it back to the real\nnumbers.  Arrays become structs that contain the size and a pointer to data stored within.\nThe structs are generated at compile time, but there are only four which are\nnamed after the type they contain. Here they all are:  struct arrayNat {\n  int size; int * data;\n};\n\nstruct arrayInt {\n  int size; int * data;\n};\n\nstruct arrayReal {\n  int size; double * data;\n};\n\nstruct arrayProb {\n  int size; double * data;\n};", 
            "title": "Type Conversions"
        }, 
        {
            "location": "/transforms/hkc/#measures", 
            "text": "Measures compile to C functions that take a location for a sample, return the\nweight of the measure and store a sample in the location is was given. A simple\nexample is  uniform(0,1)  a measure over type  real .  #include  time.h \n#include  stdlib.h \n#include  stdio.h \n#include  math.h \n\ndouble measure(double * s_a)\n {\n    *s_a = ((double)0) + ((double)rand()) / ((double)RAND_MAX) * ((double)1) - ((double)0);\n    return 0;\n }\n\nint main()\n {\n    double sample;\n    while (1)\n    {\n        measure( sample);\n        printf( %.17f\\n ,sample);\n    }\n    return 0;\n }  Recall that weights have type  prob  and are stored in the log-domain. This\nexample has a weight of 1.  Calling  hkc  on a measure will create a function like the one above and also a\nmain function that infinitely takes samples. Using  hkc -F ARG  will produce\njust the function with the name of its argument.", 
            "title": "Measures"
        }, 
        {
            "location": "/transforms/hkc/#lambdas", 
            "text": "Lambdas compile to functions in C:  fn x array(real):\n  (summate i from 0 to size(x): x[i])\n   *\n  prob2real(recip(nat2prob((size(x) + 1))))  Becomes:  #include  stdlib.h \n#include  stdio.h \n#include  math.h \n\nstruct arrayReal {\n   int size; double * data;\n };\n\ndouble fn_a(struct arrayReal x_b)\n {\n   unsigned int i_c;\n   double acc_d;\n   double p_e;\n   double _f;\n   double r_g;\n   acc_d = 0;\n   for (i_c = 0; i_c   x_b.size; i_c++)\n   {\n     acc_d += *(x_b.data + i_c);\n   }\n   p_e = log1p(((1 + x_b.size) - 1));\n   _f = -p_e;\n   r_g = (expm1(_f) + 1);\n   return (r_g * acc_d);\n }  Using the  -F  flag will allow the user to add their own name to a function,\notherwise the name is chosen automatically as  fn_ unique identifier .", 
            "title": "Lambdas"
        }, 
        {
            "location": "/transforms/hkc/#computations", 
            "text": "When compiling a computation, HKC just creates a main function to compute the\nvalue and print it. For example:  summate i from 1 to 100000000:\n  nat2real(i) / nat2real(i)  becomes:  #include  stdlib.h \n#include  stdio.h \n#include  math.h \n\nint main()\n {\n    double result;\n    int i_a;\n    double acc_b;\n    double _c;\n    acc_b = 0;\n    for (i_a = 1; i_a   100000000; i_a++)\n    {\n       _c = (1 / ((double)i_a));\n       acc_b += (_c * ((double)i_a));\n    }\n    result = acc_b;\n    printf( %.17f\\n ,result);\n    return 0;\n }", 
            "title": "Computations"
        }, 
        {
            "location": "/transforms/hkc/#parallel-programs", 
            "text": "Calling HKC with the  -j  flag will generate the code with parallel regions to\ncompute the value. The parallel code uses OpenMP directives. To check if you re\ncompiler supports OpenMP, check  here .  For example, GCC requires the  -fopenmp  flag for OpenMP support:  hkc -j foo.hk -o foo.c\ngcc -lm -fopenmp foo.c -o foo.bin", 
            "title": "Parallel Programs"
        }, 
        {
            "location": "/internals/ast/", 
            "text": "Internal Representation of Hakaru terms\n\n\nThe Hakaru AST can be found defined in\n\nhaskell/Language/Hakaru/Syntax/AST.hs\n. It is made up of several parts which this section and the next one will explain.\n\n\nWe should note, this datatype makes use of\n\nAbstract Binding Trees\n\nwhich we discuss in more detail in the next\n\nsection\n. ABTs can be understood as a way to abstract\nthe use of variables in the AST. The advantage of this is it allows\nall variable substitution and manipulation logic to live in one place\nand not be specific to a particular AST.\n\n\nDatakind\n\n\nThe AST is typed using the Hakaru kind, defined in \nhaskell/Language/Types/DataKind.hs\n. All Hakaru types are defined in terms of\nthe primitives in this datakind.\n\n\n-- | The universe\\/kind of Hakaru types.\ndata Hakaru\n    = HNat -- ^ The natural numbers; aka, the non-negative integers.\n\n    -- | The integers.\n    | HInt\n\n    -- | Non-negative real numbers. Unlike what you might expect,\n    -- this is /not/ restructed to the @[0,1]@ interval!\n    | HProb\n\n    -- | The affinely extended real number line. That is, the real\n    -- numbers extended with positive and negative infinities.\n    | HReal\n\n    -- | The measure monad\n    | HMeasure !Hakaru\n\n    -- | The built-in type for uniform arrays.\n    | HArray !Hakaru\n\n    -- | The type of Hakaru functions.\n    | !Hakaru :-\n !Hakaru\n\n    -- | A user-defined polynomial datatype. Each such type is\n    -- specified by a \\\ntag\\\n (the @HakaruCon@) which names the type, and a sum-of-product representation of the type itself.\n    | HData !HakaruCon [[HakaruFun]]\n\n\n\n\n\nPlease read Datakind.hs for more details.\n\n\nTerm\n\n\nThe Term datatype includes all the syntactic constructions for the Hakaru language.\nFor all those where we know the number of arguments we expect that language construct\nto get, we define the \n(:$)\n constructor, which takes \nSCons\n and \nSArgs\n datatypes\nas arguments.\n\n\n-- | The generating functor for Hakaru ASTs. This type is given in\n-- open-recursive form, where the first type argument gives the\n-- recursive form. The recursive form @abt@ does not have exactly\n-- the same kind as @Term abt@ because every 'Term' represents a\n-- locally-closed term whereas the underlying @abt@ may bind some\n-- variables.\ndata Term :: ([Hakaru] -\n Hakaru -\n *) -\n Hakaru -\n * where\n    -- Simple syntactic forms (i.e., generalized quantifiers)\n    (:$) :: !(SCon args a) -\n !(SArgs abt args) -\n Term abt a\n\n    -- N-ary operators\n    NaryOp_ :: !(NaryOp a) -\n !(Seq (abt '[] a)) -\n Term abt a\n\n    -- Literal\\/Constant values\n    Literal_ :: !(Literal a) -\n Term abt a\n\n    Empty_ :: !(Sing ('HArray a)) -\n Term abt ('HArray a)\n    Array_\n        :: !(abt '[] 'HNat)\n        -\n !(abt '[ 'HNat ] a)\n        -\n Term abt ('HArray a)\n\n    -- -- User-defined data types\n    -- A data constructor applied to some expressions. N.B., this\n    -- definition only accounts for data constructors which are\n    -- fully saturated. Unsaturated constructors will need to be\n    -- eta-expanded.\n    Datum_ :: !(Datum (abt '[]) (HData' t)) -\n Term abt (HData' t)\n\n    -- Generic case-analysis (via ABTs and Structural Focalization).\n    Case_ :: !(abt '[] a) -\n [Branch a abt b] -\n Term abt b\n\n    -- Linear combinations of measures.\n    Superpose_\n        :: L.NonEmpty (abt '[] 'HProb, abt '[] ('HMeasure a))\n        -\n Term abt ('HMeasure a)\n\n    Reject_ :: !(Sing ('HMeasure a)) -\n Term abt ('HMeasure a)\n\n\n\n\nSCons and SArgs\n\n\nWhen using \n(:$)\n we have a way to describe primitives where we\nknow the number of arguments they should get. In that regard,\nSArgs is a typed list of abt terms indexed by its size.\n\n\n-- | The arguments to a @(':$')@ node in the 'Term'; that is, a list\n-- of ASTs, where the whole list is indexed by a (type-level) list\n-- of the indices of each element.\ndata SArgs :: ([Hakaru] -\n Hakaru -\n *) -\n [([Hakaru], Hakaru)] -\n *\n    where\n    End :: SArgs abt '[]\n    (:*) :: !(abt vars a)\n        -\n !(SArgs abt args)\n        -\n SArgs abt ( '(vars, a) ': args)\n\n\n\n\nThese are combined with SCons which describes the constructor, and\nthe types it expects for its arguments. For example suppose we had\nan AST for a function \nf\n and it\ns argument \nx\n, we could construct\na Term for applying \nf\n to \nx\n by writing \nApp_:$ f :* x :* End\n.\n\n\n-- | The constructor of a @(':$')@ node in the 'Term'. Each of these\n-- constructors denotes a \\\nnormal\\/standard\\/basic\\\n syntactic\n-- form (i.e., a generalized quantifier). In the literature, these\n-- syntactic forms are sometimes called \\\noperators\\\n, but we avoid\n-- calling them that so as not to introduce confusion vs 'PrimOp'\n-- etc. Instead we use the term \\\noperator\\\n to refer to any primitive\n-- function or constant; that is, non-binding syntactic forms. Also\n-- in the literature, the 'SCon' type itself is usually called the\n-- \\\nsignature\\\n of the term language. However, we avoid calling\n-- it that since our 'Term' has constructors other than just @(:$)@,\n-- so 'SCon' does not give a complete signature for our terms.\n--\n-- The main reason for breaking this type out and using it in\n-- conjunction with @(':$')@ and 'SArgs' is so that we can easily\n-- pattern match on /fully saturated/ nodes. For example, we want\n-- to be able to match @MeasureOp_ Uniform :$ lo :* hi :* End@\n-- without needing to deal with 'App_' nodes nor 'viewABT'.\ndata SCon :: [([Hakaru], Hakaru)] -\n Hakaru -\n * where\n    Lam_ :: SCon '[ '( '[ a ], b ) ] (a ':-\n b)\n    App_ :: SCon '[ LC (a ':-\n b ), LC a ] b\n    Let_ :: SCon '[ LC a, '( '[ a ], b ) ] b\n\n    CoerceTo_   :: !(Coercion a b) -\n SCon '[ LC a ] b\n    UnsafeFrom_ :: !(Coercion a b) -\n SCon '[ LC b ] a\n\n    PrimOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        =\n !(PrimOp typs a) -\n SCon args a\n    ArrayOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        =\n !(ArrayOp typs a) -\n SCon args a\n    MeasureOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        =\n !(MeasureOp typs a) -\n SCon args ('HMeasure a)\n\n    Dirac :: SCon '[ LC a ] ('HMeasure a)\n\n    MBind :: SCon\n        '[ LC ('HMeasure a)\n        ,  '( '[ a ], 'HMeasure b)\n        ] ('HMeasure b)\n\n    Plate :: SCon\n        '[ LC 'HNat\n        , '( '[ 'HNat ], 'HMeasure a)\n        ] ('HMeasure ('HArray a))\n\n    Chain :: SCon\n        '[ LC 'HNat, LC s\n        , '( '[ s ],  'HMeasure (HPair a s))\n        ] ('HMeasure (HPair ('HArray a) s))\n\n    Integrate\n        :: SCon '[ LC 'HReal, LC 'HReal, '( '[ 'HReal ], 'HProb) ] 'HProb\n\n    Summate\n        :: HDiscrete a\n        -\n HSemiring b\n        -\n SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Product\n        :: HDiscrete a\n        -\n HSemiring b\n        -\n SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Expect :: SCon '[ LC ('HMeasure a), '( '[ a ], 'HProb) ] 'HProb\n\n    Observe :: SCon '[ LC ('HMeasure a), LC a ] ('HMeasure a)\n\n\n\n\nYou\nll notice in \nSCon\n there are definitions for PrimOp, MeasureOp, and ArrayOp\nthese are done more organizational purposes and have constructions for the\ndifferent categories of primitives.\n\n\nMeasureOp\n\n\nPrimitives of type measure are defined in MeasureOp.\n\n\n-- | Primitive operators to produce, consume, or transform\n-- distributions\\/measures. This corresponds to the old @Mochastic@\n-- class, except that 'MBind' and 'Superpose_' are handled elsewhere\n-- since they are not simple operators. (Also 'Dirac' is handled\n-- elsewhere since it naturally fits with 'MBind', even though it\n-- is a siple operator.)\ndata MeasureOp :: [Hakaru] -\n Hakaru -\n * where\n    Lebesgue    :: MeasureOp '[]                 'HReal\n    Counting    :: MeasureOp '[]                 'HInt\n    Categorical :: MeasureOp '[ 'HArray 'HProb ] 'HNat\n    Uniform     :: MeasureOp '[ 'HReal, 'HReal ] 'HReal\n    Normal      :: MeasureOp '[ 'HReal, 'HProb ] 'HReal\n    Poisson     :: MeasureOp '[ 'HProb         ] 'HNat\n    Gamma       :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n    Beta        :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n\n\n\n\nArrayOp\n\n\nPrimitives that involve manipulating value of type array,\nend up in ArrayOp.\n\n\n-- | Primitive operators for consuming or transforming arrays.\ndata ArrayOp :: [Hakaru] -\n Hakaru -\n * where\n    Index  :: !(Sing a) -\n ArrayOp '[ 'HArray a, 'HNat ] a\n    Size   :: !(Sing a) -\n ArrayOp '[ 'HArray a ] 'HNat\n    Reduce :: !(Sing a) -\n ArrayOp '[ a ':-\n a ':-\n a, a, 'HArray a ] a\n\n\n\n\nPrimOp\n\n\nAll primitive operations which don\nt return something\nof type array or measure are placed in PrimOp\n\n\n-- | Simple primitive functions, and constants.\ndata PrimOp :: [Hakaru] -\n Hakaru -\n * where\n\n    -- -- -- Here we have /monomorphic/ operators\n    -- -- The Boolean operators\n    Not  :: PrimOp '[ HBool ] HBool\n    -- And, Or, Xor, Iff\n    Impl :: PrimOp '[ HBool, HBool ] HBool\n    -- Impl x y == Or (Not x) y\n    Diff :: PrimOp '[ HBool, HBool ] HBool\n    -- Diff x y == Not (Impl x y)\n    Nand :: PrimOp '[ HBool, HBool ] HBool\n    -- Nand aka Alternative Denial, Sheffer stroke\n    Nor  :: PrimOp '[ HBool, HBool ] HBool\n    -- Nor aka Joint Denial, aka Quine dagger, aka Pierce arrow\n\n    -- -- Trigonometry operators\n    Pi    :: PrimOp '[] 'HProb\n    Sin   :: PrimOp '[ 'HReal ] 'HReal\n    Cos   :: PrimOp '[ 'HReal ] 'HReal\n    Tan   :: PrimOp '[ 'HReal ] 'HReal\n    Asin  :: PrimOp '[ 'HReal ] 'HReal\n    Acos  :: PrimOp '[ 'HReal ] 'HReal\n    Atan  :: PrimOp '[ 'HReal ] 'HReal\n    Sinh  :: PrimOp '[ 'HReal ] 'HReal\n    Cosh  :: PrimOp '[ 'HReal ] 'HReal\n    Tanh  :: PrimOp '[ 'HReal ] 'HReal\n    Asinh :: PrimOp '[ 'HReal ] 'HReal\n    Acosh :: PrimOp '[ 'HReal ] 'HReal\n    Atanh :: PrimOp '[ 'HReal ] 'HReal\n\n    -- -- Other Real\\/Prob-valued operators\n    RealPow   :: PrimOp '[ 'HProb, 'HReal ] 'HProb\n    Exp       :: PrimOp '[ 'HReal ] 'HProb\n    Log       :: PrimOp '[ 'HProb ] 'HReal\n    Infinity  :: HIntegrable a -\n PrimOp '[] a\n    GammaFunc :: PrimOp '[ 'HReal ] 'HProb\n    BetaFunc  :: PrimOp '[ 'HProb, 'HProb ] 'HProb\n\n    -- -- -- Here we have the /polymorphic/ operators\n    -- -- HEq and HOrd operators\n    Equal :: !(HEq  a) -\n PrimOp '[ a, a ] HBool\n    Less  :: !(HOrd a) -\n PrimOp '[ a, a ] HBool\n\n    -- -- HSemiring operators (the non-n-ary ones)\n    NatPow :: !(HSemiring a) -\n PrimOp '[ a, 'HNat ] a\n\n    -- -- HRing operators\n    Negate :: !(HRing a) -\n PrimOp '[ a ] a\n    Abs    :: !(HRing a) -\n PrimOp '[ a ] (NonNegative a)\n    Signum :: !(HRing a) -\n PrimOp '[ a ] a\n\n    -- -- HFractional operators\n    Recip :: !(HFractional a) -\n PrimOp '[ a ] a\n\n    -- -- HRadical operators\n    NatRoot :: !(HRadical a) -\n PrimOp '[ a, 'HNat ] a\n\n    -- -- HContinuous operators\n    Erf :: !(HContinuous a) -\n PrimOp '[ a ] a", 
            "title": "AST and Hakaru Datakind"
        }, 
        {
            "location": "/internals/ast/#internal-representation-of-hakaru-terms", 
            "text": "The Hakaru AST can be found defined in haskell/Language/Hakaru/Syntax/AST.hs . It is made up of several parts which this section and the next one will explain.  We should note, this datatype makes use of Abstract Binding Trees \nwhich we discuss in more detail in the next section . ABTs can be understood as a way to abstract\nthe use of variables in the AST. The advantage of this is it allows\nall variable substitution and manipulation logic to live in one place\nand not be specific to a particular AST.", 
            "title": "Internal Representation of Hakaru terms"
        }, 
        {
            "location": "/internals/ast/#datakind", 
            "text": "The AST is typed using the Hakaru kind, defined in  haskell/Language/Types/DataKind.hs . All Hakaru types are defined in terms of\nthe primitives in this datakind.  -- | The universe\\/kind of Hakaru types.\ndata Hakaru\n    = HNat -- ^ The natural numbers; aka, the non-negative integers.\n\n    -- | The integers.\n    | HInt\n\n    -- | Non-negative real numbers. Unlike what you might expect,\n    -- this is /not/ restructed to the @[0,1]@ interval!\n    | HProb\n\n    -- | The affinely extended real number line. That is, the real\n    -- numbers extended with positive and negative infinities.\n    | HReal\n\n    -- | The measure monad\n    | HMeasure !Hakaru\n\n    -- | The built-in type for uniform arrays.\n    | HArray !Hakaru\n\n    -- | The type of Hakaru functions.\n    | !Hakaru :-  !Hakaru\n\n    -- | A user-defined polynomial datatype. Each such type is\n    -- specified by a \\ tag\\  (the @HakaruCon@) which names the type, and a sum-of-product representation of the type itself.\n    | HData !HakaruCon [[HakaruFun]]  Please read Datakind.hs for more details.", 
            "title": "Datakind"
        }, 
        {
            "location": "/internals/ast/#term", 
            "text": "The Term datatype includes all the syntactic constructions for the Hakaru language.\nFor all those where we know the number of arguments we expect that language construct\nto get, we define the  (:$)  constructor, which takes  SCons  and  SArgs  datatypes\nas arguments.  -- | The generating functor for Hakaru ASTs. This type is given in\n-- open-recursive form, where the first type argument gives the\n-- recursive form. The recursive form @abt@ does not have exactly\n-- the same kind as @Term abt@ because every 'Term' represents a\n-- locally-closed term whereas the underlying @abt@ may bind some\n-- variables.\ndata Term :: ([Hakaru] -  Hakaru -  *) -  Hakaru -  * where\n    -- Simple syntactic forms (i.e., generalized quantifiers)\n    (:$) :: !(SCon args a) -  !(SArgs abt args) -  Term abt a\n\n    -- N-ary operators\n    NaryOp_ :: !(NaryOp a) -  !(Seq (abt '[] a)) -  Term abt a\n\n    -- Literal\\/Constant values\n    Literal_ :: !(Literal a) -  Term abt a\n\n    Empty_ :: !(Sing ('HArray a)) -  Term abt ('HArray a)\n    Array_\n        :: !(abt '[] 'HNat)\n        -  !(abt '[ 'HNat ] a)\n        -  Term abt ('HArray a)\n\n    -- -- User-defined data types\n    -- A data constructor applied to some expressions. N.B., this\n    -- definition only accounts for data constructors which are\n    -- fully saturated. Unsaturated constructors will need to be\n    -- eta-expanded.\n    Datum_ :: !(Datum (abt '[]) (HData' t)) -  Term abt (HData' t)\n\n    -- Generic case-analysis (via ABTs and Structural Focalization).\n    Case_ :: !(abt '[] a) -  [Branch a abt b] -  Term abt b\n\n    -- Linear combinations of measures.\n    Superpose_\n        :: L.NonEmpty (abt '[] 'HProb, abt '[] ('HMeasure a))\n        -  Term abt ('HMeasure a)\n\n    Reject_ :: !(Sing ('HMeasure a)) -  Term abt ('HMeasure a)", 
            "title": "Term"
        }, 
        {
            "location": "/internals/ast/#scons-and-sargs", 
            "text": "When using  (:$)  we have a way to describe primitives where we\nknow the number of arguments they should get. In that regard,\nSArgs is a typed list of abt terms indexed by its size.  -- | The arguments to a @(':$')@ node in the 'Term'; that is, a list\n-- of ASTs, where the whole list is indexed by a (type-level) list\n-- of the indices of each element.\ndata SArgs :: ([Hakaru] -  Hakaru -  *) -  [([Hakaru], Hakaru)] -  *\n    where\n    End :: SArgs abt '[]\n    (:*) :: !(abt vars a)\n        -  !(SArgs abt args)\n        -  SArgs abt ( '(vars, a) ': args)  These are combined with SCons which describes the constructor, and\nthe types it expects for its arguments. For example suppose we had\nan AST for a function  f  and it s argument  x , we could construct\na Term for applying  f  to  x  by writing  App_:$ f :* x :* End .  -- | The constructor of a @(':$')@ node in the 'Term'. Each of these\n-- constructors denotes a \\ normal\\/standard\\/basic\\  syntactic\n-- form (i.e., a generalized quantifier). In the literature, these\n-- syntactic forms are sometimes called \\ operators\\ , but we avoid\n-- calling them that so as not to introduce confusion vs 'PrimOp'\n-- etc. Instead we use the term \\ operator\\  to refer to any primitive\n-- function or constant; that is, non-binding syntactic forms. Also\n-- in the literature, the 'SCon' type itself is usually called the\n-- \\ signature\\  of the term language. However, we avoid calling\n-- it that since our 'Term' has constructors other than just @(:$)@,\n-- so 'SCon' does not give a complete signature for our terms.\n--\n-- The main reason for breaking this type out and using it in\n-- conjunction with @(':$')@ and 'SArgs' is so that we can easily\n-- pattern match on /fully saturated/ nodes. For example, we want\n-- to be able to match @MeasureOp_ Uniform :$ lo :* hi :* End@\n-- without needing to deal with 'App_' nodes nor 'viewABT'.\ndata SCon :: [([Hakaru], Hakaru)] -  Hakaru -  * where\n    Lam_ :: SCon '[ '( '[ a ], b ) ] (a ':-  b)\n    App_ :: SCon '[ LC (a ':-  b ), LC a ] b\n    Let_ :: SCon '[ LC a, '( '[ a ], b ) ] b\n\n    CoerceTo_   :: !(Coercion a b) -  SCon '[ LC a ] b\n    UnsafeFrom_ :: !(Coercion a b) -  SCon '[ LC b ] a\n\n    PrimOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        =  !(PrimOp typs a) -  SCon args a\n    ArrayOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        =  !(ArrayOp typs a) -  SCon args a\n    MeasureOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        =  !(MeasureOp typs a) -  SCon args ('HMeasure a)\n\n    Dirac :: SCon '[ LC a ] ('HMeasure a)\n\n    MBind :: SCon\n        '[ LC ('HMeasure a)\n        ,  '( '[ a ], 'HMeasure b)\n        ] ('HMeasure b)\n\n    Plate :: SCon\n        '[ LC 'HNat\n        , '( '[ 'HNat ], 'HMeasure a)\n        ] ('HMeasure ('HArray a))\n\n    Chain :: SCon\n        '[ LC 'HNat, LC s\n        , '( '[ s ],  'HMeasure (HPair a s))\n        ] ('HMeasure (HPair ('HArray a) s))\n\n    Integrate\n        :: SCon '[ LC 'HReal, LC 'HReal, '( '[ 'HReal ], 'HProb) ] 'HProb\n\n    Summate\n        :: HDiscrete a\n        -  HSemiring b\n        -  SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Product\n        :: HDiscrete a\n        -  HSemiring b\n        -  SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Expect :: SCon '[ LC ('HMeasure a), '( '[ a ], 'HProb) ] 'HProb\n\n    Observe :: SCon '[ LC ('HMeasure a), LC a ] ('HMeasure a)  You ll notice in  SCon  there are definitions for PrimOp, MeasureOp, and ArrayOp\nthese are done more organizational purposes and have constructions for the\ndifferent categories of primitives.", 
            "title": "SCons and SArgs"
        }, 
        {
            "location": "/internals/ast/#measureop", 
            "text": "Primitives of type measure are defined in MeasureOp.  -- | Primitive operators to produce, consume, or transform\n-- distributions\\/measures. This corresponds to the old @Mochastic@\n-- class, except that 'MBind' and 'Superpose_' are handled elsewhere\n-- since they are not simple operators. (Also 'Dirac' is handled\n-- elsewhere since it naturally fits with 'MBind', even though it\n-- is a siple operator.)\ndata MeasureOp :: [Hakaru] -  Hakaru -  * where\n    Lebesgue    :: MeasureOp '[]                 'HReal\n    Counting    :: MeasureOp '[]                 'HInt\n    Categorical :: MeasureOp '[ 'HArray 'HProb ] 'HNat\n    Uniform     :: MeasureOp '[ 'HReal, 'HReal ] 'HReal\n    Normal      :: MeasureOp '[ 'HReal, 'HProb ] 'HReal\n    Poisson     :: MeasureOp '[ 'HProb         ] 'HNat\n    Gamma       :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n    Beta        :: MeasureOp '[ 'HProb, 'HProb ] 'HProb", 
            "title": "MeasureOp"
        }, 
        {
            "location": "/internals/ast/#arrayop", 
            "text": "Primitives that involve manipulating value of type array,\nend up in ArrayOp.  -- | Primitive operators for consuming or transforming arrays.\ndata ArrayOp :: [Hakaru] -  Hakaru -  * where\n    Index  :: !(Sing a) -  ArrayOp '[ 'HArray a, 'HNat ] a\n    Size   :: !(Sing a) -  ArrayOp '[ 'HArray a ] 'HNat\n    Reduce :: !(Sing a) -  ArrayOp '[ a ':-  a ':-  a, a, 'HArray a ] a", 
            "title": "ArrayOp"
        }, 
        {
            "location": "/internals/ast/#primop", 
            "text": "All primitive operations which don t return something\nof type array or measure are placed in PrimOp  -- | Simple primitive functions, and constants.\ndata PrimOp :: [Hakaru] -  Hakaru -  * where\n\n    -- -- -- Here we have /monomorphic/ operators\n    -- -- The Boolean operators\n    Not  :: PrimOp '[ HBool ] HBool\n    -- And, Or, Xor, Iff\n    Impl :: PrimOp '[ HBool, HBool ] HBool\n    -- Impl x y == Or (Not x) y\n    Diff :: PrimOp '[ HBool, HBool ] HBool\n    -- Diff x y == Not (Impl x y)\n    Nand :: PrimOp '[ HBool, HBool ] HBool\n    -- Nand aka Alternative Denial, Sheffer stroke\n    Nor  :: PrimOp '[ HBool, HBool ] HBool\n    -- Nor aka Joint Denial, aka Quine dagger, aka Pierce arrow\n\n    -- -- Trigonometry operators\n    Pi    :: PrimOp '[] 'HProb\n    Sin   :: PrimOp '[ 'HReal ] 'HReal\n    Cos   :: PrimOp '[ 'HReal ] 'HReal\n    Tan   :: PrimOp '[ 'HReal ] 'HReal\n    Asin  :: PrimOp '[ 'HReal ] 'HReal\n    Acos  :: PrimOp '[ 'HReal ] 'HReal\n    Atan  :: PrimOp '[ 'HReal ] 'HReal\n    Sinh  :: PrimOp '[ 'HReal ] 'HReal\n    Cosh  :: PrimOp '[ 'HReal ] 'HReal\n    Tanh  :: PrimOp '[ 'HReal ] 'HReal\n    Asinh :: PrimOp '[ 'HReal ] 'HReal\n    Acosh :: PrimOp '[ 'HReal ] 'HReal\n    Atanh :: PrimOp '[ 'HReal ] 'HReal\n\n    -- -- Other Real\\/Prob-valued operators\n    RealPow   :: PrimOp '[ 'HProb, 'HReal ] 'HProb\n    Exp       :: PrimOp '[ 'HReal ] 'HProb\n    Log       :: PrimOp '[ 'HProb ] 'HReal\n    Infinity  :: HIntegrable a -  PrimOp '[] a\n    GammaFunc :: PrimOp '[ 'HReal ] 'HProb\n    BetaFunc  :: PrimOp '[ 'HProb, 'HProb ] 'HProb\n\n    -- -- -- Here we have the /polymorphic/ operators\n    -- -- HEq and HOrd operators\n    Equal :: !(HEq  a) -  PrimOp '[ a, a ] HBool\n    Less  :: !(HOrd a) -  PrimOp '[ a, a ] HBool\n\n    -- -- HSemiring operators (the non-n-ary ones)\n    NatPow :: !(HSemiring a) -  PrimOp '[ a, 'HNat ] a\n\n    -- -- HRing operators\n    Negate :: !(HRing a) -  PrimOp '[ a ] a\n    Abs    :: !(HRing a) -  PrimOp '[ a ] (NonNegative a)\n    Signum :: !(HRing a) -  PrimOp '[ a ] a\n\n    -- -- HFractional operators\n    Recip :: !(HFractional a) -  PrimOp '[ a ] a\n\n    -- -- HRadical operators\n    NatRoot :: !(HRadical a) -  PrimOp '[ a, 'HNat ] a\n\n    -- -- HContinuous operators\n    Erf :: !(HContinuous a) -  PrimOp '[ a ] a", 
            "title": "PrimOp"
        }, 
        {
            "location": "/internals/abt/", 
            "text": "Abstract Binding Trees\n\n\nHakaru makes use of many program transformations in its codebase.\nBecause of this, a special mechanism is included for handing\nvariable bindings and substitutions. We abstract this into its\nown typeclass called \nABT\n. This can be found in \nLanguage.Hakaru.Syntax.ABT\n.\n\n\nBelow is an excerpt of this typeclass\n\n\nclass ABT (syn :: ([k] -\n k -\n *) -\n k -\n *) (abt :: [k] -\n k -\n *) | abt -\n syn where\n    -- Smart constructors for building a 'View' and then injecting it into the @abt@.\n    syn  :: syn abt  a -\n abt '[] a\n    var  :: Variable a -\n abt '[] a\n    bind :: Variable a -\n abt xs b -\n abt (a ': xs) b\n    caseBind :: abt (x ': xs) a -\n (Variable x -\n abt xs a -\n r) -\n r\n    ...", 
            "title": "ABT"
        }, 
        {
            "location": "/internals/abt/#abstract-binding-trees", 
            "text": "Hakaru makes use of many program transformations in its codebase.\nBecause of this, a special mechanism is included for handing\nvariable bindings and substitutions. We abstract this into its\nown typeclass called  ABT . This can be found in  Language.Hakaru.Syntax.ABT .  Below is an excerpt of this typeclass  class ABT (syn :: ([k] -  k -  *) -  k -  *) (abt :: [k] -  k -  *) | abt -  syn where\n    -- Smart constructors for building a 'View' and then injecting it into the @abt@.\n    syn  :: syn abt  a -  abt '[] a\n    var  :: Variable a -  abt '[] a\n    bind :: Variable a -  abt xs b -  abt (a ': xs) b\n    caseBind :: abt (x ': xs) a -  (Variable x -  abt xs a -  r) -  r\n    ...", 
            "title": "Abstract Binding Trees"
        }, 
        {
            "location": "/internals/datums/", 
            "text": "Data representation\n\n\nData types are stored using a sum of product representation.\nThey can be found in \nLanguage.Hakaru.Syntax.Datum\n.\n\n\n-- The first component is a hint for what the data constructor\n-- should be called when pretty-printing, giving error messages,\n-- etc. Like the hints for variable names, its value is not actually\n-- used to decide which constructor is meant or which pattern\n-- matches.\ndata Datum :: (Hakaru -\n *) -\n Hakaru -\n * where\n    Datum\n        :: {-# UNPACK #-} !Text\n        -\n !(Sing (HData' t))\n        -\n !(DatumCode (Code t) ast (HData' t))\n        -\n Datum ast (HData' t)\n\n-- | The intermediate components of a data constructor. The intuition\n-- behind the two indices is that the @[[HakaruFun]]@ is a functor\n-- applied to the Hakaru type. Initially the @[[HakaruFun]]@ functor\n-- will be the 'Code' associated with the Hakaru type; hence it's\n-- the one-step unrolling of the fixed point for our recursive\n-- datatypes. But as we go along, we'll be doing induction on the\n-- @[[HakaruFun]]@ functor.\ndata DatumCode :: [[HakaruFun]] -\n (Hakaru -\n *) -\n Hakaru -\n * where\n    -- Skip rightwards along the sum.\n    Inr :: !(DatumCode  xss abt a) -\n DatumCode (xs ': xss) abt a\n    -- Inject into the sum.\n    Inl :: !(DatumStruct xs abt a) -\n DatumCode (xs ': xss) abt a\n\ndata DatumStruct :: [HakaruFun] -\n (Hakaru -\n *) -\n Hakaru -\n * where\n    -- BUG: haddock doesn't like annotations on GADT constructors\n    -- \nhttps://github.com/hakaru-dev/hakaru/issues/6\n\n\n    -- Combine components of the product. (\\\net\\\n means \\\nand\\\n in Latin)\n    Et  :: !(DatumFun    x         abt a)\n        -\n !(DatumStruct xs        abt a)\n        -\n   DatumStruct (x ': xs) abt a\n\n    -- Close off the product.\n    Done :: DatumStruct '[] abt a\n\ndata DatumFun :: HakaruFun -\n (Hakaru -\n *) -\n Hakaru -\n * where\n    -- Hit a leaf which isn't a recursive component of the datatype.\n    Konst :: !(ast b) -\n DatumFun ('K b) ast a\n    -- Hit a leaf which is a recursive component of the datatype.\n    Ident :: !(ast a) -\n DatumFun 'I     ast a", 
            "title": "Datums"
        }, 
        {
            "location": "/internals/datums/#data-representation", 
            "text": "Data types are stored using a sum of product representation.\nThey can be found in  Language.Hakaru.Syntax.Datum .  -- The first component is a hint for what the data constructor\n-- should be called when pretty-printing, giving error messages,\n-- etc. Like the hints for variable names, its value is not actually\n-- used to decide which constructor is meant or which pattern\n-- matches.\ndata Datum :: (Hakaru -  *) -  Hakaru -  * where\n    Datum\n        :: {-# UNPACK #-} !Text\n        -  !(Sing (HData' t))\n        -  !(DatumCode (Code t) ast (HData' t))\n        -  Datum ast (HData' t)\n\n-- | The intermediate components of a data constructor. The intuition\n-- behind the two indices is that the @[[HakaruFun]]@ is a functor\n-- applied to the Hakaru type. Initially the @[[HakaruFun]]@ functor\n-- will be the 'Code' associated with the Hakaru type; hence it's\n-- the one-step unrolling of the fixed point for our recursive\n-- datatypes. But as we go along, we'll be doing induction on the\n-- @[[HakaruFun]]@ functor.\ndata DatumCode :: [[HakaruFun]] -  (Hakaru -  *) -  Hakaru -  * where\n    -- Skip rightwards along the sum.\n    Inr :: !(DatumCode  xss abt a) -  DatumCode (xs ': xss) abt a\n    -- Inject into the sum.\n    Inl :: !(DatumStruct xs abt a) -  DatumCode (xs ': xss) abt a\n\ndata DatumStruct :: [HakaruFun] -  (Hakaru -  *) -  Hakaru -  * where\n    -- BUG: haddock doesn't like annotations on GADT constructors\n    --  https://github.com/hakaru-dev/hakaru/issues/6 \n\n    -- Combine components of the product. (\\ et\\  means \\ and\\  in Latin)\n    Et  :: !(DatumFun    x         abt a)\n        -  !(DatumStruct xs        abt a)\n        -    DatumStruct (x ': xs) abt a\n\n    -- Close off the product.\n    Done :: DatumStruct '[] abt a\n\ndata DatumFun :: HakaruFun -  (Hakaru -  *) -  Hakaru -  * where\n    -- Hit a leaf which isn't a recursive component of the datatype.\n    Konst :: !(ast b) -  DatumFun ('K b) ast a\n    -- Hit a leaf which is a recursive component of the datatype.\n    Ident :: !(ast a) -  DatumFun 'I     ast a", 
            "title": "Data representation"
        }, 
        {
            "location": "/internals/coercions/", 
            "text": "Coercions\n\n\nFor convenience, Hakaru offers functions to convert between the four\ndifferent numeric types in the language. These types are\n\n\n\n\nnat - Natural numbers\n\n\nint - Integers\n\n\nprob - Positive real numbers\n\n\nreal - Real numbers\n\n\n\n\nAmongst these types there are a collection of safe and unsafe\ncoercions. A safe coercion is one which is always guaranteed to\nbe valid. For example, converting a \nnat\n to an \nint\n is always\nsafe. Converting an \nint\n to a \nnat\n is unsafe as the value can\nnegative, and lead to runtime errors.", 
            "title": "Coercions"
        }, 
        {
            "location": "/internals/coercions/#coercions", 
            "text": "For convenience, Hakaru offers functions to convert between the four\ndifferent numeric types in the language. These types are   nat - Natural numbers  int - Integers  prob - Positive real numbers  real - Real numbers   Amongst these types there are a collection of safe and unsafe\ncoercions. A safe coercion is one which is always guaranteed to\nbe valid. For example, converting a  nat  to an  int  is always\nsafe. Converting an  int  to a  nat  is unsafe as the value can\nnegative, and lead to runtime errors.", 
            "title": "Coercions"
        }, 
        {
            "location": "/internals/transforms/", 
            "text": "Program transformations in Hakaru\n\n\nCoalesce\n\n\nCoalesce is an internal transformation that works on the untyped Hakaru AST. It\ntakes recursive \nNAryOp\n terms that have the same type and combines them into\na single term. For instance:\n\n\n3.0 + 1.5 + 0.3\n\n\n\n\nis parser as:\n\n\nNaryOp Sum [3.0, NaryOp Sum [1.5, NaryOp Sum [0.3]]]\n\n\n\n\nwhich when coalesced becomes:\n\n\nNaryOp Sum [3.0,1.5,0.3]", 
            "title": "Transformaitons"
        }, 
        {
            "location": "/internals/transforms/#program-transformations-in-hakaru", 
            "text": "", 
            "title": "Program transformations in Hakaru"
        }, 
        {
            "location": "/internals/transforms/#coalesce", 
            "text": "Coalesce is an internal transformation that works on the untyped Hakaru AST. It\ntakes recursive  NAryOp  terms that have the same type and combines them into\na single term. For instance:  3.0 + 1.5 + 0.3  is parser as:  NaryOp Sum [3.0, NaryOp Sum [1.5, NaryOp Sum [0.3]]]  which when coalesced becomes:  NaryOp Sum [3.0,1.5,0.3]", 
            "title": "Coalesce"
        }, 
        {
            "location": "/internals/testing/", 
            "text": "Testing infrastructure in Hakaru\n\n\nHakaru can be tested by running \ncabal test\n from the\nroot directory of the project.\n\n\nTests written in Hakaru will be found in the \ntests/\n\nsubdirectory at the root of the project. Tests written\nin Haskell can be found at \nhaskell/Tests/\n.", 
            "title": "Testing"
        }, 
        {
            "location": "/internals/testing/#testing-infrastructure-in-hakaru", 
            "text": "Hakaru can be tested by running  cabal test  from the\nroot directory of the project.  Tests written in Hakaru will be found in the  tests/ \nsubdirectory at the root of the project. Tests written\nin Haskell can be found at  haskell/Tests/ .", 
            "title": "Testing infrastructure in Hakaru"
        }, 
        {
            "location": "/internals/newfeature/", 
            "text": "Adding a feature to the Hakaru language\n\n\nTo add a feature to the Hakaru language you must\n\n\n\n\nAdd an entry to the AST\n\n\nUpdate symbol resolution and optionally the parser to recognize this construct\n\n\nUpdate the pretty printers if this is something exposed to users\n\n\nUpdate the typechecker to handle it\n\n\nUpdate all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it\n\n\nUpdate the sampler if this primitive is intended to exist at runtime\n\n\nUpdate the compilers to emit the right code for this symbol\n\n\n\n\nWe give an example of what this looks like by adding \ndouble\n to the language.", 
            "title": "Adding a Language Feautre"
        }, 
        {
            "location": "/internals/newfeature/#adding-a-feature-to-the-hakaru-language", 
            "text": "To add a feature to the Hakaru language you must   Add an entry to the AST  Update symbol resolution and optionally the parser to recognize this construct  Update the pretty printers if this is something exposed to users  Update the typechecker to handle it  Update all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it  Update the sampler if this primitive is intended to exist at runtime  Update the compilers to emit the right code for this symbol   We give an example of what this looks like by adding  double  to the language.", 
            "title": "Adding a feature to the Hakaru language"
        }, 
        {
            "location": "/examples/", 
            "text": "Examples\n\n\nGaussian Mixture Model\n\n\nBelow is a model for a Gaussian Mixture model. This can be seen\nas a Bayesian version of K-means clustering.\n\n\n# Prelude to define dirichlet\ndef add(a prob, b prob):\n    a + b\n\ndef sum(a array(prob)):\n    reduce(add, 0, a)\n\ndef normalize(x array(prob)):\n    total = sum(x)\n    array i of size(x):\n       x[i] / total\n\ndef dirichlet(as array(prob)):\n    xs \n~ plate i of int2nat(size(as)-1):\n            beta(summate j from i+1 to size(as): as[j],\n                 as[i])\n    return array i of size(as):\n             x = product j from 0 to i: xs[j]\n             x * if i+1==size(as): 1 else: real2prob(1-xs[i])\n\n\n# num of clusters\nK = 5\n# num of points\nN = 20\n\n# prior probability of picking cluster K\npi  \n~ dirichlet(array _ of K: 1)\n# prior on mean and precision\nmu  \n~ plate _ of K:\n         normal(0, 5e-9)\ntau \n~ plate _ of K:\n         gamma(2, 0.05)\n# observed data\nx   \n~ plate _ of N:\n         i \n~ categorical(pi)\n         normal(mu[i], tau[i])\n\nreturn (x, mu). pair(array(real), array(real))\n\n\n\n\nLatent Dirichlet Allocation\n\n\nBelow is the LDA topic model.\n\n\nK = 2 # number of topics\nM = 3 # number of docs\nV = 7 # size of vocabulary\n\n# number of words in each document\ndoc = [4, 5, 3]\n\ntopic_prior = array _ of K: 1.0\nword_prior  = array _ of V: 1.0\n\nphi \n~ plate _ of K:     # word dist for topic k\n         dirichlet(word_prior)\n\n# likelihood\nz   \n~ plate m of M:\n         theta \n~ dirichlet(topic_prior)\n         plate _ of doc[m]: # topic marker for word n in doc m\n           categorical(theta)\n\nw   \n~ plate m of M: # for doc m\n         plate n of doc[m]: # for word n in doc m\n           categorical(phi[z[m][n]])\n\nreturn (w, z)", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#gaussian-mixture-model", 
            "text": "Below is a model for a Gaussian Mixture model. This can be seen\nas a Bayesian version of K-means clustering.  # Prelude to define dirichlet\ndef add(a prob, b prob):\n    a + b\n\ndef sum(a array(prob)):\n    reduce(add, 0, a)\n\ndef normalize(x array(prob)):\n    total = sum(x)\n    array i of size(x):\n       x[i] / total\n\ndef dirichlet(as array(prob)):\n    xs  ~ plate i of int2nat(size(as)-1):\n            beta(summate j from i+1 to size(as): as[j],\n                 as[i])\n    return array i of size(as):\n             x = product j from 0 to i: xs[j]\n             x * if i+1==size(as): 1 else: real2prob(1-xs[i])\n\n\n# num of clusters\nK = 5\n# num of points\nN = 20\n\n# prior probability of picking cluster K\npi   ~ dirichlet(array _ of K: 1)\n# prior on mean and precision\nmu   ~ plate _ of K:\n         normal(0, 5e-9)\ntau  ~ plate _ of K:\n         gamma(2, 0.05)\n# observed data\nx    ~ plate _ of N:\n         i  ~ categorical(pi)\n         normal(mu[i], tau[i])\n\nreturn (x, mu). pair(array(real), array(real))", 
            "title": "Gaussian Mixture Model"
        }, 
        {
            "location": "/examples/#latent-dirichlet-allocation", 
            "text": "Below is the LDA topic model.  K = 2 # number of topics\nM = 3 # number of docs\nV = 7 # size of vocabulary\n\n# number of words in each document\ndoc = [4, 5, 3]\n\ntopic_prior = array _ of K: 1.0\nword_prior  = array _ of V: 1.0\n\nphi  ~ plate _ of K:     # word dist for topic k\n         dirichlet(word_prior)\n\n# likelihood\nz    ~ plate m of M:\n         theta  ~ dirichlet(topic_prior)\n         plate _ of doc[m]: # topic marker for word n in doc m\n           categorical(theta)\n\nw    ~ plate m of M: # for doc m\n         plate n of doc[m]: # for word n in doc m\n           categorical(phi[z[m][n]])\n\nreturn (w, z)", 
            "title": "Latent Dirichlet Allocation"
        }
    ]
}