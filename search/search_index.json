{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hakaru Hakaru is a simply-typed probabilistic programming language, designed for easy specification of probabilistic models and inference algorithms. This type of language is useful for the development of machine learning algorithms and stochastic modeling. Hakaru enables the design of modular probabilistic inference programs by providing: A language for representing probabilistic distributions, queries, and inferences Methods for transforming probabilistic information, such as conditional probability and probabilistic inference, using computer algebra This documentation provides information for installing and using Hakaru. Sample programs are included to demonstrate some of Hakaru\u2019s functionality and Hakaru implementation details are included to help guide future Hakaru developments and extensions. Warning: This code is alpha and experimental. Contact us at ppaml@indiana.edu if you have any questions or concerns. Introduction The Introduction presents probabilistic programming and illustrates how Hakaru can be used to solve and describe these types of problems, how to install Hakaru on your machine, and some sample programs to get you started. What is Probabilistic Programming? Probabilistic programming systems allow us to write programs which describe probability distributions, and provide mechanisms to sample and condition the distributions they represent on data. In this page, we give a sense of the sorts of problems Hakaru is great at solving, and how you would describe them in Hakaru. Installing Hakaru You can install Hakaru on Linux, OSX, and Windows and extend its functionality using MapleSoft\u2019s Maple. Generating Samples from your Hakaru Program You can use the hakaru command to generate samples from your probabilistic model. Quick Start: A Mixture Model Example This page will introduce you to Hakaru\u2019s basic functionality by creating a program to sample and condition a mixture model of a coin toss. Compiling to Haskell A Hakaru program can be ported into Haskell which can then be converted into machine code for other applications. Compiling to C Depending on the scale, a Hakaru program might be resource-intensive to run. In these situations, you could port your Hakaru program to C using the hkc command to take advantage of other tools such as OpenMP for parallel processing. Hakaru Workflow and Examples What is the Hakaru Workflow? Hakaru provides a language and tools to aid in each step of the Bayesian inference workflow. Tutorial: Hakaru Workflow for Discrete Models This example of a burglary alarm demonstrates the workflow typically followed when creating Hakaru programs. Tutorial: Hakaru Workflow for Continuous Models This example of a thermometer calibration demonstrates the workflow typically followed when creating Hakaru programs that involve random real numbers in their description. Examples Two examples, a Gaussian Mixture Model and a Latent Dirichlet Allocation (LDA) topic model, highlight the types of problems that Hakaru is uniquely suited to help you solve. Language Guide The Language Guide presents an overview of Hakaru\u2019s language primitives and core functionality. Primitive Probability Distributions Common probability distributions, such as the normal distribution, are already encoded in Hakaru and are considered to be language primitives. This page provides usage instructions for accessing the primitive distributions in your programs. Let and Bind Let ( = ) and Bind ( <~ ) enable the use of variables in Hakaru programs, which is essential for extracting a value from a probability distribution. Conditionals Hakaru supports a restricted if expression for selections between two conditions. Functions Hakaru supports both named and anonymous function definitions. Types and Coercions Hakaru has basic types which can also be combined to make complex ones. To aid in the communication of information between Hakaru functions, coercions are defined to allow conversions between compatible types. Data Types and Match Hakaru supports some built-in data types from Haskell. The match function is used for deconstructing them to extract their elements and to reconstructing data back into these data types. Arrays and Plate Hakaru has special syntax for arrays, which is considered distinct from the other supported data types. A specialized array, plate , is used for describing measures over arrays. Loops Hakaru loops are specialized to compute the summation or product of the elements in an array. Transformations Hakaru includes some inference algorithms that you can use to transform your probabilistic models into other forms to extract desireable information. Its inference algorithms are implemented predominantly as program transformations. Note: By default, Hakaru assigns a weight to each generated sample. Typically a weight of one is used, but it is possible for the weights to vary between samples. This might result in differing results from the original and transformed programs when summarizing a program\u2019s output by counting them. Expect The expectation feature ( expect ) computes expectation of a measure with respect to a given function. Normalize The normalization transformation ( normalize ) reweights a program so that it represents a normal distribution. Disintegrate The disintegration transformation ( disintegrate ) produces a program representing the conditional distribution based on a joint probability distribution. This command is equivalent to model conditioning in probability theory. Density The density transformation ( density ) is used to create a conditional distribution model that is used to estimate the density of the distribution at a particular point. Hakaru-Maple The simplify transformation ( hk-maple -c Simplify ) is used to improve Hakaru programs by simplifying probabilistic models using computer algebra. This transformation requires the use of Maple. Hakaru provides two other transformations also written in Maple. Metropolis Hastings The Metropolis Hastings transform ( mh ) is used to convert a Hakaru program into a Metropolis Hastings transition kernel. Internals The internals section of the manual provides some insight into how Hakaru is implemented and offers guidance into how the system can be extended. AST ABT Datums Coercions Transformations Testing Hakaru modules Adding a Language Feature Citing Us When referring to Hakaru, please cite the following academic paper : P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic Programming, pp. 62-79, 2016. @inproceedings{narayanan2016probabilistic, title = {Probabilistic inference by program transformation in Hakaru (system description)}, author = {Narayanan, Praveen and Carette, Jacques and Romano, Wren and Shan, Chung{-}chieh and Zinkov, Robert}, booktitle = {International Symposium on Functional and Logic Programming - 13th International Symposium, {FLOPS} 2016, Kochi, Japan, March 4-6, 2016, Proceedings}, pages = {62--79}, year = {2016}, organization = {Springer}, url = {http://dx.doi.org/10.1007/978-3-319-29604-3_5}, doi = {10.1007/978-3-319-29604-3_5}, }","title":"Home"},{"location":"#introduction","text":"The Introduction presents probabilistic programming and illustrates how Hakaru can be used to solve and describe these types of problems, how to install Hakaru on your machine, and some sample programs to get you started.","title":"Introduction"},{"location":"#what-is-probabilistic-programming","text":"Probabilistic programming systems allow us to write programs which describe probability distributions, and provide mechanisms to sample and condition the distributions they represent on data. In this page, we give a sense of the sorts of problems Hakaru is great at solving, and how you would describe them in Hakaru.","title":"What is Probabilistic Programming?"},{"location":"#installing-hakaru","text":"You can install Hakaru on Linux, OSX, and Windows and extend its functionality using MapleSoft\u2019s Maple.","title":"Installing Hakaru"},{"location":"#generating-samples-from-your-hakaru-program","text":"You can use the hakaru command to generate samples from your probabilistic model.","title":"Generating Samples from your Hakaru Program"},{"location":"#quick-start-a-mixture-model-example","text":"This page will introduce you to Hakaru\u2019s basic functionality by creating a program to sample and condition a mixture model of a coin toss.","title":"Quick Start: A Mixture Model Example"},{"location":"#compiling-to-haskell","text":"A Hakaru program can be ported into Haskell which can then be converted into machine code for other applications.","title":"Compiling to Haskell"},{"location":"#compiling-to-c","text":"Depending on the scale, a Hakaru program might be resource-intensive to run. In these situations, you could port your Hakaru program to C using the hkc command to take advantage of other tools such as OpenMP for parallel processing.","title":"Compiling to C"},{"location":"#hakaru-workflow-and-examples","text":"","title":"Hakaru Workflow and Examples"},{"location":"#what-is-the-hakaru-workflow","text":"Hakaru provides a language and tools to aid in each step of the Bayesian inference workflow.","title":"What is the Hakaru Workflow?"},{"location":"#tutorial-hakaru-workflow-for-discrete-models","text":"This example of a burglary alarm demonstrates the workflow typically followed when creating Hakaru programs.","title":"Tutorial: Hakaru Workflow for Discrete Models"},{"location":"#tutorial-hakaru-workflow-for-continuous-models","text":"This example of a thermometer calibration demonstrates the workflow typically followed when creating Hakaru programs that involve random real numbers in their description.","title":"Tutorial: Hakaru Workflow for Continuous Models"},{"location":"#examples","text":"Two examples, a Gaussian Mixture Model and a Latent Dirichlet Allocation (LDA) topic model, highlight the types of problems that Hakaru is uniquely suited to help you solve.","title":"Examples"},{"location":"#language-guide","text":"The Language Guide presents an overview of Hakaru\u2019s language primitives and core functionality.","title":"Language Guide"},{"location":"#primitive-probability-distributions","text":"Common probability distributions, such as the normal distribution, are already encoded in Hakaru and are considered to be language primitives. This page provides usage instructions for accessing the primitive distributions in your programs.","title":"Primitive Probability Distributions"},{"location":"#let-and-bind","text":"Let ( = ) and Bind ( <~ ) enable the use of variables in Hakaru programs, which is essential for extracting a value from a probability distribution.","title":"Let and Bind"},{"location":"#conditionals","text":"Hakaru supports a restricted if expression for selections between two conditions.","title":"Conditionals"},{"location":"#functions","text":"Hakaru supports both named and anonymous function definitions.","title":"Functions"},{"location":"#types-and-coercions","text":"Hakaru has basic types which can also be combined to make complex ones. To aid in the communication of information between Hakaru functions, coercions are defined to allow conversions between compatible types.","title":"Types and Coercions"},{"location":"#data-types-and-match","text":"Hakaru supports some built-in data types from Haskell. The match function is used for deconstructing them to extract their elements and to reconstructing data back into these data types.","title":"Data Types and Match"},{"location":"#arrays-and-plate","text":"Hakaru has special syntax for arrays, which is considered distinct from the other supported data types. A specialized array, plate , is used for describing measures over arrays.","title":"Arrays and Plate"},{"location":"#loops","text":"Hakaru loops are specialized to compute the summation or product of the elements in an array.","title":"Loops"},{"location":"#transformations","text":"Hakaru includes some inference algorithms that you can use to transform your probabilistic models into other forms to extract desireable information. Its inference algorithms are implemented predominantly as program transformations. Note: By default, Hakaru assigns a weight to each generated sample. Typically a weight of one is used, but it is possible for the weights to vary between samples. This might result in differing results from the original and transformed programs when summarizing a program\u2019s output by counting them.","title":"Transformations"},{"location":"#expect","text":"The expectation feature ( expect ) computes expectation of a measure with respect to a given function.","title":"Expect"},{"location":"#normalize","text":"The normalization transformation ( normalize ) reweights a program so that it represents a normal distribution.","title":"Normalize"},{"location":"#disintegrate","text":"The disintegration transformation ( disintegrate ) produces a program representing the conditional distribution based on a joint probability distribution. This command is equivalent to model conditioning in probability theory.","title":"Disintegrate"},{"location":"#density","text":"The density transformation ( density ) is used to create a conditional distribution model that is used to estimate the density of the distribution at a particular point.","title":"Density"},{"location":"#hakaru-maple","text":"The simplify transformation ( hk-maple -c Simplify ) is used to improve Hakaru programs by simplifying probabilistic models using computer algebra. This transformation requires the use of Maple. Hakaru provides two other transformations also written in Maple.","title":"Hakaru-Maple"},{"location":"#metropolis-hastings","text":"The Metropolis Hastings transform ( mh ) is used to convert a Hakaru program into a Metropolis Hastings transition kernel.","title":"Metropolis Hastings"},{"location":"#internals","text":"The internals section of the manual provides some insight into how Hakaru is implemented and offers guidance into how the system can be extended. AST ABT Datums Coercions Transformations Testing Hakaru modules Adding a Language Feature","title":"Internals"},{"location":"#citing-us","text":"When referring to Hakaru, please cite the following academic paper : P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic Programming, pp. 62-79, 2016. @inproceedings{narayanan2016probabilistic, title = {Probabilistic inference by program transformation in Hakaru (system description)}, author = {Narayanan, Praveen and Carette, Jacques and Romano, Wren and Shan, Chung{-}chieh and Zinkov, Robert}, booktitle = {International Symposium on Functional and Logic Programming - 13th International Symposium, {FLOPS} 2016, Kochi, Japan, March 4-6, 2016, Proceedings}, pages = {62--79}, year = {2016}, organization = {Springer}, url = {http://dx.doi.org/10.1007/978-3-319-29604-3_5}, doi = {10.1007/978-3-319-29604-3_5}, }","title":"Citing Us"},{"location":"examples/","text":"Examples Gaussian Mixture Model Below is a model for a Gaussian Mixture model. This can be seen as a Bayesian version of K-means clustering. # Prelude to define dirichlet def add(a prob, b prob): a + b def sum(a array(prob)): reduce(add, 0, a) def normalize(x array(prob)): total = sum(x) array i of size(x): x[i] / total def dirichlet(as array(prob)): xs <~ plate i of int2nat(size(as)-1): beta(summate j from i+1 to size(as): as[j], as[i]) return array i of size(as): x = product j from 0 to i: xs[j] x * if i+1==size(as): 1 else: real2prob(1-xs[i]) # num of clusters K = 5 # num of points N = 20 # prior probability of picking cluster K pi <~ dirichlet(array _ of K: 1) # prior on mean and precision mu <~ plate _ of K: normal(0, 5e-9) tau <~ plate _ of K: gamma(2, 0.05) # observed data x <~ plate _ of N: i <~ categorical(pi) normal(mu[i], tau[i]) return (x, mu). pair(array(real), array(real)) Latent Dirichlet Allocation Below is the Latent Dirichlet Allocation (LDA) topic model . K = 2 # number of topics M = 3 # number of docs V = 7 # size of vocabulary # number of words in each document doc = [4, 5, 3] topic_prior = array _ of K: 1.0 word_prior = array _ of V: 1.0 phi <~ plate _ of K: # word dist for topic k dirichlet(word_prior) # likelihood z <~ plate m of M: theta <~ dirichlet(topic_prior) plate _ of doc[m]: # topic marker for word n in doc m categorical(theta) w <~ plate m of M: # for doc m plate n of doc[m]: # for word n in doc m categorical(phi[z[m][n]]) return (w, z)","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#gaussian-mixture-model","text":"Below is a model for a Gaussian Mixture model. This can be seen as a Bayesian version of K-means clustering. # Prelude to define dirichlet def add(a prob, b prob): a + b def sum(a array(prob)): reduce(add, 0, a) def normalize(x array(prob)): total = sum(x) array i of size(x): x[i] / total def dirichlet(as array(prob)): xs <~ plate i of int2nat(size(as)-1): beta(summate j from i+1 to size(as): as[j], as[i]) return array i of size(as): x = product j from 0 to i: xs[j] x * if i+1==size(as): 1 else: real2prob(1-xs[i]) # num of clusters K = 5 # num of points N = 20 # prior probability of picking cluster K pi <~ dirichlet(array _ of K: 1) # prior on mean and precision mu <~ plate _ of K: normal(0, 5e-9) tau <~ plate _ of K: gamma(2, 0.05) # observed data x <~ plate _ of N: i <~ categorical(pi) normal(mu[i], tau[i]) return (x, mu). pair(array(real), array(real))","title":"Gaussian Mixture Model"},{"location":"examples/#latent-dirichlet-allocation","text":"Below is the Latent Dirichlet Allocation (LDA) topic model . K = 2 # number of topics M = 3 # number of docs V = 7 # size of vocabulary # number of words in each document doc = [4, 5, 3] topic_prior = array _ of K: 1.0 word_prior = array _ of V: 1.0 phi <~ plate _ of K: # word dist for topic k dirichlet(word_prior) # likelihood z <~ plate m of M: theta <~ dirichlet(topic_prior) plate _ of doc[m]: # topic marker for word n in doc m categorical(theta) w <~ plate m of M: # for doc m plate n of doc[m]: # for word n in doc m categorical(phi[z[m][n]]) return (w, z)","title":"Latent Dirichlet Allocation"},{"location":"internals/abt/","text":"Abstract Binding Trees Hakaru makes use of many program transformations in its codebase. Because of this, a special mechanism is included for handing variable bindings and substitutions. We abstract this into its own typeclass called ABT . This can be found in Language.Hakaru.Syntax.ABT . Below is an excerpt of this typeclass class ABT (syn :: ([k] -> k -> *) -> k -> *) (abt :: [k] -> k -> *) | abt -> syn where -- Smart constructors for building a 'View' and then injecting it into the @abt@. syn :: syn abt a -> abt '[] a var :: Variable a -> abt '[] a bind :: Variable a -> abt xs b -> abt (a ': xs) b caseBind :: abt (x ': xs) a -> (Variable x -> abt xs a -> r) -> r ... The advantage of having this typeclass is that we think about variable binding independently of the AST for our language. For example, we can define variable substitution once and for all.","title":"ABT"},{"location":"internals/abt/#abstract-binding-trees","text":"Hakaru makes use of many program transformations in its codebase. Because of this, a special mechanism is included for handing variable bindings and substitutions. We abstract this into its own typeclass called ABT . This can be found in Language.Hakaru.Syntax.ABT . Below is an excerpt of this typeclass class ABT (syn :: ([k] -> k -> *) -> k -> *) (abt :: [k] -> k -> *) | abt -> syn where -- Smart constructors for building a 'View' and then injecting it into the @abt@. syn :: syn abt a -> abt '[] a var :: Variable a -> abt '[] a bind :: Variable a -> abt xs b -> abt (a ': xs) b caseBind :: abt (x ': xs) a -> (Variable x -> abt xs a -> r) -> r ... The advantage of having this typeclass is that we think about variable binding independently of the AST for our language. For example, we can define variable substitution once and for all.","title":"Abstract Binding Trees"},{"location":"internals/ast/","text":"Internal Representation of Hakaru terms The Hakaru AST can be found defined in haskell/Language/Hakaru/Syntax/AST.hs . It is made up of several parts which this section and the next one will explain. We should note, this datatype makes use of Abstract Binding Trees which we discuss in more detail in the next section . ABTs can be understood as a way to abstract the use of variables in the AST. The advantage of this is it allows all variable substitution and manipulation logic to live in one place and not be specific to a particular AST. Datakind The AST is typed using the Hakaru kind, defined in haskell/Language/Types/DataKind.hs . All Hakaru types are defined in terms of the primitives in this datakind. -- | The universe\\/kind of Hakaru types. data Hakaru = HNat -- ^ The natural numbers; aka, the non-negative integers. -- | The integers. | HInt -- | Non-negative real numbers. Unlike what you might expect, -- this is /not/ restructed to the @[0,1]@ interval! | HProb -- | The affinely extended real number line. That is, the real -- numbers extended with positive and negative infinities. | HReal -- | The measure monad | HMeasure !Hakaru -- | The built-in type for uniform arrays. | HArray !Hakaru -- | The type of Hakaru functions. | !Hakaru :-> !Hakaru -- | A user-defined polynomial datatype. Each such type is -- specified by a \\\"tag\\\" (the @HakaruCon@) which names the type, and a sum-of-product representation of the type itself. | HData !HakaruCon [[HakaruFun]] Please read Datakind.hs for more details. Term The Term datatype includes all the syntactic constructions for the Hakaru language. For all those where we know the number of arguments we expect that language construct to get, we define the (:$) constructor, which takes SCons and SArgs datatypes as arguments. -- | The generating functor for Hakaru ASTs. This type is given in -- open-recursive form, where the first type argument gives the -- recursive form. The recursive form @abt@ does not have exactly -- the same kind as @Term abt@ because every 'Term' represents a -- locally-closed term whereas the underlying @abt@ may bind some -- variables. data Term :: ([Hakaru] -> Hakaru -> *) -> Hakaru -> * where -- Simple syntactic forms (i.e., generalized quantifiers) (:$) :: !(SCon args a) -> !(SArgs abt args) -> Term abt a -- N-ary operators NaryOp_ :: !(NaryOp a) -> !(Seq (abt '[] a)) -> Term abt a -- Literal\\/Constant values Literal_ :: !(Literal a) -> Term abt a Empty_ :: !(Sing ('HArray a)) -> Term abt ('HArray a) Array_ :: !(abt '[] 'HNat) -> !(abt '[ 'HNat ] a) -> Term abt ('HArray a) -- -- User-defined data types -- A data constructor applied to some expressions. N.B., this -- definition only accounts for data constructors which are -- fully saturated. Unsaturated constructors will need to be -- eta-expanded. Datum_ :: !(Datum (abt '[]) (HData' t)) -> Term abt (HData' t) -- Generic case-analysis (via ABTs and Structural Focalization). Case_ :: !(abt '[] a) -> [Branch a abt b] -> Term abt b -- Linear combinations of measures. Superpose_ :: L.NonEmpty (abt '[] 'HProb, abt '[] ('HMeasure a)) -> Term abt ('HMeasure a) Reject_ :: !(Sing ('HMeasure a)) -> Term abt ('HMeasure a) SCons and SArgs When using (:$) we have a way to describe primitives where we know the number of arguments they should get. In that regard, SArgs is a typed list of abt terms indexed by its size. -- | The arguments to a @(':$')@ node in the 'Term'; that is, a list -- of ASTs, where the whole list is indexed by a (type-level) list -- of the indices of each element. data SArgs :: ([Hakaru] -> Hakaru -> *) -> [([Hakaru], Hakaru)] -> * where End :: SArgs abt '[] (:*) :: !(abt vars a) -> !(SArgs abt args) -> SArgs abt ( '(vars, a) ': args) These are combined with SCons which describes the constructor, and the types it expects for its arguments. For example suppose we had an AST for a function f and it\u2019s argument x , we could construct a Term for applying f to x by writing App_:$ f :* x :* End . -- | The constructor of a @(':$')@ node in the 'Term'. Each of these -- constructors denotes a \\\"normal\\/standard\\/basic\\\" syntactic -- form (i.e., a generalized quantifier). In the literature, these -- syntactic forms are sometimes called \\\"operators\\\", but we avoid -- calling them that so as not to introduce confusion vs 'PrimOp' -- etc. Instead we use the term \\\"operator\\\" to refer to any primitive -- function or constant; that is, non-binding syntactic forms. Also -- in the literature, the 'SCon' type itself is usually called the -- \\\"signature\\\" of the term language. However, we avoid calling -- it that since our 'Term' has constructors other than just @(:$)@, -- so 'SCon' does not give a complete signature for our terms. -- -- The main reason for breaking this type out and using it in -- conjunction with @(':$')@ and 'SArgs' is so that we can easily -- pattern match on /fully saturated/ nodes. For example, we want -- to be able to match @MeasureOp_ Uniform :$ lo :* hi :* End@ -- without needing to deal with 'App_' nodes nor 'viewABT'. data SCon :: [([Hakaru], Hakaru)] -> Hakaru -> * where Lam_ :: SCon '[ '( '[ a ], b ) ] (a ':-> b) App_ :: SCon '[ LC (a ':-> b ), LC a ] b Let_ :: SCon '[ LC a, '( '[ a ], b ) ] b CoerceTo_ :: !(Coercion a b) -> SCon '[ LC a ] b UnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a PrimOp_ :: (typs ~ UnLCs args, args ~ LCs typs) => !(PrimOp typs a) -> SCon args a ArrayOp_ :: (typs ~ UnLCs args, args ~ LCs typs) => !(ArrayOp typs a) -> SCon args a MeasureOp_ :: (typs ~ UnLCs args, args ~ LCs typs) => !(MeasureOp typs a) -> SCon args ('HMeasure a) Dirac :: SCon '[ LC a ] ('HMeasure a) MBind :: SCon '[ LC ('HMeasure a) , '( '[ a ], 'HMeasure b) ] ('HMeasure b) Plate :: SCon '[ LC 'HNat , '( '[ 'HNat ], 'HMeasure a) ] ('HMeasure ('HArray a)) Chain :: SCon '[ LC 'HNat, LC s , '( '[ s ], 'HMeasure (HPair a s)) ] ('HMeasure (HPair ('HArray a) s)) Integrate :: SCon '[ LC 'HReal, LC 'HReal, '( '[ 'HReal ], 'HProb) ] 'HProb Summate :: HDiscrete a -> HSemiring b -> SCon '[ LC a, LC a, '( '[ a ], b) ] b Product :: HDiscrete a -> HSemiring b -> SCon '[ LC a, LC a, '( '[ a ], b) ] b Expect :: SCon '[ LC ('HMeasure a), '( '[ a ], 'HProb) ] 'HProb Observe :: SCon '[ LC ('HMeasure a), LC a ] ('HMeasure a) You\u2019ll notice in SCon there are definitions for PrimOp, MeasureOp, and ArrayOp these are done more organizational purposes and have constructions for the different categories of primitives. MeasureOp Primitives of type measure are defined in MeasureOp. -- | Primitive operators to produce, consume, or transform -- distributions\\/measures. This corresponds to the old @Mochastic@ -- class, except that 'MBind' and 'Superpose_' are handled elsewhere -- since they are not simple operators. (Also 'Dirac' is handled -- elsewhere since it naturally fits with 'MBind', even though it -- is a siple operator.) data MeasureOp :: [Hakaru] -> Hakaru -> * where Lebesgue :: MeasureOp '[ 'HReal, 'HReal ] 'HReal Counting :: MeasureOp '[] 'HInt Categorical :: MeasureOp '[ 'HArray 'HProb ] 'HNat Uniform :: MeasureOp '[ 'HReal, 'HReal ] 'HReal Normal :: MeasureOp '[ 'HReal, 'HProb ] 'HReal Poisson :: MeasureOp '[ 'HProb ] 'HNat Gamma :: MeasureOp '[ 'HProb, 'HProb ] 'HProb Beta :: MeasureOp '[ 'HProb, 'HProb ] 'HProb ArrayOp Primitives that involve manipulating value of type array, end up in ArrayOp. -- | Primitive operators for consuming or transforming arrays. data ArrayOp :: [Hakaru] -> Hakaru -> * where Index :: !(Sing a) -> ArrayOp '[ 'HArray a, 'HNat ] a Size :: !(Sing a) -> ArrayOp '[ 'HArray a ] 'HNat Reduce :: !(Sing a) -> ArrayOp '[ a ':-> a ':-> a, a, 'HArray a ] a PrimOp All primitive operations which don\u2019t return something of type array or measure are placed in PrimOp -- | Simple primitive functions, and constants. data PrimOp :: [Hakaru] -> Hakaru -> * where -- -- -- Here we have /monomorphic/ operators -- -- The Boolean operators Not :: PrimOp '[ HBool ] HBool -- And, Or, Xor, Iff Impl :: PrimOp '[ HBool, HBool ] HBool -- Impl x y == Or (Not x) y Diff :: PrimOp '[ HBool, HBool ] HBool -- Diff x y == Not (Impl x y) Nand :: PrimOp '[ HBool, HBool ] HBool -- Nand aka Alternative Denial, Sheffer stroke Nor :: PrimOp '[ HBool, HBool ] HBool -- Nor aka Joint Denial, aka Quine dagger, aka Pierce arrow -- -- Trigonometry operators Pi :: PrimOp '[] 'HProb Sin :: PrimOp '[ 'HReal ] 'HReal Cos :: PrimOp '[ 'HReal ] 'HReal Tan :: PrimOp '[ 'HReal ] 'HReal Asin :: PrimOp '[ 'HReal ] 'HReal Acos :: PrimOp '[ 'HReal ] 'HReal Atan :: PrimOp '[ 'HReal ] 'HReal Sinh :: PrimOp '[ 'HReal ] 'HReal Cosh :: PrimOp '[ 'HReal ] 'HReal Tanh :: PrimOp '[ 'HReal ] 'HReal Asinh :: PrimOp '[ 'HReal ] 'HReal Acosh :: PrimOp '[ 'HReal ] 'HReal Atanh :: PrimOp '[ 'HReal ] 'HReal -- -- Other Real\\/Prob-valued operators RealPow :: PrimOp '[ 'HProb, 'HReal ] 'HProb Exp :: PrimOp '[ 'HReal ] 'HProb Log :: PrimOp '[ 'HProb ] 'HReal Infinity :: HIntegrable a -> PrimOp '[] a GammaFunc :: PrimOp '[ 'HReal ] 'HProb BetaFunc :: PrimOp '[ 'HProb, 'HProb ] 'HProb -- -- -- Here we have the /polymorphic/ operators -- -- HEq and HOrd operators Equal :: !(HEq a) -> PrimOp '[ a, a ] HBool Less :: !(HOrd a) -> PrimOp '[ a, a ] HBool -- -- HSemiring operators (the non-n-ary ones) NatPow :: !(HSemiring a) -> PrimOp '[ a, 'HNat ] a -- -- HRing operators Negate :: !(HRing a) -> PrimOp '[ a ] a Abs :: !(HRing a) -> PrimOp '[ a ] (NonNegative a) Signum :: !(HRing a) -> PrimOp '[ a ] a -- -- HFractional operators Recip :: !(HFractional a) -> PrimOp '[ a ] a -- -- HRadical operators NatRoot :: !(HRadical a) -> PrimOp '[ a, 'HNat ] a -- -- HContinuous operators Erf :: !(HContinuous a) -> PrimOp '[ a ] a","title":"AST and Hakaru Datakind"},{"location":"internals/ast/#internal-representation-of-hakaru-terms","text":"The Hakaru AST can be found defined in haskell/Language/Hakaru/Syntax/AST.hs . It is made up of several parts which this section and the next one will explain. We should note, this datatype makes use of Abstract Binding Trees which we discuss in more detail in the next section . ABTs can be understood as a way to abstract the use of variables in the AST. The advantage of this is it allows all variable substitution and manipulation logic to live in one place and not be specific to a particular AST.","title":"Internal Representation of Hakaru terms"},{"location":"internals/ast/#datakind","text":"The AST is typed using the Hakaru kind, defined in haskell/Language/Types/DataKind.hs . All Hakaru types are defined in terms of the primitives in this datakind. -- | The universe\\/kind of Hakaru types. data Hakaru = HNat -- ^ The natural numbers; aka, the non-negative integers. -- | The integers. | HInt -- | Non-negative real numbers. Unlike what you might expect, -- this is /not/ restructed to the @[0,1]@ interval! | HProb -- | The affinely extended real number line. That is, the real -- numbers extended with positive and negative infinities. | HReal -- | The measure monad | HMeasure !Hakaru -- | The built-in type for uniform arrays. | HArray !Hakaru -- | The type of Hakaru functions. | !Hakaru :-> !Hakaru -- | A user-defined polynomial datatype. Each such type is -- specified by a \\\"tag\\\" (the @HakaruCon@) which names the type, and a sum-of-product representation of the type itself. | HData !HakaruCon [[HakaruFun]] Please read Datakind.hs for more details.","title":"Datakind"},{"location":"internals/ast/#term","text":"The Term datatype includes all the syntactic constructions for the Hakaru language. For all those where we know the number of arguments we expect that language construct to get, we define the (:$) constructor, which takes SCons and SArgs datatypes as arguments. -- | The generating functor for Hakaru ASTs. This type is given in -- open-recursive form, where the first type argument gives the -- recursive form. The recursive form @abt@ does not have exactly -- the same kind as @Term abt@ because every 'Term' represents a -- locally-closed term whereas the underlying @abt@ may bind some -- variables. data Term :: ([Hakaru] -> Hakaru -> *) -> Hakaru -> * where -- Simple syntactic forms (i.e., generalized quantifiers) (:$) :: !(SCon args a) -> !(SArgs abt args) -> Term abt a -- N-ary operators NaryOp_ :: !(NaryOp a) -> !(Seq (abt '[] a)) -> Term abt a -- Literal\\/Constant values Literal_ :: !(Literal a) -> Term abt a Empty_ :: !(Sing ('HArray a)) -> Term abt ('HArray a) Array_ :: !(abt '[] 'HNat) -> !(abt '[ 'HNat ] a) -> Term abt ('HArray a) -- -- User-defined data types -- A data constructor applied to some expressions. N.B., this -- definition only accounts for data constructors which are -- fully saturated. Unsaturated constructors will need to be -- eta-expanded. Datum_ :: !(Datum (abt '[]) (HData' t)) -> Term abt (HData' t) -- Generic case-analysis (via ABTs and Structural Focalization). Case_ :: !(abt '[] a) -> [Branch a abt b] -> Term abt b -- Linear combinations of measures. Superpose_ :: L.NonEmpty (abt '[] 'HProb, abt '[] ('HMeasure a)) -> Term abt ('HMeasure a) Reject_ :: !(Sing ('HMeasure a)) -> Term abt ('HMeasure a)","title":"Term"},{"location":"internals/ast/#scons-and-sargs","text":"When using (:$) we have a way to describe primitives where we know the number of arguments they should get. In that regard, SArgs is a typed list of abt terms indexed by its size. -- | The arguments to a @(':$')@ node in the 'Term'; that is, a list -- of ASTs, where the whole list is indexed by a (type-level) list -- of the indices of each element. data SArgs :: ([Hakaru] -> Hakaru -> *) -> [([Hakaru], Hakaru)] -> * where End :: SArgs abt '[] (:*) :: !(abt vars a) -> !(SArgs abt args) -> SArgs abt ( '(vars, a) ': args) These are combined with SCons which describes the constructor, and the types it expects for its arguments. For example suppose we had an AST for a function f and it\u2019s argument x , we could construct a Term for applying f to x by writing App_:$ f :* x :* End . -- | The constructor of a @(':$')@ node in the 'Term'. Each of these -- constructors denotes a \\\"normal\\/standard\\/basic\\\" syntactic -- form (i.e., a generalized quantifier). In the literature, these -- syntactic forms are sometimes called \\\"operators\\\", but we avoid -- calling them that so as not to introduce confusion vs 'PrimOp' -- etc. Instead we use the term \\\"operator\\\" to refer to any primitive -- function or constant; that is, non-binding syntactic forms. Also -- in the literature, the 'SCon' type itself is usually called the -- \\\"signature\\\" of the term language. However, we avoid calling -- it that since our 'Term' has constructors other than just @(:$)@, -- so 'SCon' does not give a complete signature for our terms. -- -- The main reason for breaking this type out and using it in -- conjunction with @(':$')@ and 'SArgs' is so that we can easily -- pattern match on /fully saturated/ nodes. For example, we want -- to be able to match @MeasureOp_ Uniform :$ lo :* hi :* End@ -- without needing to deal with 'App_' nodes nor 'viewABT'. data SCon :: [([Hakaru], Hakaru)] -> Hakaru -> * where Lam_ :: SCon '[ '( '[ a ], b ) ] (a ':-> b) App_ :: SCon '[ LC (a ':-> b ), LC a ] b Let_ :: SCon '[ LC a, '( '[ a ], b ) ] b CoerceTo_ :: !(Coercion a b) -> SCon '[ LC a ] b UnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a PrimOp_ :: (typs ~ UnLCs args, args ~ LCs typs) => !(PrimOp typs a) -> SCon args a ArrayOp_ :: (typs ~ UnLCs args, args ~ LCs typs) => !(ArrayOp typs a) -> SCon args a MeasureOp_ :: (typs ~ UnLCs args, args ~ LCs typs) => !(MeasureOp typs a) -> SCon args ('HMeasure a) Dirac :: SCon '[ LC a ] ('HMeasure a) MBind :: SCon '[ LC ('HMeasure a) , '( '[ a ], 'HMeasure b) ] ('HMeasure b) Plate :: SCon '[ LC 'HNat , '( '[ 'HNat ], 'HMeasure a) ] ('HMeasure ('HArray a)) Chain :: SCon '[ LC 'HNat, LC s , '( '[ s ], 'HMeasure (HPair a s)) ] ('HMeasure (HPair ('HArray a) s)) Integrate :: SCon '[ LC 'HReal, LC 'HReal, '( '[ 'HReal ], 'HProb) ] 'HProb Summate :: HDiscrete a -> HSemiring b -> SCon '[ LC a, LC a, '( '[ a ], b) ] b Product :: HDiscrete a -> HSemiring b -> SCon '[ LC a, LC a, '( '[ a ], b) ] b Expect :: SCon '[ LC ('HMeasure a), '( '[ a ], 'HProb) ] 'HProb Observe :: SCon '[ LC ('HMeasure a), LC a ] ('HMeasure a) You\u2019ll notice in SCon there are definitions for PrimOp, MeasureOp, and ArrayOp these are done more organizational purposes and have constructions for the different categories of primitives.","title":"SCons and SArgs"},{"location":"internals/ast/#measureop","text":"Primitives of type measure are defined in MeasureOp. -- | Primitive operators to produce, consume, or transform -- distributions\\/measures. This corresponds to the old @Mochastic@ -- class, except that 'MBind' and 'Superpose_' are handled elsewhere -- since they are not simple operators. (Also 'Dirac' is handled -- elsewhere since it naturally fits with 'MBind', even though it -- is a siple operator.) data MeasureOp :: [Hakaru] -> Hakaru -> * where Lebesgue :: MeasureOp '[ 'HReal, 'HReal ] 'HReal Counting :: MeasureOp '[] 'HInt Categorical :: MeasureOp '[ 'HArray 'HProb ] 'HNat Uniform :: MeasureOp '[ 'HReal, 'HReal ] 'HReal Normal :: MeasureOp '[ 'HReal, 'HProb ] 'HReal Poisson :: MeasureOp '[ 'HProb ] 'HNat Gamma :: MeasureOp '[ 'HProb, 'HProb ] 'HProb Beta :: MeasureOp '[ 'HProb, 'HProb ] 'HProb","title":"MeasureOp"},{"location":"internals/ast/#arrayop","text":"Primitives that involve manipulating value of type array, end up in ArrayOp. -- | Primitive operators for consuming or transforming arrays. data ArrayOp :: [Hakaru] -> Hakaru -> * where Index :: !(Sing a) -> ArrayOp '[ 'HArray a, 'HNat ] a Size :: !(Sing a) -> ArrayOp '[ 'HArray a ] 'HNat Reduce :: !(Sing a) -> ArrayOp '[ a ':-> a ':-> a, a, 'HArray a ] a","title":"ArrayOp"},{"location":"internals/ast/#primop","text":"All primitive operations which don\u2019t return something of type array or measure are placed in PrimOp -- | Simple primitive functions, and constants. data PrimOp :: [Hakaru] -> Hakaru -> * where -- -- -- Here we have /monomorphic/ operators -- -- The Boolean operators Not :: PrimOp '[ HBool ] HBool -- And, Or, Xor, Iff Impl :: PrimOp '[ HBool, HBool ] HBool -- Impl x y == Or (Not x) y Diff :: PrimOp '[ HBool, HBool ] HBool -- Diff x y == Not (Impl x y) Nand :: PrimOp '[ HBool, HBool ] HBool -- Nand aka Alternative Denial, Sheffer stroke Nor :: PrimOp '[ HBool, HBool ] HBool -- Nor aka Joint Denial, aka Quine dagger, aka Pierce arrow -- -- Trigonometry operators Pi :: PrimOp '[] 'HProb Sin :: PrimOp '[ 'HReal ] 'HReal Cos :: PrimOp '[ 'HReal ] 'HReal Tan :: PrimOp '[ 'HReal ] 'HReal Asin :: PrimOp '[ 'HReal ] 'HReal Acos :: PrimOp '[ 'HReal ] 'HReal Atan :: PrimOp '[ 'HReal ] 'HReal Sinh :: PrimOp '[ 'HReal ] 'HReal Cosh :: PrimOp '[ 'HReal ] 'HReal Tanh :: PrimOp '[ 'HReal ] 'HReal Asinh :: PrimOp '[ 'HReal ] 'HReal Acosh :: PrimOp '[ 'HReal ] 'HReal Atanh :: PrimOp '[ 'HReal ] 'HReal -- -- Other Real\\/Prob-valued operators RealPow :: PrimOp '[ 'HProb, 'HReal ] 'HProb Exp :: PrimOp '[ 'HReal ] 'HProb Log :: PrimOp '[ 'HProb ] 'HReal Infinity :: HIntegrable a -> PrimOp '[] a GammaFunc :: PrimOp '[ 'HReal ] 'HProb BetaFunc :: PrimOp '[ 'HProb, 'HProb ] 'HProb -- -- -- Here we have the /polymorphic/ operators -- -- HEq and HOrd operators Equal :: !(HEq a) -> PrimOp '[ a, a ] HBool Less :: !(HOrd a) -> PrimOp '[ a, a ] HBool -- -- HSemiring operators (the non-n-ary ones) NatPow :: !(HSemiring a) -> PrimOp '[ a, 'HNat ] a -- -- HRing operators Negate :: !(HRing a) -> PrimOp '[ a ] a Abs :: !(HRing a) -> PrimOp '[ a ] (NonNegative a) Signum :: !(HRing a) -> PrimOp '[ a ] a -- -- HFractional operators Recip :: !(HFractional a) -> PrimOp '[ a ] a -- -- HRadical operators NatRoot :: !(HRadical a) -> PrimOp '[ a, 'HNat ] a -- -- HContinuous operators Erf :: !(HContinuous a) -> PrimOp '[ a ] a","title":"PrimOp"},{"location":"internals/coercions/","text":"Coercions For convenience, Hakaru offers functions to convert between the four different numeric types in the language. These types are nat - Natural numbers int - Integers prob - Positive real numbers real - Real numbers Amongst these types there are a collection of safe and unsafe coercions. A safe coercion is one which is always guaranteed to be valid. For example, converting a nat to an int is always safe. Converting an int to a nat is unsafe as the value can negative, and lead to runtime errors. These are represented in the AST using the CoerceTo and UnsafeFrom constructors. Note that coercions are always defined in terms of the safe direction to go to. CoerceTo_ :: !(Coercion a b) -> SCon '[ LC a ] b UnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a Internally, coercions are specified using the Coercion datatype. This datatype states that each coercion is made up of a series of primitive coercions. data Coercion :: Hakaru -> Hakaru -> * where CNil :: Coercion a a CCons :: !(PrimCoercion a b) -> !(Coercion b c) -> Coercion a c These primitive coercions can either involve loosening a restriction on the sign of the value, or changing the numeric value to be over a continuous value. For example, to coerce from int to real, we would have a single Coercion with a PrimCoercion in it with the Continuous data constructor. data PrimCoercion :: Hakaru -> Hakaru -> * where Signed :: !(HRing a) -> PrimCoercion (NonNegative a) a Continuous :: !(HContinuous a) -> PrimCoercion (HIntegral a) a","title":"Coercions"},{"location":"internals/coercions/#coercions","text":"For convenience, Hakaru offers functions to convert between the four different numeric types in the language. These types are nat - Natural numbers int - Integers prob - Positive real numbers real - Real numbers Amongst these types there are a collection of safe and unsafe coercions. A safe coercion is one which is always guaranteed to be valid. For example, converting a nat to an int is always safe. Converting an int to a nat is unsafe as the value can negative, and lead to runtime errors. These are represented in the AST using the CoerceTo and UnsafeFrom constructors. Note that coercions are always defined in terms of the safe direction to go to. CoerceTo_ :: !(Coercion a b) -> SCon '[ LC a ] b UnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a Internally, coercions are specified using the Coercion datatype. This datatype states that each coercion is made up of a series of primitive coercions. data Coercion :: Hakaru -> Hakaru -> * where CNil :: Coercion a a CCons :: !(PrimCoercion a b) -> !(Coercion b c) -> Coercion a c These primitive coercions can either involve loosening a restriction on the sign of the value, or changing the numeric value to be over a continuous value. For example, to coerce from int to real, we would have a single Coercion with a PrimCoercion in it with the Continuous data constructor. data PrimCoercion :: Hakaru -> Hakaru -> * where Signed :: !(HRing a) -> PrimCoercion (NonNegative a) a Continuous :: !(HContinuous a) -> PrimCoercion (HIntegral a) a","title":"Coercions"},{"location":"internals/datums/","text":"Data representation Data types are stored using a sum of product representation. They can be found in Language.Hakaru.Syntax.Datum . -- The first component is a hint for what the data constructor -- should be called when pretty-printing, giving error messages, -- etc. Like the hints for variable names, its value is not actually -- used to decide which constructor is meant or which pattern -- matches. data Datum :: (Hakaru -> *) -> Hakaru -> * where Datum :: {-# UNPACK #-} !Text -> !(Sing (HData' t)) -> !(DatumCode (Code t) ast (HData' t)) -> Datum ast (HData' t) -- | The intermediate components of a data constructor. The intuition -- behind the two indices is that the @[[HakaruFun]]@ is a functor -- applied to the Hakaru type. Initially the @[[HakaruFun]]@ functor -- will be the 'Code' associated with the Hakaru type; hence it's -- the one-step unrolling of the fixed point for our recursive -- datatypes. But as we go along, we'll be doing induction on the -- @[[HakaruFun]]@ functor. data DatumCode :: [[HakaruFun]] -> (Hakaru -> *) -> Hakaru -> * where -- Skip rightwards along the sum. Inr :: !(DatumCode xss abt a) -> DatumCode (xs ': xss) abt a -- Inject into the sum. Inl :: !(DatumStruct xs abt a) -> DatumCode (xs ': xss) abt a data DatumStruct :: [HakaruFun] -> (Hakaru -> *) -> Hakaru -> * where -- BUG: haddock doesn't like annotations on GADT constructors -- <https://github.com/hakaru-dev/hakaru/issues/6> -- Combine components of the product. (\\\"et\\\" means \\\"and\\\" in Latin) Et :: !(DatumFun x abt a) -> !(DatumStruct xs abt a) -> DatumStruct (x ': xs) abt a -- Close off the product. Done :: DatumStruct '[] abt a data DatumFun :: HakaruFun -> (Hakaru -> *) -> Hakaru -> * where -- Hit a leaf which isn't a recursive component of the datatype. Konst :: !(ast b) -> DatumFun ('K b) ast a -- Hit a leaf which is a recursive component of the datatype. Ident :: !(ast a) -> DatumFun 'I ast a In Hakaru we have implemented Bool, Pair, Either, Maybe, and List.","title":"Datums"},{"location":"internals/datums/#data-representation","text":"Data types are stored using a sum of product representation. They can be found in Language.Hakaru.Syntax.Datum . -- The first component is a hint for what the data constructor -- should be called when pretty-printing, giving error messages, -- etc. Like the hints for variable names, its value is not actually -- used to decide which constructor is meant or which pattern -- matches. data Datum :: (Hakaru -> *) -> Hakaru -> * where Datum :: {-# UNPACK #-} !Text -> !(Sing (HData' t)) -> !(DatumCode (Code t) ast (HData' t)) -> Datum ast (HData' t) -- | The intermediate components of a data constructor. The intuition -- behind the two indices is that the @[[HakaruFun]]@ is a functor -- applied to the Hakaru type. Initially the @[[HakaruFun]]@ functor -- will be the 'Code' associated with the Hakaru type; hence it's -- the one-step unrolling of the fixed point for our recursive -- datatypes. But as we go along, we'll be doing induction on the -- @[[HakaruFun]]@ functor. data DatumCode :: [[HakaruFun]] -> (Hakaru -> *) -> Hakaru -> * where -- Skip rightwards along the sum. Inr :: !(DatumCode xss abt a) -> DatumCode (xs ': xss) abt a -- Inject into the sum. Inl :: !(DatumStruct xs abt a) -> DatumCode (xs ': xss) abt a data DatumStruct :: [HakaruFun] -> (Hakaru -> *) -> Hakaru -> * where -- BUG: haddock doesn't like annotations on GADT constructors -- <https://github.com/hakaru-dev/hakaru/issues/6> -- Combine components of the product. (\\\"et\\\" means \\\"and\\\" in Latin) Et :: !(DatumFun x abt a) -> !(DatumStruct xs abt a) -> DatumStruct (x ': xs) abt a -- Close off the product. Done :: DatumStruct '[] abt a data DatumFun :: HakaruFun -> (Hakaru -> *) -> Hakaru -> * where -- Hit a leaf which isn't a recursive component of the datatype. Konst :: !(ast b) -> DatumFun ('K b) ast a -- Hit a leaf which is a recursive component of the datatype. Ident :: !(ast a) -> DatumFun 'I ast a In Hakaru we have implemented Bool, Pair, Either, Maybe, and List.","title":"Data representation"},{"location":"internals/newfeature/","text":"Adding a feature to the Hakaru language To add a feature to the Hakaru language you must Add an entry to the AST Update symbol resolution and optionally the parser to recognize this construct Update the pretty printers if this is something exposed to users Update the typechecker to handle it Update all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it Update the sampler if this primitive is intended to exist at runtime Update the compilers to emit the right code for this symbol TODO: We give an example of what this looks like by adding double to the language. Documenting a Hakaru Feature If you add a new feature to Hakaru, you should write accompanying documentation so that others can learn how to use it. The Hakaru documentation is written using MkDocs , which uses MarkDown to format source files. In order to download MkDocs, it is recommended that you use the Python package manager pip . The pip package manager is bundled with Python starting in versions 2.7.9 and 3.4, so it will be installed alongside Python. If you are using an earlier version of Python, you will need to install pip manually. On Windows, you must download get-pip.py and run it in the command prompr using python get-pip.py On Linux, you can install pip from the command line using sudo apt-get install python-pip On OSX, you can install pip from the command line using sudo easy-install pip Once you have installed pip , you can download the required packages for the Hakaru documentation: pip install mkdocs pip install python-markdown-math pip install mkdocs-extensions pip install mkdocs-bootswatch","title":"Adding a Language Feature"},{"location":"internals/newfeature/#adding-a-feature-to-the-hakaru-language","text":"To add a feature to the Hakaru language you must Add an entry to the AST Update symbol resolution and optionally the parser to recognize this construct Update the pretty printers if this is something exposed to users Update the typechecker to handle it Update all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it Update the sampler if this primitive is intended to exist at runtime Update the compilers to emit the right code for this symbol TODO: We give an example of what this looks like by adding double to the language.","title":"Adding a feature to the Hakaru language"},{"location":"internals/newfeature/#documenting-a-hakaru-feature","text":"If you add a new feature to Hakaru, you should write accompanying documentation so that others can learn how to use it. The Hakaru documentation is written using MkDocs , which uses MarkDown to format source files. In order to download MkDocs, it is recommended that you use the Python package manager pip . The pip package manager is bundled with Python starting in versions 2.7.9 and 3.4, so it will be installed alongside Python. If you are using an earlier version of Python, you will need to install pip manually. On Windows, you must download get-pip.py and run it in the command prompr using python get-pip.py On Linux, you can install pip from the command line using sudo apt-get install python-pip On OSX, you can install pip from the command line using sudo easy-install pip Once you have installed pip , you can download the required packages for the Hakaru documentation: pip install mkdocs pip install python-markdown-math pip install mkdocs-extensions pip install mkdocs-bootswatch","title":"Documenting a Hakaru Feature"},{"location":"internals/testing/","text":"Testing infrastructure in Hakaru Unit testing has been created for Hakaru\u2019s main functions. The test suite is managed by Cabal and written in Haskell. Tests written to test programs written using Hakaru are located in the tests/ subdirectory at the root of the project. Tests written in Haskell to check Hakaru functionality can be found at haskell/Tests/ . Running Tests Hakaru can be tested by running cabal test or stack test from the root directory of the project. Note: Tests that require Maple, such as simplify tests, are only run if a local installation of Maple is detected. Creating New Tests Hakaru testing is managed in the hakaru.cabal file found in the root directory. The main file for using the test suite is TestSuite.hs , which can be found in haskell\\tests . For all tests, two programs are required \u2013 the program to test and a program representing the expected result. Writing Hakaru Tests When creating Hakaru tests, your two programs must be saved as seperate files. You can add a test of your Hakaru programs to a Haskell program by using the testConcreteFiles function from haskell/Tests/TestTools.hs . This function takes two Hakaru programs as arguments. It first runs simplify on the first file and then asserts if it is equivilant to the second file. Writing Haskell Tests Haskell tests are created using the HUnit testing framework. Most existing Haskell tests use the testSStriv function from haskell/Tests/TestTools.hs . This function asserts that the first Haskell function passed to it simplifies to the same function provided by the second function argument. The simplification performed specifically calls the simplify Hakaru transform.","title":"Testing"},{"location":"internals/testing/#testing-infrastructure-in-hakaru","text":"Unit testing has been created for Hakaru\u2019s main functions. The test suite is managed by Cabal and written in Haskell. Tests written to test programs written using Hakaru are located in the tests/ subdirectory at the root of the project. Tests written in Haskell to check Hakaru functionality can be found at haskell/Tests/ .","title":"Testing infrastructure in Hakaru"},{"location":"internals/testing/#running-tests","text":"Hakaru can be tested by running cabal test or stack test from the root directory of the project. Note: Tests that require Maple, such as simplify tests, are only run if a local installation of Maple is detected.","title":"Running Tests"},{"location":"internals/testing/#creating-new-tests","text":"Hakaru testing is managed in the hakaru.cabal file found in the root directory. The main file for using the test suite is TestSuite.hs , which can be found in haskell\\tests . For all tests, two programs are required \u2013 the program to test and a program representing the expected result.","title":"Creating New Tests"},{"location":"internals/testing/#writing-hakaru-tests","text":"When creating Hakaru tests, your two programs must be saved as seperate files. You can add a test of your Hakaru programs to a Haskell program by using the testConcreteFiles function from haskell/Tests/TestTools.hs . This function takes two Hakaru programs as arguments. It first runs simplify on the first file and then asserts if it is equivilant to the second file.","title":"Writing Hakaru Tests"},{"location":"internals/testing/#writing-haskell-tests","text":"Haskell tests are created using the HUnit testing framework. Most existing Haskell tests use the testSStriv function from haskell/Tests/TestTools.hs . This function asserts that the first Haskell function passed to it simplifies to the same function provided by the second function argument. The simplification performed specifically calls the simplify Hakaru transform.","title":"Writing Haskell Tests"},{"location":"internals/transforms/","text":"Program transformations in Hakaru Coalesce Coalesce is an internal transformation that works on the untyped Hakaru AST. It takes recursive NAryOp terms that have the same type and combines them into a single term. For instance: 3.0 + 1.5 + 0.3 is parser as: NaryOp Sum [3.0, NaryOp Sum [1.5, NaryOp Sum [0.3]]] which when coalesced becomes: NaryOp Sum [3.0,1.5,0.3] Optimizations The Hakaru AST has a suite of standard compiler optimizations which have a substantial effect on the runtime of the resulting program. The current pipeline is described by the optimizations variable in Language.Hakaru.Syntax.Transforms . In order, the optimizations performed are: A-normalization Uniquification of variables (needed for let-floating) Let-floating Common subexpression elimination Pruning of dead binders Uniquification of variables (for the C backend) Constant Propagation Each pass is described in more detail below. A-normalization Found in Language.Hakaru.Syntax.ANF See The Essence of Compiling with Continuations by Flannigan, Sabry, Duba, and Felleisen A-normalization converts expressions into administrative normal form (ANF). This ensures that all intermediate values are named and all arguments to functions or primitive operations are either literals or variables. ANF is a common program representation for functional language compilers which can simplify some compiler passes and make others more effective. As an example, consider (add1 (let ([x (f y)]) 5)) This expression in ANF looks like the following (let ([x (f y)]) (add1 5)) which opens up the opportunity for constant folding to eliminate the (add1 5) expression. This pass exists mostly to simplify the implementation of CSE, but is useful for other passes as well. Uniquification Found in Language.Hakaru.Syntax.Uniquify Ensures all variables in the program have unique variable identifiers. This is not strictly necessary, but simplifies the implementation of other passes, several of which rely on this property. Let-floating Found in Language.Hakaru.Syntax.Hoist See Let-Floating: Moving Bindings to Give Faster Programs (1996) by Simon Peyton Jones , Will Partain , Andr\u00e9 Santos Let-floating alters the bindings structure of the program in order to improve performance. Typically, this entails moving definitions into or out of lambda expressions. When a lambda expression encodes a loop, this effectively accomplishes loop invariant code motion. This pass only moves definitions upward in the AST. For the most part, we are only interested in looping constructs like summate and product , and moving summate expressions out of other summate or product expressions when they do not depend on the index. This can radically alter the asymptotics of the resulting program, as nested loops are converted into sequentially executed loops. The only assumption this pass makes about the input AST is that all variable identifiers are unique. This is to handle the case where two branches of a match statement introduce the same variable. If both binders are hoisted out of the match statement, they one binding will shadow the other. This pass, as implemented, unconditionally floats expression to where their data dependencies are fulfilled. This is not safe in a general purpose language, and we may need to layer some heuristics on top of this pass to make it less aggressive if we end up introducing performance regressions. Common Subexpression Elimination Found in Language.Hakaru.Syntax.CSE Common subexpression elimination eliminates redundant computation by reusing results for equivalent expressions. The current implementation of this pass relies on the program being in ANF. ANF simplifies the implementation of CSE greatly by ensuring all expressions are named and that if two expressions may be shared, one of them is let-bound so that it dominates the other. In short, ANF simplifies the program to a simple top-down traversal of the AST. Consider the example (+ (add1 z) (add1 z)) Eliminating the common expression (add1 z) requires us to traverse the expression in evaluation order, track expression which have already been evaluated, recognize when an expression is duplicated, and introduce it with a new name that dominates all use sites of that expression. However, an expression in ANF allows us to perform CSE simply by keeping track of let-bound expressions and propagating those expressions downward into the AST. Consider the example in ANF (let ([t1 (add1 z)]) (let ([t2 (add1 z)]) (+ t1 t2))) To remove the common subexpression, we simply have to note that the (add1 z) bound to t2 is equivalent to the expression bound to t1 and replace it with the variable t1 . (let ([t1 (add1 z)]) (let ([t2 t1]) (+ t1 t2))) Trivial bindings can then be eliminated, if desired, giving (let ([t1 (add1 z)]) (+ t1 t1))) A major goal of CSE is to cleanup any work which is duplicated by the let-floating pass. Pruning Found in Language.Hakaru.Syntax.Prune This is essentially a limited form of dead code elimination. If an expression is bound to a variable which is never referenced, then that expression need never be executed, as the code language has no side effects. This pass serves to clean up some of the junk introduced by other passes. Cases which are handled (let ([x e1]) e2) => e2 if x not in fv(e2) (let ([x e1]) x) => e1 Constant Propagation Found in Language.Hakaru.Evalutation.ConstantPropagation Performs simple constant propagation and constant folding. The current implementation does not do that much work, mostly just evaluating primitive operations when their arguments are constant. Unused Passes Loop Peeling Found in Language.Hakaru.Syntax.Unroll Loop peeling was an initial attempt at performing loop invariant code motion by leveraging CSE to do most of the heavy lifting. Peeling is a common strategy to make other optimization passes \u201cloop-aware\u201d. The idea is to peel off one iteration of a loop and then apply the existing suite of optimizations. Consider the following summate whose body e is some loop-invariant computation. (summate lo hi (\u03bb x -> e)) After peeling we obtain (if (= lo hi) 0 (let ([x lo]) (let ([t1 e]) (let ([t2 (summate (+ lo 1) hi (\u03bb x -> e))]) (+ t1 t2))))) After applying CSE, the loop invariant body is simply reused on each iteration (if (= lo hi) 0 (let ([x lo]) (let ([t1 e]) (let ([t2 (summate (+ lo 1) hi (\u03bb x -> t1))]) (+ t1 t2))))) ANF ensures that all subexpression in the e bound to t1 are shareable with the copy of e used in the body of the summate , allowing us to hoist out subexpressions of e and not just the entire summate body. This pass is currently disabled in favor of the let-floating pass, which does a better job without causing an exponential blow up in code size. Some of Hakaru\u2019s looping constructs, such as array , cannot be peeled, so we cannot move loop invariant operations out of array statements.","title":"Transformaitons"},{"location":"internals/transforms/#program-transformations-in-hakaru","text":"","title":"Program transformations in Hakaru"},{"location":"internals/transforms/#coalesce","text":"Coalesce is an internal transformation that works on the untyped Hakaru AST. It takes recursive NAryOp terms that have the same type and combines them into a single term. For instance: 3.0 + 1.5 + 0.3 is parser as: NaryOp Sum [3.0, NaryOp Sum [1.5, NaryOp Sum [0.3]]] which when coalesced becomes: NaryOp Sum [3.0,1.5,0.3]","title":"Coalesce"},{"location":"internals/transforms/#optimizations","text":"The Hakaru AST has a suite of standard compiler optimizations which have a substantial effect on the runtime of the resulting program. The current pipeline is described by the optimizations variable in Language.Hakaru.Syntax.Transforms . In order, the optimizations performed are: A-normalization Uniquification of variables (needed for let-floating) Let-floating Common subexpression elimination Pruning of dead binders Uniquification of variables (for the C backend) Constant Propagation Each pass is described in more detail below.","title":"Optimizations"},{"location":"internals/transforms/#a-normalization","text":"Found in Language.Hakaru.Syntax.ANF See The Essence of Compiling with Continuations by Flannigan, Sabry, Duba, and Felleisen A-normalization converts expressions into administrative normal form (ANF). This ensures that all intermediate values are named and all arguments to functions or primitive operations are either literals or variables. ANF is a common program representation for functional language compilers which can simplify some compiler passes and make others more effective. As an example, consider (add1 (let ([x (f y)]) 5)) This expression in ANF looks like the following (let ([x (f y)]) (add1 5)) which opens up the opportunity for constant folding to eliminate the (add1 5) expression. This pass exists mostly to simplify the implementation of CSE, but is useful for other passes as well.","title":"A-normalization"},{"location":"internals/transforms/#uniquification","text":"Found in Language.Hakaru.Syntax.Uniquify Ensures all variables in the program have unique variable identifiers. This is not strictly necessary, but simplifies the implementation of other passes, several of which rely on this property.","title":"Uniquification"},{"location":"internals/transforms/#let-floating","text":"Found in Language.Hakaru.Syntax.Hoist See Let-Floating: Moving Bindings to Give Faster Programs (1996) by Simon Peyton Jones , Will Partain , Andr\u00e9 Santos Let-floating alters the bindings structure of the program in order to improve performance. Typically, this entails moving definitions into or out of lambda expressions. When a lambda expression encodes a loop, this effectively accomplishes loop invariant code motion. This pass only moves definitions upward in the AST. For the most part, we are only interested in looping constructs like summate and product , and moving summate expressions out of other summate or product expressions when they do not depend on the index. This can radically alter the asymptotics of the resulting program, as nested loops are converted into sequentially executed loops. The only assumption this pass makes about the input AST is that all variable identifiers are unique. This is to handle the case where two branches of a match statement introduce the same variable. If both binders are hoisted out of the match statement, they one binding will shadow the other. This pass, as implemented, unconditionally floats expression to where their data dependencies are fulfilled. This is not safe in a general purpose language, and we may need to layer some heuristics on top of this pass to make it less aggressive if we end up introducing performance regressions.","title":"Let-floating"},{"location":"internals/transforms/#common-subexpression-elimination","text":"Found in Language.Hakaru.Syntax.CSE Common subexpression elimination eliminates redundant computation by reusing results for equivalent expressions. The current implementation of this pass relies on the program being in ANF. ANF simplifies the implementation of CSE greatly by ensuring all expressions are named and that if two expressions may be shared, one of them is let-bound so that it dominates the other. In short, ANF simplifies the program to a simple top-down traversal of the AST. Consider the example (+ (add1 z) (add1 z)) Eliminating the common expression (add1 z) requires us to traverse the expression in evaluation order, track expression which have already been evaluated, recognize when an expression is duplicated, and introduce it with a new name that dominates all use sites of that expression. However, an expression in ANF allows us to perform CSE simply by keeping track of let-bound expressions and propagating those expressions downward into the AST. Consider the example in ANF (let ([t1 (add1 z)]) (let ([t2 (add1 z)]) (+ t1 t2))) To remove the common subexpression, we simply have to note that the (add1 z) bound to t2 is equivalent to the expression bound to t1 and replace it with the variable t1 . (let ([t1 (add1 z)]) (let ([t2 t1]) (+ t1 t2))) Trivial bindings can then be eliminated, if desired, giving (let ([t1 (add1 z)]) (+ t1 t1))) A major goal of CSE is to cleanup any work which is duplicated by the let-floating pass.","title":"Common Subexpression Elimination"},{"location":"internals/transforms/#pruning","text":"Found in Language.Hakaru.Syntax.Prune This is essentially a limited form of dead code elimination. If an expression is bound to a variable which is never referenced, then that expression need never be executed, as the code language has no side effects. This pass serves to clean up some of the junk introduced by other passes. Cases which are handled (let ([x e1]) e2) => e2 if x not in fv(e2) (let ([x e1]) x) => e1","title":"Pruning"},{"location":"internals/transforms/#constant-propagation","text":"Found in Language.Hakaru.Evalutation.ConstantPropagation Performs simple constant propagation and constant folding. The current implementation does not do that much work, mostly just evaluating primitive operations when their arguments are constant.","title":"Constant Propagation"},{"location":"internals/transforms/#unused-passes","text":"","title":"Unused Passes"},{"location":"internals/transforms/#loop-peeling","text":"Found in Language.Hakaru.Syntax.Unroll Loop peeling was an initial attempt at performing loop invariant code motion by leveraging CSE to do most of the heavy lifting. Peeling is a common strategy to make other optimization passes \u201cloop-aware\u201d. The idea is to peel off one iteration of a loop and then apply the existing suite of optimizations. Consider the following summate whose body e is some loop-invariant computation. (summate lo hi (\u03bb x -> e)) After peeling we obtain (if (= lo hi) 0 (let ([x lo]) (let ([t1 e]) (let ([t2 (summate (+ lo 1) hi (\u03bb x -> e))]) (+ t1 t2))))) After applying CSE, the loop invariant body is simply reused on each iteration (if (= lo hi) 0 (let ([x lo]) (let ([t1 e]) (let ([t2 (summate (+ lo 1) hi (\u03bb x -> t1))]) (+ t1 t2))))) ANF ensures that all subexpression in the e bound to t1 are shareable with the copy of e used in the body of the summate , allowing us to hoist out subexpressions of e and not just the entire summate body. This pass is currently disabled in favor of the let-floating pass, which does a better job without causing an exponential blow up in code size. Some of Hakaru\u2019s looping constructs, such as array , cannot be peeled, so we cannot move loop invariant operations out of array statements.","title":"Loop Peeling"},{"location":"intro/installation/","text":"Installing Hakaru You can download Hakaru by cloning the latest version from our GitHub repository: git clone https://github.com/hakaru-dev/hakaru.git Hakaru can be installed by using either stack install or cabal install inside the hakaru directory. One way that you can access these tools is by installing the Haskell Platform which supports Linux, OSX, and Windows operating systems. If you are using stack , you can install and verify your installation of Hakaru by running the commands: stack install stack test You can find the output of stack test in the .stack-work/logs/hakaru-0.4.0-test.txt file. If you are using cabal , you can install Hakaru by running the commands: cabal update cabal install -j --only-dependencies --enable-tests cabal configure --enable-tests cabal build cabal install cabal test On Windows systems, you can use the stack and cabal commands by running them in a Linux shell such as Cygwin or Git Bash. Note: If you want to use cabal and have installed the Haskell Platform, you might need to add a reference to the directory containing cabal.exe to the PATH environment variable. If you are using GHC 7.10 or earlier on a Windows system and want to use the cabal command, you must install the logfloat dependency manually after running cabal update due to a GHC bug : cabal update cabal install -j logfloat -f -useffi cabal install -j --only-dependencies --enable-tests cabal configure --enable-tests cabal build cabal install cabal test Extending Hakaru with Maple Hakaru uses Maple to perform computer-algebra guided optimizations. You must have a licensed copy of Maple installed to access this component of the Hakaru language. On Linux systems, Hakaru can be setup to use Maple by running: export LOCAL_MAPLE=\"`which maple`\" cd hakaru/maple echo 'libname := \"/path-to-hakaru/hakaru/maple\",libname:' >> ~/.mapleinit maple update-archive.mpl where \u201cpath-to-hakaru\u201d must be an explicit path (i.e. do not use ~ even if maple is installed in a sub-directory of your home directory). On Windows systems, Hakaru can be setup to use Maple by performing the following steps in Administrator mode: Create a User Environment Variable LOCAL_MAPLE using the Windows command prompt (cmd) by running: SETX LOCAL_MAPLE \"<path to Maple bin directory>\\cmaple.exe\" This variable can also be created via the Advanced System Properties. Note: You might need to restart your computer for the variable to be recognized. Add the path to cmaple.exe to your PATH system environment variable. This can be done via the Advanced System Properties. Note: You might need to restart your computer for the variable to be recognized. In the Windows command prompt (cmd), create a file maple.ini by running: echo libname := \"C:\\\\<path to hakaru>\\\\hakaru\\\\maple\",libname: >> \"C:\\<path to maple>\\lib\\maple.ini\" In the Windows command prompt (cmd), Navigate to the hakaru\\maple directory and run: cmaple update-archive.mpl Testing Your Maple Installation with Hakaru If you have correctly installaed Hakaru\u2019s Maple extension, running echo \"normal(0,1)\" | hk-maple -c Simplify - in a bash command line will return normal(0, 1) . If you get an error about hk-maple not being found, you may need to adjust your PATH environment variable to point to where cabal installed Hakaru. If you have not set the LOCAL_MAPLE environment variable, then the hk-maple command might try to locate a SSH file that might not exist on your machine to try and access a remote installation of Maple.","title":"Installing Hakaru"},{"location":"intro/installation/#installing-hakaru","text":"You can download Hakaru by cloning the latest version from our GitHub repository: git clone https://github.com/hakaru-dev/hakaru.git Hakaru can be installed by using either stack install or cabal install inside the hakaru directory. One way that you can access these tools is by installing the Haskell Platform which supports Linux, OSX, and Windows operating systems. If you are using stack , you can install and verify your installation of Hakaru by running the commands: stack install stack test You can find the output of stack test in the .stack-work/logs/hakaru-0.4.0-test.txt file. If you are using cabal , you can install Hakaru by running the commands: cabal update cabal install -j --only-dependencies --enable-tests cabal configure --enable-tests cabal build cabal install cabal test On Windows systems, you can use the stack and cabal commands by running them in a Linux shell such as Cygwin or Git Bash. Note: If you want to use cabal and have installed the Haskell Platform, you might need to add a reference to the directory containing cabal.exe to the PATH environment variable. If you are using GHC 7.10 or earlier on a Windows system and want to use the cabal command, you must install the logfloat dependency manually after running cabal update due to a GHC bug : cabal update cabal install -j logfloat -f -useffi cabal install -j --only-dependencies --enable-tests cabal configure --enable-tests cabal build cabal install cabal test","title":"Installing Hakaru"},{"location":"intro/installation/#extending-hakaru-with-maple","text":"Hakaru uses Maple to perform computer-algebra guided optimizations. You must have a licensed copy of Maple installed to access this component of the Hakaru language. On Linux systems, Hakaru can be setup to use Maple by running: export LOCAL_MAPLE=\"`which maple`\" cd hakaru/maple echo 'libname := \"/path-to-hakaru/hakaru/maple\",libname:' >> ~/.mapleinit maple update-archive.mpl where \u201cpath-to-hakaru\u201d must be an explicit path (i.e. do not use ~ even if maple is installed in a sub-directory of your home directory). On Windows systems, Hakaru can be setup to use Maple by performing the following steps in Administrator mode: Create a User Environment Variable LOCAL_MAPLE using the Windows command prompt (cmd) by running: SETX LOCAL_MAPLE \"<path to Maple bin directory>\\cmaple.exe\" This variable can also be created via the Advanced System Properties. Note: You might need to restart your computer for the variable to be recognized. Add the path to cmaple.exe to your PATH system environment variable. This can be done via the Advanced System Properties. Note: You might need to restart your computer for the variable to be recognized. In the Windows command prompt (cmd), create a file maple.ini by running: echo libname := \"C:\\\\<path to hakaru>\\\\hakaru\\\\maple\",libname: >> \"C:\\<path to maple>\\lib\\maple.ini\" In the Windows command prompt (cmd), Navigate to the hakaru\\maple directory and run: cmaple update-archive.mpl","title":"Extending Hakaru with Maple"},{"location":"intro/installation/#testing-your-maple-installation-with-hakaru","text":"If you have correctly installaed Hakaru\u2019s Maple extension, running echo \"normal(0,1)\" | hk-maple -c Simplify - in a bash command line will return normal(0, 1) . If you get an error about hk-maple not being found, you may need to adjust your PATH environment variable to point to where cabal installed Hakaru. If you have not set the LOCAL_MAPLE environment variable, then the hk-maple command might try to locate a SSH file that might not exist on your machine to try and access a remote installation of Maple.","title":"Testing Your Maple Installation with Hakaru"},{"location":"intro/probprog/","text":"What is Probabilistic Programming? Probabilistic programming is a software-driven method for creating probabilistic models and then using them to make probabilistic inferences. It provides a means for writing programs which describe probabilistic models such that they can be used to make probabilistic inferences. For example, the Hakaru program poisson(5) represents the Poisson distribution with a rate of five. A Probabilistic Programming Language (PPL) is a computer language designed to describe probabilistic models and distributions such that probabilistic inferences can be made programmatically 1 . Hakaru is an example of a PPL. Why do we need a programming language for describing probability distributions? Consider a machine learning problem. A typical workflow for this type of design is, when presented with a problem, to design an inference algorithm for a specific probabilistic distribution and query. The development of a distribution, query, and inference algorithm can be a time consuming task, even for someone that is skilled in these areas. Automating this process via a PPL allows for a broader exploration of the design space without the added effort that is required to using a traditional approach. Probabilistic Models The world is intrinsically an uncertain place. When you try to predict what will happen in the world given some data you have collected, you are engaging in some sort of probabilistic modeling. A distribution or program that describes your data is called the model . In probabilistic modeling, the quantity you wish to predict is treated as a parameter and known data is described as some noisy function of this parameter. This function is called the likelihood of the parameter. For example, you might want to estimate the average time it takes for a bus to arrive at a stop based on actual arrival times. In this situation, you determine that you can represent the likelihood function using a Poisson distribution: x \\sim \\text{Poisson}(\\lambda) where x is the actual arrival time, and \\lambda is the quantity you are using to make the prediction. You can also represent this likelihood function as a density function which returns how likely it is for x to be generated for a given choice of \\lambda : f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} Methods of Probabilistic Reasoning There are two main approaches to statistical reasoning: Frequentist and Bayesian. In Frequentist reasoning, the goal is to maximize the likelihood function. In the density model for our bus arrival times, this would mean finding a value for \\lambda that maximizes f . In Bayesian reasoning, an estimation is made using the given function parameters and a conditioned data set collected for the event. For our density model, we would design an estimation functions using our parameters and given it our set of bus arrival times to predict a value for f . You can use either approach to probabilistic reasoning in your Hakaru programs. Example: Bayesian Tug-of-War To demonstrate the value of this problem-solving approach, we will write a Hakaru program to represent a simplified version of the tug-of-war example from probmods.org. A completed version of this program can be found in the Hakaru examples directory . Three friends, Alice, Bob and Carol, want to know which of them is the strongest. They decide that the winner of a game of tug-of-war must be stronger than their opponent, and arrange to take turns playing tug-of-war against each other. The person who wins the most matches will be deemed the strongest of them. You can write a probabilistic program to represent this scenario. In this simulation, the friends are only interested in knowing who will win the third match ( match3 ) when they already know who won the first two matches. You can represent this problem using either the frequentist or Bayesian reasoning methods, but this example will use the Bayesian approach. Your program will begin with the description of the probabilistic model. You can assume that each friend\u2019s strength comes from a standard normal distribution and that the strength that they pull with also follows some normal distribution centered around their true strength. You can also trivially assume that the friend that pulled the hardest will win the match. You can represent this model in Hakaru by writing: def pulls(strength real): normal(strength, 1) def winner(a real, b real): a_pull <~ pulls(a) b_pull <~ pulls(b) return (a_pull > b_pull) alice <~ normal(0,1) bob <~ normal(0,1) carol <~ normal(0,1) match1 <~ winner(alice, bob) match2 <~ winner(bob, carol) match3 <~ winner(alice, carol) Note: This Hakaru code will not compile yet. Now that you have created your model, you can condition your sample generation based on known data. In the third match, Alice is competing against Carol. She wants to know how likely she is to win the match. She won her match against Bob ( match1 ) and that Carol lost her match to Bob ( match2 ). In your model, the result of match1 is True when Alice wins and the result of match2 is True when Carol loses. You can use this knowledge to write a conditions for your scenario that will return the result of match3 when Alice wins match1 and Carol loses match2 . If a simulation is run that does not match this pattern, it is rejected. This restriction can be written in Hakaru as: if match1 && match2: return match3 else: reject. measure(bool) You have now created a Hakaru program that describes a probabilistic model and restricted the accepted samples based on known data. You should save your program as tugofwar_rejection.hk so that you can run Hakaru to infer the outcome of match3 . If you call hakaru tugofwar_rejection.hk , you will get a continuous stream of Boolean results. You can make the calculations more legible by restricting the number of program executions and counting how many of each Boolean appears. For example, if you restrict the number of program executions to 10000 and collect the results, you will see that True occurs much more frequently than False . This means that Alice is likely to win match3 against Carol. hakaru -w tugofwar_rejection.hk | head -n 10000 | sort | uniq -c 3060 false 6940 true Simulation and Inference Ideally, you could collect a sufficient number of samples from your observed population to create your probabilistic model. This could be accomplished with populations that only require a manageable number of samples or that are easy to collect. However, for many experiments this is not possible due to limited resources and time. In cases like this, you could generate samples using a simulation . In a simulation, you can select the population\u2019s mean and then generate values around this data point. If you wanted to know what a population would look like with a different mean, you simply need to change that value in your model and run the simulation again. What about the cases where you do have some samples and you want to know something about it? In this case, you use the data you have to guide the generation of samples in order to learn how the data occurred. This approach is called inference . To be able to make inferences from your known samples, you must add reasoning mechanisms to your model to gauge the usefulness of a model-generated sample with respect to some data that you have already collected. For example, you might have collected some disease data from a hospital and want to know how it spread in the affected patients. After creating a probabilistic model of disease transmission, you can use your collected data to reason about the samples generated from your model to judge its relevance in the creation of the data that you have collected. In the tug-of-war example, you used Hakaru to restrict which samples were kept (Alice must have won match1 and Bob must have won match2 ) and which ones were discarded. This inference approach is called rejection sampling because restricted samples generated from your model are discarded. Would this approach still work if the model were changed? Could we use this same technique to determine if Alice will win her match and by how much? As you pose more complex questions, creating models as rejection samplers becomes increasingly inefficient because of the number of discarded samples. It would be better if your model could be transformed such that only observed data points are generated so that computational resources are not wasted on data that will not exist in your data set. Hakaru uses importance sampling where, instead of being rejected immediately, each sample is assigned a weight so that a sample average can be calculated. As more samples are generated, sample weights are updated to reflect the likelihood of that sample\u2019s rejection. While this works well for model inference when the model has only a few dimensions, there are more powerful tools that can be used for more complex scenarios. The Metropolis-Hastings Algorithm: A Markov Chain Monte Carlo Method You might encounter situations where direct sampling from your model is difficult, which is common for multi-dimensional models. In models with high dimensionality, sample points tend to cluster in regions so that when a \u201cgood\u201d sample is found, there is a higher chance of finding other good samples in the same area. This means that we want to stay in that region to collect more. In this situation, importance sampling becomes less efficient because it does not consider what other samples it has already found when generating a new one. Instead, a Markov Chain Monte Carlo (MCMC) method should be used. The MCMC methods are used to sample probability distributions by constructing a Markov Chain. A Markov Chain is used to make predictions solely based on a process\u2019s current state, so it does not require extensive memory for its calculations. In MCMC, a Markov chain is used to generate the next sample based on the current one, making it more likely to stay in densely packed probability regions. As a model increases in dimensions, MCMC methods become essential for the generation of samples because the task of finding high-value samples becomes more difficult. The Metropolis-Hastings algorithm 2 is an MCMC method for generating a sequence of random samples from a probabilistic distribution. This is useful for approximating a distribution that fits your existing data. The algorithm is included in Hakaru\u2019s transformations as the command tool mh . This transform converts your probabilistic program into a Markov Chain which can be used for sample generation. Proababilistic programming language (Wikipedia) \u21a9 D.J.C. MacKay, \u201cIntroduction to Monte Carlo Methods\u201d, Learning in Graphical Models, vol. 89, pp. 175-204, 1998. \u21a9","title":"What is Probabilistic Programming?"},{"location":"intro/probprog/#what-is-probabilistic-programming","text":"Probabilistic programming is a software-driven method for creating probabilistic models and then using them to make probabilistic inferences. It provides a means for writing programs which describe probabilistic models such that they can be used to make probabilistic inferences. For example, the Hakaru program poisson(5) represents the Poisson distribution with a rate of five. A Probabilistic Programming Language (PPL) is a computer language designed to describe probabilistic models and distributions such that probabilistic inferences can be made programmatically 1 . Hakaru is an example of a PPL. Why do we need a programming language for describing probability distributions? Consider a machine learning problem. A typical workflow for this type of design is, when presented with a problem, to design an inference algorithm for a specific probabilistic distribution and query. The development of a distribution, query, and inference algorithm can be a time consuming task, even for someone that is skilled in these areas. Automating this process via a PPL allows for a broader exploration of the design space without the added effort that is required to using a traditional approach.","title":"What is Probabilistic Programming?"},{"location":"intro/probprog/#probabilistic-models","text":"The world is intrinsically an uncertain place. When you try to predict what will happen in the world given some data you have collected, you are engaging in some sort of probabilistic modeling. A distribution or program that describes your data is called the model . In probabilistic modeling, the quantity you wish to predict is treated as a parameter and known data is described as some noisy function of this parameter. This function is called the likelihood of the parameter. For example, you might want to estimate the average time it takes for a bus to arrive at a stop based on actual arrival times. In this situation, you determine that you can represent the likelihood function using a Poisson distribution: x \\sim \\text{Poisson}(\\lambda) where x is the actual arrival time, and \\lambda is the quantity you are using to make the prediction. You can also represent this likelihood function as a density function which returns how likely it is for x to be generated for a given choice of \\lambda : f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}","title":"Probabilistic Models"},{"location":"intro/probprog/#methods-of-probabilistic-reasoning","text":"There are two main approaches to statistical reasoning: Frequentist and Bayesian. In Frequentist reasoning, the goal is to maximize the likelihood function. In the density model for our bus arrival times, this would mean finding a value for \\lambda that maximizes f . In Bayesian reasoning, an estimation is made using the given function parameters and a conditioned data set collected for the event. For our density model, we would design an estimation functions using our parameters and given it our set of bus arrival times to predict a value for f . You can use either approach to probabilistic reasoning in your Hakaru programs.","title":"Methods of Probabilistic Reasoning"},{"location":"intro/probprog/#example-bayesian-tug-of-war","text":"To demonstrate the value of this problem-solving approach, we will write a Hakaru program to represent a simplified version of the tug-of-war example from probmods.org. A completed version of this program can be found in the Hakaru examples directory . Three friends, Alice, Bob and Carol, want to know which of them is the strongest. They decide that the winner of a game of tug-of-war must be stronger than their opponent, and arrange to take turns playing tug-of-war against each other. The person who wins the most matches will be deemed the strongest of them. You can write a probabilistic program to represent this scenario. In this simulation, the friends are only interested in knowing who will win the third match ( match3 ) when they already know who won the first two matches. You can represent this problem using either the frequentist or Bayesian reasoning methods, but this example will use the Bayesian approach. Your program will begin with the description of the probabilistic model. You can assume that each friend\u2019s strength comes from a standard normal distribution and that the strength that they pull with also follows some normal distribution centered around their true strength. You can also trivially assume that the friend that pulled the hardest will win the match. You can represent this model in Hakaru by writing: def pulls(strength real): normal(strength, 1) def winner(a real, b real): a_pull <~ pulls(a) b_pull <~ pulls(b) return (a_pull > b_pull) alice <~ normal(0,1) bob <~ normal(0,1) carol <~ normal(0,1) match1 <~ winner(alice, bob) match2 <~ winner(bob, carol) match3 <~ winner(alice, carol) Note: This Hakaru code will not compile yet. Now that you have created your model, you can condition your sample generation based on known data. In the third match, Alice is competing against Carol. She wants to know how likely she is to win the match. She won her match against Bob ( match1 ) and that Carol lost her match to Bob ( match2 ). In your model, the result of match1 is True when Alice wins and the result of match2 is True when Carol loses. You can use this knowledge to write a conditions for your scenario that will return the result of match3 when Alice wins match1 and Carol loses match2 . If a simulation is run that does not match this pattern, it is rejected. This restriction can be written in Hakaru as: if match1 && match2: return match3 else: reject. measure(bool) You have now created a Hakaru program that describes a probabilistic model and restricted the accepted samples based on known data. You should save your program as tugofwar_rejection.hk so that you can run Hakaru to infer the outcome of match3 . If you call hakaru tugofwar_rejection.hk , you will get a continuous stream of Boolean results. You can make the calculations more legible by restricting the number of program executions and counting how many of each Boolean appears. For example, if you restrict the number of program executions to 10000 and collect the results, you will see that True occurs much more frequently than False . This means that Alice is likely to win match3 against Carol. hakaru -w tugofwar_rejection.hk | head -n 10000 | sort | uniq -c 3060 false 6940 true","title":"Example: Bayesian Tug-of-War"},{"location":"intro/probprog/#simulation-and-inference","text":"Ideally, you could collect a sufficient number of samples from your observed population to create your probabilistic model. This could be accomplished with populations that only require a manageable number of samples or that are easy to collect. However, for many experiments this is not possible due to limited resources and time. In cases like this, you could generate samples using a simulation . In a simulation, you can select the population\u2019s mean and then generate values around this data point. If you wanted to know what a population would look like with a different mean, you simply need to change that value in your model and run the simulation again. What about the cases where you do have some samples and you want to know something about it? In this case, you use the data you have to guide the generation of samples in order to learn how the data occurred. This approach is called inference . To be able to make inferences from your known samples, you must add reasoning mechanisms to your model to gauge the usefulness of a model-generated sample with respect to some data that you have already collected. For example, you might have collected some disease data from a hospital and want to know how it spread in the affected patients. After creating a probabilistic model of disease transmission, you can use your collected data to reason about the samples generated from your model to judge its relevance in the creation of the data that you have collected. In the tug-of-war example, you used Hakaru to restrict which samples were kept (Alice must have won match1 and Bob must have won match2 ) and which ones were discarded. This inference approach is called rejection sampling because restricted samples generated from your model are discarded. Would this approach still work if the model were changed? Could we use this same technique to determine if Alice will win her match and by how much? As you pose more complex questions, creating models as rejection samplers becomes increasingly inefficient because of the number of discarded samples. It would be better if your model could be transformed such that only observed data points are generated so that computational resources are not wasted on data that will not exist in your data set. Hakaru uses importance sampling where, instead of being rejected immediately, each sample is assigned a weight so that a sample average can be calculated. As more samples are generated, sample weights are updated to reflect the likelihood of that sample\u2019s rejection. While this works well for model inference when the model has only a few dimensions, there are more powerful tools that can be used for more complex scenarios.","title":"Simulation and Inference"},{"location":"intro/probprog/#the-metropolis-hastings-algorithm-a-markov-chain-monte-carlo-method","text":"You might encounter situations where direct sampling from your model is difficult, which is common for multi-dimensional models. In models with high dimensionality, sample points tend to cluster in regions so that when a \u201cgood\u201d sample is found, there is a higher chance of finding other good samples in the same area. This means that we want to stay in that region to collect more. In this situation, importance sampling becomes less efficient because it does not consider what other samples it has already found when generating a new one. Instead, a Markov Chain Monte Carlo (MCMC) method should be used. The MCMC methods are used to sample probability distributions by constructing a Markov Chain. A Markov Chain is used to make predictions solely based on a process\u2019s current state, so it does not require extensive memory for its calculations. In MCMC, a Markov chain is used to generate the next sample based on the current one, making it more likely to stay in densely packed probability regions. As a model increases in dimensions, MCMC methods become essential for the generation of samples because the task of finding high-value samples becomes more difficult. The Metropolis-Hastings algorithm 2 is an MCMC method for generating a sequence of random samples from a probabilistic distribution. This is useful for approximating a distribution that fits your existing data. The algorithm is included in Hakaru\u2019s transformations as the command tool mh . This transform converts your probabilistic program into a Markov Chain which can be used for sample generation. Proababilistic programming language (Wikipedia) \u21a9 D.J.C. MacKay, \u201cIntroduction to Monte Carlo Methods\u201d, Learning in Graphical Models, vol. 89, pp. 175-204, 1998. \u21a9","title":"The Metropolis-Hastings Algorithm: A Markov Chain Monte Carlo Method"},{"location":"intro/quickstart/","text":"Quick Start: A Mixture Model Example Let\u2019s start with a simple model of a coin toss experiment so that you can become familiar with some of Hakaru\u2019s data types and functionality. We will assume that a single coin flip can be represented using a Bernoulli distribution. After we have created the Bernoulli model, we will use it to create a mixture model and condition the model to estimate what the original coin toss experiment looked like based on the resulting mixture model samples. Modeling a Bernoulli Experiment We will use the categorical Hakaru Random Primitive to write a Bernoulli distribution 1 for our model. The categorical primitive requires an array representing the probability of achieving each category in the experiement. Let\u2019s start with a fair experiment and state that each side of the coin has an equal chance of being picked. The result of the coin toss is stored in the variable b using Hakaru\u2019s notation for bind : b <~ categorical([0.5, 0.5]) For data type simplicity, we will map Heads to true and Tails to false . By putting the values of true and false into an array, we can use the value in b to select which of them to return as the result of the coin toss: return [true, false][b] A characteristic of the Bernoulli distribution is that it assumes that only one experiment is conducted. To collect samples, we need to run this experiment multiple times. To aid in this task, we can rewrite the Bernoulli model as a function . We will call our function bern : def bern (): b <~ categorical([0.5, 0.5]) return [true, false][b] Now that we are using functions, we can generalize our model so that we can run experiments on both fair and trick coins. To do this, we should pass in a probability p as a function argument, which is then used to populate the categorical primitive. Hakaru has a specialized data type for probabilities called prob , which we will use as the data type for our function input: def bern (p prob): b <~ categorical([p, (1 - p)]) return [true, false][b] If you we to run this function, we will get a Type Mismatch error. This is because the value (1 - p) is converted to type real as a result of the subtraction operation and categorical expects all of the values in its array to be of type prob . One solution would be to manually pass in the value of (1 - p) as a function argument, which would artificially complicate our function. Instead, we can use Hakaru\u2019s coercions to recast (1 - p) to type prob : def bern (p prob): b <~ categorical([p, real2prob(1 - p)]) return [true, false][b] We can now use our model to run a series of Bernoullli experiments. Let\u2019s set up our program to use a fair coin and save it as bernoulli.hk : def bern (p prob): b <~ categorical([p, real2prob(1 - p)]) return [true, false][b] bern(0.5) Running this program using hakaru bernoulli.hk should result in an infinite stream of coin toss trials: false true false true true true ... Now that we have set up our Bernoulli experiment, let\u2019s use it to create a mixture model. Creating a Mixture Model Let\u2019s use our coin flip experiment to create a mixture model by drawing a sample from a normal distribution when the coin is Heads ( true ) and from a uniform distribution when it is Tails ( false ). This is called a mixture model 2 because we are selecting samples from different distributions. Let\u2019s start by saving a copy of your Bernoulli function into a new program so that we can use it in our new model. For this example, we will call it twomixture.hk . Let\u2019s start by binding the return value of our bern function to a variable called coin to represent the outcome of an experiment: coin <~ bern(0.5) Now that we have stored the result of our experiment, let\u2019s use it to generate a sample. Our model has a selection condition where Heads causes a sample to be drawn from achieving normal distribution and Tails draws from a uniform distribution. There are two ways of handling this in Hakaru \u2013 conditionals and pattern matching . Since we are working with Booleans, let\u2019s use patern matching so that we can see what it looks like in Hakaru. Hakaru pattern matching requires a sequence and a set of possible patterns to compare the sequence to. In our model, our sequence would be coin because that is what we are using to select a distribution. Our possible patterns are the possible values that coin could have \u2013 true and false . When the pattern is true , we call the normal distribution and when it is false we call the uniform distribution. Both the normal and uniform functions are included in Hakaru\u2019s Random Primitives , so we do not need to define our own functions for them. The outcome of the pattern match will not be saved unless we bind it to a variable, so let\u2019s bind it to a variable called sample : sample <~ match coin: true: normal(0,1) false: uniform(0,1) Now that we have both the result of our coin toss experiment ( coin ) and our mixture model ( sample ), we can return the values: return(coin, sample) We have completed the mixture model program and can run it using the command hakaru twomixture.hk to collect samples indefinitely: (true, -0.37622272051934547) (false, 4.666320977960081e-2) (true, 1.3351978120820147) (true, 0.4657111228024136) (false, 0.6528078075939211) (false, 0.2410145787295287) (false, 0.624335005419879) (true, -1.5127939371882644) (false, 0.15925713370352967) (true, 2.2762774663914114e-2) ... Of course, Hakaru would not be very interesting if it only provided the means for you to define your model. Let\u2019s try conditioning our model so that we can experiment with different values for sample to estimate what values of coin were used. Conditioning a Hakaru Program Suppose for our twomixture.hk program, we know the value of sample and want to see what the original values for coin were. We can symbolically produce the unnormalized conditional distribution from which coin samples are taken by using Hakaru\u2019s disintegration transform. Before we use disintegrate , we must change the line return (coin, sample) to return (sample, coin) . This tells disintegrate that we want to create a posterior distribution for coin using known values for sample . Once we have setup our model for the disintegrate transform, we can transform our model by calling disintegrate twomixture.hk . The disintegrate transform creates a new model written as an anonymous function so that it is easier for you to use in other applications. In the model generated by the disintegrate transform, our variable sample has been renamed to x5 : fn x5 real: bern = fn p prob: b <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][b] coin <~ bern(1/2) (match coin: true: x12 <~ weight((exp((negate(((x5 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi))), return ()) return coin _: reject. measure(bool)) <|> bern = fn p prob: b <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][b] coin <~ bern(1/2) (match coin: false: (match (not((x5 < 0)) && not((1 < x5))): true: x12 <~ return () return coin _: reject. measure(bool)) _: reject. measure(bool)) Note: The output for disintegrate will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling hakaru disintegrate model1.hk > modelDis.hk . For this example, we will call our new program twomixture_D.hk . We can use this program to experiment with different values of sample ( x5 ) to see what the original coin toss experiment looked like. To avoid altering the function generated by disintegrate , let\u2019s assign it to a variable coinToss so that we can reference it at the end of our program. For our first experiment, let\u2019s try a value of 0.3 . This means that we are conditioning our model to be more likely to pick samples from the uniform distribution: coinToss = fn x5 real: bern = fn p prob: b <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][b] coin <~ bern(1/2) (match coin: true: x12 <~ weight((exp((negate(((x5 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi))), return ()) return coin _: reject. measure(bool)) <|> bern = fn p prob: b <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][b] coin <~ bern(1/2) (match coin: false: (match (not((x5 < 0)) && not((1 < x5))): true: x12 <~ return () return coin _: reject. measure(bool)) _: reject. measure(bool)) coinToss(0.3) We can now run the program to estimate what values for coin . Let\u2019s use some Unix commands to run the program 1000 times and gather the results into counts: hakaru -w twomixture_D.hk | head -n 1000 | sort | uniq -c 526 false 474 true As we can see, when x5 = 0.3 , our coin tosses were more likely to be Tails ( false ) than Heads ( true ). Let\u2019s change our argument to coinToss to 3.0 so that we are conditioned to pick values from the normal distribution much more frequently. Running this program shows that our coin tosses must have all been Heads for this value to be possible: hakaru -w twomixture_D.hk | head -n 1000 | sort | uniq -c 1000 true You have written a model to represent a Bernoulli experiement and used it to create a mixture model using a normal and uniform distribution. You have also used the disintegrate transform to generate a new model that can be conditioned with different mixture model results to infer what the original distribution of coin toss experiements might have been. For more Hakaru examples, see the Examples . Bernoulli distribution (Wikipedia) \u21a9 Mixture Model (Wikipedia) \u21a9","title":"Quick Start: A Mixture Model Example"},{"location":"intro/quickstart/#quick-start-a-mixture-model-example","text":"Let\u2019s start with a simple model of a coin toss experiment so that you can become familiar with some of Hakaru\u2019s data types and functionality. We will assume that a single coin flip can be represented using a Bernoulli distribution. After we have created the Bernoulli model, we will use it to create a mixture model and condition the model to estimate what the original coin toss experiment looked like based on the resulting mixture model samples.","title":"Quick Start: A Mixture Model Example"},{"location":"intro/quickstart/#modeling-a-bernoulli-experiment","text":"We will use the categorical Hakaru Random Primitive to write a Bernoulli distribution 1 for our model. The categorical primitive requires an array representing the probability of achieving each category in the experiement. Let\u2019s start with a fair experiment and state that each side of the coin has an equal chance of being picked. The result of the coin toss is stored in the variable b using Hakaru\u2019s notation for bind : b <~ categorical([0.5, 0.5]) For data type simplicity, we will map Heads to true and Tails to false . By putting the values of true and false into an array, we can use the value in b to select which of them to return as the result of the coin toss: return [true, false][b] A characteristic of the Bernoulli distribution is that it assumes that only one experiment is conducted. To collect samples, we need to run this experiment multiple times. To aid in this task, we can rewrite the Bernoulli model as a function . We will call our function bern : def bern (): b <~ categorical([0.5, 0.5]) return [true, false][b] Now that we are using functions, we can generalize our model so that we can run experiments on both fair and trick coins. To do this, we should pass in a probability p as a function argument, which is then used to populate the categorical primitive. Hakaru has a specialized data type for probabilities called prob , which we will use as the data type for our function input: def bern (p prob): b <~ categorical([p, (1 - p)]) return [true, false][b] If you we to run this function, we will get a Type Mismatch error. This is because the value (1 - p) is converted to type real as a result of the subtraction operation and categorical expects all of the values in its array to be of type prob . One solution would be to manually pass in the value of (1 - p) as a function argument, which would artificially complicate our function. Instead, we can use Hakaru\u2019s coercions to recast (1 - p) to type prob : def bern (p prob): b <~ categorical([p, real2prob(1 - p)]) return [true, false][b] We can now use our model to run a series of Bernoullli experiments. Let\u2019s set up our program to use a fair coin and save it as bernoulli.hk : def bern (p prob): b <~ categorical([p, real2prob(1 - p)]) return [true, false][b] bern(0.5) Running this program using hakaru bernoulli.hk should result in an infinite stream of coin toss trials: false true false true true true ... Now that we have set up our Bernoulli experiment, let\u2019s use it to create a mixture model.","title":"Modeling a Bernoulli Experiment"},{"location":"intro/quickstart/#creating-a-mixture-model","text":"Let\u2019s use our coin flip experiment to create a mixture model by drawing a sample from a normal distribution when the coin is Heads ( true ) and from a uniform distribution when it is Tails ( false ). This is called a mixture model 2 because we are selecting samples from different distributions. Let\u2019s start by saving a copy of your Bernoulli function into a new program so that we can use it in our new model. For this example, we will call it twomixture.hk . Let\u2019s start by binding the return value of our bern function to a variable called coin to represent the outcome of an experiment: coin <~ bern(0.5) Now that we have stored the result of our experiment, let\u2019s use it to generate a sample. Our model has a selection condition where Heads causes a sample to be drawn from achieving normal distribution and Tails draws from a uniform distribution. There are two ways of handling this in Hakaru \u2013 conditionals and pattern matching . Since we are working with Booleans, let\u2019s use patern matching so that we can see what it looks like in Hakaru. Hakaru pattern matching requires a sequence and a set of possible patterns to compare the sequence to. In our model, our sequence would be coin because that is what we are using to select a distribution. Our possible patterns are the possible values that coin could have \u2013 true and false . When the pattern is true , we call the normal distribution and when it is false we call the uniform distribution. Both the normal and uniform functions are included in Hakaru\u2019s Random Primitives , so we do not need to define our own functions for them. The outcome of the pattern match will not be saved unless we bind it to a variable, so let\u2019s bind it to a variable called sample : sample <~ match coin: true: normal(0,1) false: uniform(0,1) Now that we have both the result of our coin toss experiment ( coin ) and our mixture model ( sample ), we can return the values: return(coin, sample) We have completed the mixture model program and can run it using the command hakaru twomixture.hk to collect samples indefinitely: (true, -0.37622272051934547) (false, 4.666320977960081e-2) (true, 1.3351978120820147) (true, 0.4657111228024136) (false, 0.6528078075939211) (false, 0.2410145787295287) (false, 0.624335005419879) (true, -1.5127939371882644) (false, 0.15925713370352967) (true, 2.2762774663914114e-2) ... Of course, Hakaru would not be very interesting if it only provided the means for you to define your model. Let\u2019s try conditioning our model so that we can experiment with different values for sample to estimate what values of coin were used.","title":"Creating a Mixture Model"},{"location":"intro/quickstart/#conditioning-a-hakaru-program","text":"Suppose for our twomixture.hk program, we know the value of sample and want to see what the original values for coin were. We can symbolically produce the unnormalized conditional distribution from which coin samples are taken by using Hakaru\u2019s disintegration transform. Before we use disintegrate , we must change the line return (coin, sample) to return (sample, coin) . This tells disintegrate that we want to create a posterior distribution for coin using known values for sample . Once we have setup our model for the disintegrate transform, we can transform our model by calling disintegrate twomixture.hk . The disintegrate transform creates a new model written as an anonymous function so that it is easier for you to use in other applications. In the model generated by the disintegrate transform, our variable sample has been renamed to x5 : fn x5 real: bern = fn p prob: b <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][b] coin <~ bern(1/2) (match coin: true: x12 <~ weight((exp((negate(((x5 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi))), return ()) return coin _: reject. measure(bool)) <|> bern = fn p prob: b <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][b] coin <~ bern(1/2) (match coin: false: (match (not((x5 < 0)) && not((1 < x5))): true: x12 <~ return () return coin _: reject. measure(bool)) _: reject. measure(bool)) Note: The output for disintegrate will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling hakaru disintegrate model1.hk > modelDis.hk . For this example, we will call our new program twomixture_D.hk . We can use this program to experiment with different values of sample ( x5 ) to see what the original coin toss experiment looked like. To avoid altering the function generated by disintegrate , let\u2019s assign it to a variable coinToss so that we can reference it at the end of our program. For our first experiment, let\u2019s try a value of 0.3 . This means that we are conditioning our model to be more likely to pick samples from the uniform distribution: coinToss = fn x5 real: bern = fn p prob: b <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][b] coin <~ bern(1/2) (match coin: true: x12 <~ weight((exp((negate(((x5 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi))), return ()) return coin _: reject. measure(bool)) <|> bern = fn p prob: b <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][b] coin <~ bern(1/2) (match coin: false: (match (not((x5 < 0)) && not((1 < x5))): true: x12 <~ return () return coin _: reject. measure(bool)) _: reject. measure(bool)) coinToss(0.3) We can now run the program to estimate what values for coin . Let\u2019s use some Unix commands to run the program 1000 times and gather the results into counts: hakaru -w twomixture_D.hk | head -n 1000 | sort | uniq -c 526 false 474 true As we can see, when x5 = 0.3 , our coin tosses were more likely to be Tails ( false ) than Heads ( true ). Let\u2019s change our argument to coinToss to 3.0 so that we are conditioned to pick values from the normal distribution much more frequently. Running this program shows that our coin tosses must have all been Heads for this value to be possible: hakaru -w twomixture_D.hk | head -n 1000 | sort | uniq -c 1000 true You have written a model to represent a Bernoulli experiement and used it to create a mixture model using a normal and uniform distribution. You have also used the disintegrate transform to generate a new model that can be conditioned with different mixture model results to infer what the original distribution of coin toss experiements might have been. For more Hakaru examples, see the Examples . Bernoulli distribution (Wikipedia) \u21a9 Mixture Model (Wikipedia) \u21a9","title":"Conditioning a Hakaru Program"},{"location":"intro/samplegen/","text":"Generating Samples from your Hakaru Program The Hakaru language is designed so that it is easy to express probabilistic models programmatically. In particular, Hakaru makes makes it easy to use Monte Carlo methods, which aim to generate individual samples and estimate expectation functions, or expectation, from a given distribution. The first task, drawing samples from a distribution, is often difficult because it might not be possible to sample from the target distribution directly. This can be exasperated as the dimensionality of the sample space increases. In these scenarios, a comparable distribution can be selected to draw samples from. Importance sampling is a Monte Carlo method that is used to generate samples by estimating an expectation for the target distribution instead. The estimated expectation is then used to generate samples. To account for the knowledge that the samples were not generated from the target distribution, a weight is assigned so that each sample\u2019s contribution to the estimator is adjusted according to its relevance. However, this method only works well if the distribution proposed by the expectation is similar to the target distribution. For more complex distributions, a different approach, such as the Metropolis Hastings method should be used 1 . The hakaru command is used to indefinitely generate samples from a Hakaru program using importance sampling. Each sample is assigned a weight, and a sample\u2019s weight is initialized to 1.0 . Weights are changed by Hakaru primitives and processes such as weight . Usage The hakaru command can take up to two Hakaru programs as arguments. If only one program is provided, the hakaru command generates samples based on the model described in the Hakaru program. In this case, the hakaru command can be invoked in the command-line by calling: hakaru hakaru_program.hk If a second program is given to the hakaru command, it will treat the two programs as the start of a Markov Chain. This is used when you have created a transition kernel using the Metropolis Hastings transformation. To invoke the hakaru command with a transition kernel, you would call: hakaru --transition-kernel transition.hk init.hk The first program, transition.hk , is treated as the transition kernel and the second program, init.hk , is treated as the initial state of the Markov Chain. When the hakaru command is run, a sample is drawn from init.hk . This sample is then passed to transition.hk to generate the second sample. After this point, samples generated from transition.hk are passed back into itself to generate further samples. The Dash ( - ) Operator You might encounter some scenarios where you wish to run a Hakaru command or transformation on a program and then send the resulting output to another command or transform. In these cases, you can take advantage of the dash ( - ) command-line notation. The dash notation is a shortcut used to pass standard inputs and outputs to another command in the same line of script. For example, if you wanted to run the disintegrate Hakaru command followed by the hk-maple -c Simplify command, you would enter: disintegrate program.hk | hk-maple -c Simplify - This command is equivalent to entering: disintegrate program.hk > temp.hk hk-maple -c Simplify temp.hk Note: The > operator redirects the output from disintegrate program.hk to a new file called temp.hk . Example To demonstrate weights in Hakaru, a sample problem of a burglary alarm is adapted from Pearl\u2019s textbook on probabilistic reasoning (page 35) 2 : Imagine being awakened one night by the shrill sound of your burglar alarm. What is your degree of belief that a burglary attempt has taken place? For illustrative purposes we make the following judgements: (a) There is a 95% chance that an attempted burglary will trigger the alarm system \u2013 P(Alarm|Burglary) = 0.95; (b) based on previous false alarms, there is a slight (1 percent) chance that the alarm will be triggered by a mechanism other than an attempted burglary \u2013 P(Alarm|No Burglary) = 0.01; (c) previous crime patterns indicate that there is a one in ten thousand chance that a given house will be burglarized on a given night \u2013 P(Burglary) = 10^-4. This can be modelled in Hakaru by the program: burglary <~ categorical([0.0001, 0.9999]) weight([0.95, 0.01][burglary], return [true,false][burglary]) If you save this program as weight_burglary.hk , you can generate samples from it by calling: $ hakaru weight_burglary.hk 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false ... The hakaru command will print a continuous stream of samples drawn from this program. In this example, true and false samples have different weights. This will not be immediately apparent if you manually sift through the samples. If you wanted to see the ratio of weights for a series of samples, you can use an awk script that tallies the weights for a limited set of samples: $ hakaru weight_burglary.hk | head -n 100000 | awk '{a[$2]+=$1}END{for (i in a) print i, a[i]}' false 999.87 true 12.35 If you were only interested in counting how many times the alarm was triggered correctly and erroneously, modify the awk script to be a counter instead: $ hakaru weight_burglary.hk | head -n 100000 | awk '{a[$2]+=1}END{for (i in a) print i, a[i]}' false 99987 true 13 In this case, the printing of sample weights might not be important. To suppress the printing of weights during sample generation, you can use the --no-weights or -w option: $ hakaru --no-weights weight_burglary.hk false false false false false false false false ... Hakaru can sample from more complex distributions using the Metropolis Hastings transform. The hakaru command can then be invoked using a transition kernel. For an example of the hakaru command usage in this context, refer to the Metropolis Hastings transform page. D.J.C. MacKay, \u201cIntroduction to Monte Carlo Methods\u201d, Learning in Graphical Models, vol. 89, pp. 175-204, 1998. \u21a9 J. Pearl, Probabilistic reasoning in intelligent systems: Networks of plausible inference. San Francisco: M. Kaufmann, 1988. \u21a9","title":"Generating Samples from your Hakaru Program"},{"location":"intro/samplegen/#generating-samples-from-your-hakaru-program","text":"The Hakaru language is designed so that it is easy to express probabilistic models programmatically. In particular, Hakaru makes makes it easy to use Monte Carlo methods, which aim to generate individual samples and estimate expectation functions, or expectation, from a given distribution. The first task, drawing samples from a distribution, is often difficult because it might not be possible to sample from the target distribution directly. This can be exasperated as the dimensionality of the sample space increases. In these scenarios, a comparable distribution can be selected to draw samples from. Importance sampling is a Monte Carlo method that is used to generate samples by estimating an expectation for the target distribution instead. The estimated expectation is then used to generate samples. To account for the knowledge that the samples were not generated from the target distribution, a weight is assigned so that each sample\u2019s contribution to the estimator is adjusted according to its relevance. However, this method only works well if the distribution proposed by the expectation is similar to the target distribution. For more complex distributions, a different approach, such as the Metropolis Hastings method should be used 1 . The hakaru command is used to indefinitely generate samples from a Hakaru program using importance sampling. Each sample is assigned a weight, and a sample\u2019s weight is initialized to 1.0 . Weights are changed by Hakaru primitives and processes such as weight .","title":"Generating Samples from your Hakaru Program"},{"location":"intro/samplegen/#usage","text":"The hakaru command can take up to two Hakaru programs as arguments. If only one program is provided, the hakaru command generates samples based on the model described in the Hakaru program. In this case, the hakaru command can be invoked in the command-line by calling: hakaru hakaru_program.hk If a second program is given to the hakaru command, it will treat the two programs as the start of a Markov Chain. This is used when you have created a transition kernel using the Metropolis Hastings transformation. To invoke the hakaru command with a transition kernel, you would call: hakaru --transition-kernel transition.hk init.hk The first program, transition.hk , is treated as the transition kernel and the second program, init.hk , is treated as the initial state of the Markov Chain. When the hakaru command is run, a sample is drawn from init.hk . This sample is then passed to transition.hk to generate the second sample. After this point, samples generated from transition.hk are passed back into itself to generate further samples.","title":"Usage"},{"location":"intro/samplegen/#the-dash-operator","text":"You might encounter some scenarios where you wish to run a Hakaru command or transformation on a program and then send the resulting output to another command or transform. In these cases, you can take advantage of the dash ( - ) command-line notation. The dash notation is a shortcut used to pass standard inputs and outputs to another command in the same line of script. For example, if you wanted to run the disintegrate Hakaru command followed by the hk-maple -c Simplify command, you would enter: disintegrate program.hk | hk-maple -c Simplify - This command is equivalent to entering: disintegrate program.hk > temp.hk hk-maple -c Simplify temp.hk Note: The > operator redirects the output from disintegrate program.hk to a new file called temp.hk .","title":"The Dash (-) Operator"},{"location":"intro/samplegen/#example","text":"To demonstrate weights in Hakaru, a sample problem of a burglary alarm is adapted from Pearl\u2019s textbook on probabilistic reasoning (page 35) 2 : Imagine being awakened one night by the shrill sound of your burglar alarm. What is your degree of belief that a burglary attempt has taken place? For illustrative purposes we make the following judgements: (a) There is a 95% chance that an attempted burglary will trigger the alarm system \u2013 P(Alarm|Burglary) = 0.95; (b) based on previous false alarms, there is a slight (1 percent) chance that the alarm will be triggered by a mechanism other than an attempted burglary \u2013 P(Alarm|No Burglary) = 0.01; (c) previous crime patterns indicate that there is a one in ten thousand chance that a given house will be burglarized on a given night \u2013 P(Burglary) = 10^-4. This can be modelled in Hakaru by the program: burglary <~ categorical([0.0001, 0.9999]) weight([0.95, 0.01][burglary], return [true,false][burglary]) If you save this program as weight_burglary.hk , you can generate samples from it by calling: $ hakaru weight_burglary.hk 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false 1.0000000000000004e-2 false ... The hakaru command will print a continuous stream of samples drawn from this program. In this example, true and false samples have different weights. This will not be immediately apparent if you manually sift through the samples. If you wanted to see the ratio of weights for a series of samples, you can use an awk script that tallies the weights for a limited set of samples: $ hakaru weight_burglary.hk | head -n 100000 | awk '{a[$2]+=$1}END{for (i in a) print i, a[i]}' false 999.87 true 12.35 If you were only interested in counting how many times the alarm was triggered correctly and erroneously, modify the awk script to be a counter instead: $ hakaru weight_burglary.hk | head -n 100000 | awk '{a[$2]+=1}END{for (i in a) print i, a[i]}' false 99987 true 13 In this case, the printing of sample weights might not be important. To suppress the printing of weights during sample generation, you can use the --no-weights or -w option: $ hakaru --no-weights weight_burglary.hk false false false false false false false false ... Hakaru can sample from more complex distributions using the Metropolis Hastings transform. The hakaru command can then be invoked using a transition kernel. For an example of the hakaru command usage in this context, refer to the Metropolis Hastings transform page. D.J.C. MacKay, \u201cIntroduction to Monte Carlo Methods\u201d, Learning in Graphical Models, vol. 89, pp. 175-204, 1998. \u21a9 J. Pearl, Probabilistic reasoning in intelligent systems: Networks of plausible inference. San Francisco: M. Kaufmann, 1988. \u21a9","title":"Example"},{"location":"lang/arrays/","text":"Arrays and Plate Hakaru provides special syntax for arrays, which is distinct from the other data types. Arrays To construct arrays, we provide an index variable, size argument, and an expression body. This body is evaluated for each index of the array. For example, to construct the array [0,1,2,3] : array i of 4: i Array Literals We can also create arrays using the literal syntax a comma delimited list surrounded by brackets: [0,1,2,3] Array size and indexing If a is an array, then size(a) is its number of elements, which is a nat . If i is a nat then a[i] is the element of a at index i . Indices start at zero, so the maximum valid value of i is size(a)-1 . Plate Beyond, arrays Hakaru includes special syntax for describing measures over arrays called plate . Plate using the same syntax as array but the body must have a measure type. It returns a measure over arrays. For example, if we wish to have a distribution over three independent normal distributions we would do so as follows: plate _ of 3: normal(0,1)","title":"Arrays and Plate"},{"location":"lang/arrays/#arrays-and-plate","text":"Hakaru provides special syntax for arrays, which is distinct from the other data types.","title":"Arrays and Plate"},{"location":"lang/arrays/#arrays","text":"To construct arrays, we provide an index variable, size argument, and an expression body. This body is evaluated for each index of the array. For example, to construct the array [0,1,2,3] : array i of 4: i","title":"Arrays"},{"location":"lang/arrays/#array-literals","text":"We can also create arrays using the literal syntax a comma delimited list surrounded by brackets: [0,1,2,3]","title":"Array Literals"},{"location":"lang/arrays/#array-size-and-indexing","text":"If a is an array, then size(a) is its number of elements, which is a nat . If i is a nat then a[i] is the element of a at index i . Indices start at zero, so the maximum valid value of i is size(a)-1 .","title":"Array size and indexing"},{"location":"lang/arrays/#plate","text":"Beyond, arrays Hakaru includes special syntax for describing measures over arrays called plate . Plate using the same syntax as array but the body must have a measure type. It returns a measure over arrays. For example, if we wish to have a distribution over three independent normal distributions we would do so as follows: plate _ of 3: normal(0,1)","title":"Plate"},{"location":"lang/coercions/","text":"Types and Coercions Hakaru is a simply-typed language which has a few basic types and some more complicated ones which can be built out of simpler types. Types nat is the type for natural numbers. This includes zero. int is the integer type. prob is the type for positive real number. This includes zero. real is the type for real numbers. array(x) is the type for arrays where each element is type x measure(x) is the type for probability distributions whose sample space is type x Coercions For the primitive numeric types we also offer coercion functions. prob2real int2real nat2int real2prob real2int int2nat For the ones which are always safe to apply such as nat2int we will automatically insert them if it is required for the program to typecheck.","title":"Types and Coercions"},{"location":"lang/coercions/#types-and-coercions","text":"Hakaru is a simply-typed language which has a few basic types and some more complicated ones which can be built out of simpler types.","title":"Types and Coercions"},{"location":"lang/coercions/#types","text":"nat is the type for natural numbers. This includes zero. int is the integer type. prob is the type for positive real number. This includes zero. real is the type for real numbers. array(x) is the type for arrays where each element is type x measure(x) is the type for probability distributions whose sample space is type x","title":"Types"},{"location":"lang/coercions/#coercions","text":"For the primitive numeric types we also offer coercion functions. prob2real int2real nat2int real2prob real2int int2nat For the ones which are always safe to apply such as nat2int we will automatically insert them if it is required for the program to typecheck.","title":"Coercions"},{"location":"lang/cond/","text":"Conditionals Hakaru supports an if expression. This if must have two bodies. There exists no special syntax for else if like you might find in Python. a = 4 b = 5 if a > b: a + 1 else: b - 2","title":"Conditionals"},{"location":"lang/cond/#conditionals","text":"Hakaru supports an if expression. This if must have two bodies. There exists no special syntax for else if like you might find in Python. a = 4 b = 5 if a > b: a + 1 else: b - 2","title":"Conditionals"},{"location":"lang/datatypes/","text":"Data Types and Match Hakaru with several built-in data types. pair unit either bool Match We use match to deconstruct out data types and access their elements. match left(3). either(int,bool): left(x) : 1 right(x): 2 We do include special syntax for pairs match (1,2): (x,y): x + y","title":"Data Types and Match"},{"location":"lang/datatypes/#data-types-and-match","text":"Hakaru with several built-in data types. pair unit either bool","title":"Data Types and Match"},{"location":"lang/datatypes/#match","text":"We use match to deconstruct out data types and access their elements. match left(3). either(int,bool): left(x) : 1 right(x): 2 We do include special syntax for pairs match (1,2): (x,y): x + y","title":"Match"},{"location":"lang/functions/","text":"Functions Functions can be defined using a Python-inspired style syntax. One notable difference is that each argument must be followed by its type. def add(x real, y real): x + y add(4,5) We may optionally provide a type for the return value of a function if we wish. def add(x real, y real) real: x + y add(4,5) Anonymous functions If you don\u2019t wish to name your functions, we also offer a syntax for anonymous functions. These only take on argument and must be given a type alongside the variable name. fn x real: x + 1 Internally, there are only one argument anonymous functions, and lets. The first example is equivalent to the following. add = fn x real: fn y real: x + y add(4,5)","title":"Functions"},{"location":"lang/functions/#functions","text":"Functions can be defined using a Python-inspired style syntax. One notable difference is that each argument must be followed by its type. def add(x real, y real): x + y add(4,5) We may optionally provide a type for the return value of a function if we wish. def add(x real, y real) real: x + y add(4,5)","title":"Functions"},{"location":"lang/functions/#anonymous-functions","text":"If you don\u2019t wish to name your functions, we also offer a syntax for anonymous functions. These only take on argument and must be given a type alongside the variable name. fn x real: x + 1 Internally, there are only one argument anonymous functions, and lets. The first example is equivalent to the following. add = fn x real: fn y real: x + y add(4,5)","title":"Anonymous functions"},{"location":"lang/letbind/","text":"Let and Bind In Hakaru, we can give names for expressions to our programs with = , which we call Let . This gives us the ability to share computation that might be needed in the program. x = 2 x + 3 We can use = to give a name to any expression in our language. The name you assign is in scope for the rest of the body it was defined in. Bind Hakaru also has the operator <~ . This operator, which call Bind can only be used with expressions that denote probability distributions. Bind allows us to talk about draws from a distribution using a name for any particular value that could have come from that distribution. # Bad x <~ 2 + 3 x # Good x <~ normal(0,1) return x Because Bind is about draws from a distribution, the rest of the body must also denote a probability distribution. # Bad x <~ normal(0,1) x # Good x <~ normal(0,1) return x To help distinguish Let and Bind. Here is a probabilistic program, where we let f be equal to the normal distribution, and take draws from f . f = normal(0,1) x <~ f return x*x","title":"Let and Bind"},{"location":"lang/letbind/#let-and-bind","text":"In Hakaru, we can give names for expressions to our programs with = , which we call Let . This gives us the ability to share computation that might be needed in the program. x = 2 x + 3 We can use = to give a name to any expression in our language. The name you assign is in scope for the rest of the body it was defined in.","title":"Let and Bind"},{"location":"lang/letbind/#bind","text":"Hakaru also has the operator <~ . This operator, which call Bind can only be used with expressions that denote probability distributions. Bind allows us to talk about draws from a distribution using a name for any particular value that could have come from that distribution. # Bad x <~ 2 + 3 x # Good x <~ normal(0,1) return x Because Bind is about draws from a distribution, the rest of the body must also denote a probability distribution. # Bad x <~ normal(0,1) x # Good x <~ normal(0,1) return x To help distinguish Let and Bind. Here is a probabilistic program, where we let f be equal to the normal distribution, and take draws from f . f = normal(0,1) x <~ f return x*x","title":"Bind"},{"location":"lang/loops/","text":"Loops We also express loops that compute sums ( summate ) and products ( product ). The syntax of these loops begins by declaring an inclusive lower bound and an exclusive upper bound. For example, the factorial of n is not product i from 1 to n: i but rather product i from 1 to n+1: i . This convention takes some getting used to but it makes it easy to deal with arrays. For example, if a is an array of numbers then their sum is summate i from 0 to size(a): a[i] .","title":"Loops"},{"location":"lang/loops/#loops","text":"We also express loops that compute sums ( summate ) and products ( product ). The syntax of these loops begins by declaring an inclusive lower bound and an exclusive upper bound. For example, the factorial of n is not product i from 1 to n: i but rather product i from 1 to n+1: i . This convention takes some getting used to but it makes it easy to deal with arrays. For example, if a is an array of numbers then their sum is summate i from 0 to size(a): a[i] .","title":"Loops"},{"location":"lang/rand/","text":"Primitive Probability Distributions Hakaru comes with a small set of primitive probability distributions. normal(mean. real , standard_deviation. prob ): measure(real) univariate Normal (Gaussian) distribution - uniform(low. real , high. real ): measure(real) Uniform distribution is a continuous univariate distribution defined from low to high - gamma(shape. prob , scale. prob ): measure(prob) Gamma distribution with shape and scale parameterization - beta(a. prob , b. prob ): measure(prob) Beta distribution - poisson(l. prob ): measure(nat) Poisson distribution - categorical(v. array(prob) ): measure(nat) Categorical distribution - dirac(x. a ): measure(a) Dirac distribution - The Dirac distribution appears often enough, that we have given an additional keyword in our language for it: return . The following programs are equivalent. dirac(3) return 3 lebesgue(low. real , high. real ): measure(real) the distribution constant between low and high and zero elsewhere. high must be at least low . - weight(x. prob , m. measure(a) ): measure(a) a m distribution, reweighted by x - reject: measure(a) The distribution over the empty set - Finally, we have a binary choice operator <|> , which takes two distributions, and returns an unnormalized distribution which returns one or the other. For example, to get a distribution which where with probability 0.5 draws from a uniform(0,1), and probability 0.5 draws from uniform(5,6). weight(0.5, uniform(0,1)) <|> weight(0.5, uniform(5,6))","title":"Primitive Probability Distributions"},{"location":"lang/rand/#primitive-probability-distributions","text":"Hakaru comes with a small set of primitive probability distributions.","title":"Primitive Probability Distributions"},{"location":"transforms/compile/","text":"Compiling to Haskell Hakaru can be compiled to Haskell using the compile command. For example if we wish to compile example.hk x <~ normal(0,1) y <~ normal(x,1) return y We call compile example.hk , which produces a file example.hs . cat example.hs {-# LANGUAGE DataKinds, NegativeLiterals #-} module Main where import Prelude hiding (product) import Language.Hakaru.Runtime.Prelude import Language.Hakaru.Types.Sing import qualified System.Random.MWC as MWC import Control.Monad prog = normal (nat2real (nat_ 0)) (nat2prob (nat_ 1)) >>= \\ x0 -> normal x0 (nat2prob (nat_ 1)) >>= \\ y1 -> dirac y1 main :: IO () main = do g <- MWC.createSystemRandom forever $ run g prog This is a regular Haskell file, which can then be furthered compiled into machine code.","title":"Compiling to Haskell"},{"location":"transforms/compile/#compiling-to-haskell","text":"Hakaru can be compiled to Haskell using the compile command. For example if we wish to compile example.hk x <~ normal(0,1) y <~ normal(x,1) return y We call compile example.hk , which produces a file example.hs . cat example.hs {-# LANGUAGE DataKinds, NegativeLiterals #-} module Main where import Prelude hiding (product) import Language.Hakaru.Runtime.Prelude import Language.Hakaru.Types.Sing import qualified System.Random.MWC as MWC import Control.Monad prog = normal (nat2real (nat_ 0)) (nat2prob (nat_ 1)) >>= \\ x0 -> normal x0 (nat2prob (nat_ 1)) >>= \\ y1 -> dirac y1 main :: IO () main = do g <- MWC.createSystemRandom forever $ run g prog This is a regular Haskell file, which can then be furthered compiled into machine code.","title":"Compiling to Haskell"},{"location":"transforms/density/","text":"Density The density transform ( density ) finds the density of a probability distribution at a particular point. This transform is a specialized form of the disintegrate transform that also computes the expectation of the probabilistic distribution as part of its work. Usage Your Hakaru program must be a probability distribution (type measure(x) ) in order to use the density transform. Most Hakaru programs that end with a return statement do not meet this requirement because they return values instead of functions on values. You can use the density transform in the command line by calling: density hakaru_program.hk This transformation will produce a new Hakaru program containing an anonymous function representing the density function for that model. Example The normal distribution is a commonly used distribution in probabilistic modeling. A simple Hakaru program modelling the simplest normal distribution is: normal(0,1) Assuming that this program is named norm.hk , we can calculate the density function for this distribution by running: density norm.hk Note: The output for density will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling density model1.hk > model2.hk . For this example, we will call our new program norm_density.hk . When you open the new Hakaru program, norm_density.hk , you will find an anonymous Hakaru function. You can then assign values to the function\u2019s arguments to test the probabilistic density at a specific point in the model. fn x0 real: (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1) For example, if you wanted to test the density at 0.2 , you could alter norm_density.hk to: normalDensity = fn x0 real: (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1) return normalDensity(0.2) You could then use the hakaru command to have Hakaru compute the density: $ hakaru norm_density.hk | head -n 1 0.3910426939754559 Note: If the argument head -n 1 is omitted, the hakaru command will print out the resulting density value indefinitely.","title":"Density"},{"location":"transforms/density/#density","text":"The density transform ( density ) finds the density of a probability distribution at a particular point. This transform is a specialized form of the disintegrate transform that also computes the expectation of the probabilistic distribution as part of its work.","title":"Density"},{"location":"transforms/density/#usage","text":"Your Hakaru program must be a probability distribution (type measure(x) ) in order to use the density transform. Most Hakaru programs that end with a return statement do not meet this requirement because they return values instead of functions on values. You can use the density transform in the command line by calling: density hakaru_program.hk This transformation will produce a new Hakaru program containing an anonymous function representing the density function for that model.","title":"Usage"},{"location":"transforms/density/#example","text":"The normal distribution is a commonly used distribution in probabilistic modeling. A simple Hakaru program modelling the simplest normal distribution is: normal(0,1) Assuming that this program is named norm.hk , we can calculate the density function for this distribution by running: density norm.hk Note: The output for density will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling density model1.hk > model2.hk . For this example, we will call our new program norm_density.hk . When you open the new Hakaru program, norm_density.hk , you will find an anonymous Hakaru function. You can then assign values to the function\u2019s arguments to test the probabilistic density at a specific point in the model. fn x0 real: (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1) For example, if you wanted to test the density at 0.2 , you could alter norm_density.hk to: normalDensity = fn x0 real: (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1) return normalDensity(0.2) You could then use the hakaru command to have Hakaru compute the density: $ hakaru norm_density.hk | head -n 1 0.3910426939754559 Note: If the argument head -n 1 is omitted, the hakaru command will print out the resulting density value indefinitely.","title":"Example"},{"location":"transforms/disintegrate/","text":"Disintegrate The disintegrate transformation converts a Hakaru program representing a joint probability distribution into a Hakaru program representing a posterior distribution for a target distribution variable. This transform is equivalent to model conditioning in probability theory, where the known data is provided to the transformed Hakaru model. Note: The disintegrate transform cannot be used to condition variables of type bool or expressions containing Boolean operators. Usage Before you use the disintegrate transform, your Hakaru program should contain a return statement containing the variables for your known and unknown data. The order of the variables in the return statement is important. The variable for the known data should appear first, followed by the variable representing the unknown data. You can use the disintegrate transform in the command line by calling: disintegrate hakaru_program.hk This command will return a new Hakaru program that contains an anonymous function representing the transformed program. The function argument represents the variable for which you will test different values for your unknown variable. Example Let\u2019s condition a joint probability distribution of two independent random variables that are each drawn from a normal distribution. You can define this model in Hakaru using a program such as: y <~ normal(0,1) x <~ normal(\u03b8,1) return (y,x) In this program, x and y are the independent variables. The statement return (y,x) states that you want to condition your model to create a posterior model for x using known values for y . If you save this program as hello1.hk , you would call the disintegrate transform on it by running: disintegrate hello1.hk Note: The output for disintegrate will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling disintegrate model1.hk > model2.hk . For this example, we will call our new program hello1_D.hk . The resulting program renames the known-value variable y (here it is renamed to x2 ) and creates an anonymous function that, given a value for y , calculates the corresponding value for x : fn x2 real: x <~ normal(0, 1) x7 <~ weight((exp((negate(((x2 - x) ^ 2)) / 2)) / 1 / sqrt((2 * pi))), return ()) return x","title":"Disintegrate"},{"location":"transforms/disintegrate/#disintegrate","text":"The disintegrate transformation converts a Hakaru program representing a joint probability distribution into a Hakaru program representing a posterior distribution for a target distribution variable. This transform is equivalent to model conditioning in probability theory, where the known data is provided to the transformed Hakaru model. Note: The disintegrate transform cannot be used to condition variables of type bool or expressions containing Boolean operators.","title":"Disintegrate"},{"location":"transforms/disintegrate/#usage","text":"Before you use the disintegrate transform, your Hakaru program should contain a return statement containing the variables for your known and unknown data. The order of the variables in the return statement is important. The variable for the known data should appear first, followed by the variable representing the unknown data. You can use the disintegrate transform in the command line by calling: disintegrate hakaru_program.hk This command will return a new Hakaru program that contains an anonymous function representing the transformed program. The function argument represents the variable for which you will test different values for your unknown variable.","title":"Usage"},{"location":"transforms/disintegrate/#example","text":"Let\u2019s condition a joint probability distribution of two independent random variables that are each drawn from a normal distribution. You can define this model in Hakaru using a program such as: y <~ normal(0,1) x <~ normal(\u03b8,1) return (y,x) In this program, x and y are the independent variables. The statement return (y,x) states that you want to condition your model to create a posterior model for x using known values for y . If you save this program as hello1.hk , you would call the disintegrate transform on it by running: disintegrate hello1.hk Note: The output for disintegrate will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling disintegrate model1.hk > model2.hk . For this example, we will call our new program hello1_D.hk . The resulting program renames the known-value variable y (here it is renamed to x2 ) and creates an anonymous function that, given a value for y , calculates the corresponding value for x : fn x2 real: x <~ normal(0, 1) x7 <~ weight((exp((negate(((x2 - x) ^ 2)) / 2)) / 1 / sqrt((2 * pi))), return ()) return x","title":"Example"},{"location":"transforms/expect/","text":"Expect The expectation transformation takes a program representing a measure, and a function over the sample space, and returns a program computing the expectation over that measure with respect to the given function. Usage Expect can be used inside programs with the expect keyword. expect x <~ uniform(1,3): real2prob(2*x + 1) This program computes the expectation of uniform(1,3) using the function 2*x + 1 . This program expands to the following equivalent program: integrate x from 1 to 3: recip(real2prob(3 - 1)) * real2prob(2*x + 1) This can be optimized in turn by feeding it into the simplify transformation. It will in turn return 5 .","title":"Expect"},{"location":"transforms/expect/#expect","text":"The expectation transformation takes a program representing a measure, and a function over the sample space, and returns a program computing the expectation over that measure with respect to the given function.","title":"Expect"},{"location":"transforms/expect/#usage","text":"Expect can be used inside programs with the expect keyword. expect x <~ uniform(1,3): real2prob(2*x + 1) This program computes the expectation of uniform(1,3) using the function 2*x + 1 . This program expands to the following equivalent program: integrate x from 1 to 3: recip(real2prob(3 - 1)) * real2prob(2*x + 1) This can be optimized in turn by feeding it into the simplify transformation. It will in turn return 5 .","title":"Usage"},{"location":"transforms/hk-maple/","text":"Hakaru-Maple Hakaru uses the computer algebra system Maple to aid in performing program transformations. You can use this functionality of Hakaru if you have Maple installed locally or can access Maple remotely. Maple can be accessed through the module Language.Hakaru.Maple or through the Hakaru program hk-maple . The hk-maple command invokes a Maple command on a Hakaru program. Given a Hakaru program in concrete syntax and a Maple-Hakaru command, typecheck the program invoke the Maple command on the program and its type pretty print, parse and typecheck the program resulting from Maple. See the --help flag of hk-maple for more information. The currently available Maple-Hakaru commands (also called subcommands): Simplify Disintegrate Summarize Note: calls to Maple may take a very long time. To see if your program is taking an appreciable amount of time to parse and typecheck, use the --debug flag. Subcommands Simplify Hakaru programs are interpreted by Maple as linear operators. In this interpretation, many commonly understood (by Maple) and powerful tools for simplification become available. The metric for simplification as understood by this command is sampling efficiency. Simplify attempts to be as conservative as possible in changing the given program. In particular, it should not change terms unless an improvement with respect to sampling is performed; in this case, arbitrary rearrangement may happen, even if an expression more similair to the original could be produced. Simplify is the default subcommand. Simplify preserves the semantics of the given program up to normalization of weights. If the stronger sense of equivalence is needed, the output of Simplify can be passed to normalize . Historical note: the Simplify subcommand of hk-maple used to be known as a separate command named simplify . If you encounter simplify someprog.hk <options> in any documentation, you may replace it by hk-maple -c Simplify someprog.hk <options> . Disintegrate The Maple disintegrator is an alternative implementation of the program transformation described in Disintegrate . Semantically, the Maple disintegrator and Haskell disintegrator implement the same transformation. In particular, their outputs are not (often) identical, but have equivalent sampling semantics. In practice, the ouputs may differ, since one may fail where the other succeeds. If in doubt about which disintegrator to use, consider the following order: disintegrate x disintegrate x | hk-maple - hk-maple --command disintegrate x hk-maple x | disintegrate - etc\u2026 All of the above programs should be equivalent as samplers. The disintegrator internally relies heavily on the Simplify command, so if the given problem is an easy disintegration problem but a difficult simplification problem, it is preferred to use the Haskell disintegrator followed by a call to Simplify . The chance that the Maple disintegrator produces a good program (or any program at all) is proportional to the type of program it is given. In addition to programs whose disintegration by Haskell is not efficient as a sampler, the following programs are good candidates: programs which contain superpositions with complicated conditions programs which contain complicated rational polynomials The Maple disintegrator follows the same conventions as the Haskell disintegrator. Like Simplify , Disintegrate preserves the semantics of the given program only up to normalization of weights. Summarize Recall that our simplifier generates sampler code such as fn as array(prob): fn z array(nat): fn t array(real): fn docUpdate nat: array zNew of size(as): ... (summate _b from 0 to size(as): ... (summate i from 0 to size(t): if _b == (if i == docUpdate: zNew else: z[i]): t[i] else: 0) ...) ... (that\u2019s from examples/gmm_gibbs.hk ) and fn topic_prior array(prob): fn word_prior array(prob): fn z array(nat): fn w array(nat): fn doc array(nat): fn docUpdate nat: ... (array zNew of size(topic_prior): product k from 0 to size(topic_prior): product i from 0 to size(word_prior): ... (summate j from 0 to size(w): if doc[j] == docUpdate: if k == zNew && i == w[j]: 1 else: 0 else: 0) ...) ... (that\u2019s from examples/naive_bayes_gibbs.hk ). In this Hakaru code, fn makes a function (whose argument type is explicit), and array , summate , and product are looping constructs. In particular, the innermost loops ( summate i and summate j ) run many times due to the outer loops. Furthermore, most iterations of the innermost loops produce 0 and so don\u2019t contribute to the result. We can dramatically speed up this computation by precomputing a \u201csummary\u201d outside the loops then replacing the innermost loop by an expression that reuses the summary rather than looping. To understand our current approach to this optimization, let\u2019s look at a simpler example. Suppose the innermost loop is merely summate i from 0 to size(t): if _b == z[i]: t[i] else: 0 where the free variable _b denotes a natural number known to be bounded by size(as) . This loop denotes a real number that depends on _b and the arrays z and t . It turns out that we can rewrite it to the equivalent expression let summary = Bucket i from 0 to size(t): Index _b = z[i] of size(as): Add t[i] in summary[_b] where the capitalized keywords are newly introduced to support this optimization. The variable summary is bound to an array whose size is size(as) and whose element at each index _b is the sum of those t[i] whose corresponding z[i] matches _b . A good way to compute the summary on sequential hardware is to initialize the summary to an all-zero mutable array then for i from 0 to size(t): summary[z[i]] += t[i] A good way to compute the summary on parallel hardware is to divide the data among the cores and summarize each portion in parallel then sum the summaries elementwise. The Bucket construct just introduced can carry out either of these implementation strategies, by accordingly interpreting the sub-language of map-reduce loops that Index and Add are part of. (Hence realizing this optimization from end to end in Hakaru calls for adding the capitalized constructs to the Hakaru grammar and extending the code generator(s) to handle them.) Out of context, the let-expression above seems like a waste because it computes a summary then uses only one element of it. But the right-hand-side of the summary-binding does not contain _b free, only t and z and as , because the occurrences of i and _b are not uses but bindings. So, as an instance of loop-invariant code motion, we can move the let-binding out of the loop over _b , and reuse the same summary across all iterations over _b . This way, we cut time complexity by a factor of size(as) (the number of iterations over _b ). To pave the way for loop-invariant code motion, the summary should depend on as few inner-scoped variables as possible. This goal is illustrated by the very first example above (from gmm_gibbs.hk ). Following the footsteps of the let-expression above, the summate i expression is equivalent to let summary = Bucket i from 0 to size(t): Index _b = (if i == docUpdate: zNew else: z[i]) of size(as): Add t[i] in summary[_b] but this summary depends on zNew (and on docUpdate). Our prototype implementation of summarization finds a better rewrite: let summary = Bucket i from 0 to size(t): Split i == docUpdate: Fanout(Add t[i], Nop) else: Index _b = z[i] of size(as): Add t[i] in (if _b == zNew: fst(fst(summary)) else: 0) + snd(summary)[_b] The type of this summary is not array(real) but pair(pair(real,unit), array(real)) . The array(real) summarizes those i that are not equal to docUpdate , whereas the pair(real,unit) turns out to be equal to (t[docUpdate],()) . The check _b == zNew is postponed until the body of the let, so the summary does not depend on zNew and can be moved out of the array zNew loop, reducing time complexity by another factor of size(as) . Another way to understand this optimization is that we use loop exchange to move the innermost loop out, then sum sparse arrays inside. At least for the examples above, it turns out to be not very difficult to automate rewriting a summate loop into a let-expression of the form let summary = Bucket i... in ...summary... whose body does not loop over i . The automatic rewriting traverses the body of the original loop, using a set of rewriting rules \u2013 one for each of Index , Add , Split , Fanout , and Nop \u2013 that are easy to pretend for expository purposes to have been derived by equational reasoning from the monoidal denotational semantics of the newly introduced constructs. The trick is as usual to design the map-reduce sub-language right. The design notes below provide some details, couched somewhat in Maple syntax. The summarization optimization can be accessed with hk-maple -c Summarize , which only calls Summarize on the input program, and with summary , which calls Summarize as well as generating Haskell code corresponding to the summarized program. The summary command has its own options regarding the generation of Haskell code; see summary --help for details. The language of mapreductions: A mapreduction mr denotes a monoid along with a map from indices (i) to monoid elements (such as e being t[i]). There is an implicit index \u2018i\u2019, allowed to appear in e and cond. mr ::= Fanout(mr,mr) | Index(n,o,e,mr) | Split(cond,mr,mr) | Nop() | Add(e) Nop() denotes the trivial monoid along with the constant map. Add(e) denotes the monoid of numbers under addition, along with the map i->e. Fanout(mr1,mr2) and Split(cond,mr1,mr2) both denote the product monoid of the monoids denoted by mr1 and by mr2 . But if the maps denoted by mr1 and mr2 are map1 and map2 then Fanout(mr1,mr2) denotes the map i->[map1(i),map2(i)] whereas Split(cond,mr1,mr2) denotes the map i->piecewise(cond,[map1(i),identity],[identity,map2(i)]) . Index(n,o,e,mr(o)) denotes the product monoid of the monoids denoted by mr(0)..mr(n-1) , along with a map that returns a tuple that is all identity except at o = e . Run-time helpers: Bucket(mr, i=rng) = summary := Init(mr); for iv=rng do Accum(i, iv, mr, summary) end do; return summary Init(Fanout(mr1, mr2)) = [Init(mr1), Init(mr2)] Init(Index(n, o, e, mr)) = [seq(Init(mr), o=0..n-1)] Init(Split(cond, mr1, mr2)) = [Init(mr1), Init(mr2)] Init(Nop()) = [] Init(Add(e)) = 0 Accum(i, iv, Fanout(mr1, mr2), summary) = Accum(i, iv, mr1, summary[1]); Accum(i, iv, mr2, summary[2]); Accum(i, iv, Index(n, o, e, mr), summary) = ov := eval(e, i=iv); if o::nonnegint and o<n then Accum(i, iv, mr, summary[ov+1]) end if; Accum(i, iv, Split(cond, mr1, mr2), summary) = if eval(cond, i=iv) then Accum(i, iv, mr1, summary[1]) else Accum(i, iv, mr2, summary[2]) end if; Accum(i, iv, Nop(), summary) = ; Accum(i, iv, Add(e), summary) = summary += eval(e, i=iv); Specification: If [mr, f] = summarize(e, kb, i) then f(Bucket(mr, i=rng)) = sum(e, i=rng) and we try to make mr depend on as little as possible Implementation: summarize(C[piecewise(cond,a,b)], kb, i) = [Fanout(mr1, mr2), summary -> piecewise(cond, f1(summary[1]), f2(summary[2]))] where [mr1, f1] = summarize(C[a], kb, i) [mr2, f2] = summarize(C[b], kb, i) if not depends(cond, i) Choose between the two rules below by outermosting the variables in indets(e,name) minus {i} versus indets(cond,name) minus {i} summarize(piecewise(o=e,a,0), kb, i) = [Index(n, o, e, mr), summary -> piecewise(o::nonnegint and o<n, f(summary[o+1]), 0)] where [mr, f] = summarize(a, kb, i) if not depends(o, i) and kb entails ((o::nonnegint and o<n) or (e::nonnegint and i<n)) summarize(C[piecewise(cond,a,b)], kb, i) = [Split(cond, mr1, mr2), summary -> f1(summary[1]) + f2(summary[2])] where [mr1, f1] = summarize(C[a], kb, i) [mr2, f2] = summarize(C[b], kb, i) summarize(0, kb, i) = [Nop(), summary -> 0] summarize(e, kb, i) = [Add(e), summary -> summary] Examples Simplify This program takes in a value of type prob and returns a measure of type real : fn a prob: x <~ normal(a,1) y <~ normal(x,1) z <~ normal(y,1) return z The returned value, z , is generated by passing the last value generated by the function, starting with the original function argument. This indicates that it might be reducible to a smaller program. Assuming that we named the program simplify_before.hk , we can call the Simplify transform by running: hk-maple simplify_before.hk Note: The output for Simplify will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling hk-maple model1.hk > model2.hk . For this example, we will call our new program simplify_after.hk . When you open our new program, simplify_after.hk , you will see that the original five-line program has been reduced to a single line: fn a prob: normal(prob2real(a), sqrt(3)) Disintegrate The following examples are simplified versions of the Borel-Kolmogorov paradox. Both programs choose x and y uniformly from the range 0..1 and return a pair consisting of a polynomial P(x,y) and the coordinates x and y themselves. $ pretty examples/borel-sub.hk x <~ uniform(nat2real(0), nat2real(1)) y <~ uniform(nat2real(0), nat2real(1)) return (y - nat2real(2) * x, (x, y)) $ pretty examples/borel-div.hk x <~ uniform(nat2real(0), nat2real(1)) y <~ uniform(nat2real(0), nat2real(1)) return (y / x, (x, y)) Disintegrating these two programs produces results which immediately reject values of P(x,y) which are impossible (e.g. y/x cannot be less than zero for x,y in [0,1] x [0,1] ). For remaining permitted values of x and y , y is parametrized in terms of x ; that is, the equation P(x,y)=0 is solved for x . $ hk-maple -c Disint examples/borel-sub.hk fn t1 real: if +1/1 < t1 || t1 < -2/1: reject. measure(pair(real, real)) else: if -2/1 < t1 && t1 < -1/1: weight (real2prob(+1/1 + t1 * (+1/2)), x9 <~ uniform(t1 * (-1/2), +1/1) return (x9, x9 * (+2/1) + t1)) else: if -1/1 < t1 && t1 < +0/1: weight (1/2, x9 <~ uniform(t1 * (-1/2), t1 * (-1/2) + (+1/2)) return (x9, x9 * (+2/1) + t1)) else: weight (real2prob(t1 * (-1/2) + (+1/2)), x9 <~ uniform(+0/1, t1 * (-1/2) + (+1/2)) return (x9, x9 * (+2/1) + t1)) $ hk-maple -c Disint borel-div.hk fn t1 real: if t1 < +0/1: reject. measure(pair(real, real)) else: if +1/1 < t1: weight (real2prob(1/ t1), x9 <~ uniform(+0/1, 1/ t1) weight(real2prob(x9), return (x9, x9 * t1))) else: weight (1/2, x9 <~ beta(2/1, 1/1) return (prob2real(x9), prob2real(x9) * t1)) Summarize These examples are given in Maple syntax as opposed to typical examples in Hakaru syntax. First \u201csummate i\u201d in offshore under gmm_gibbs.hk summarize(piecewise(_b=piecewise(i=docUpdate,zNew,z[i]),t[i],0), kb, i) = [Split(i=docUpdate, Fanout(Add(t[i]), Nop()), Index(size(as), _b, z[i], Add(t[i]))), summary -> piecewise(_b=zNew, summary[1][1], 0) + summary[2][_b+1]] Recursive call to summarize assuming i=docUpdate is true: summarize(piecewise(_b=zNew,t[i],0), kb, i) = [Fanout(Add(t[i]), Nop()), summary -> piecewise(_b=zNew, summary[1], 0)] Recursive call to summarize assuming i=docUpdate is false: summarize(piecewise(_b=z[i],t[i],0), kb, i) = [Index(size(as), _b, z[i], Add(t[i])), summary -> summary[_b+1]] summarize(t[i], kb, i) = [Add(t[i]), summary -> summary] First \u201csummate j\u201d in offshore under naive_bayes_gibbs.hk summarize(piecewise(doc[j]=docUpdate,piecewise(k=zNew,piecewise(i=w[j],1,0),0),0), kb, j) = [Fanout(Index(size(word_prior), i, w[j], Index(size(z), docUpdate, doc[j], Add(1))), Index(size(z), docUpdate, doc[j], Nop())), summary -> piecewise(k=zNew, summary[1][w[j]+1][docUpdate+1]], 0)] Recursive call to summarize assuming k=zNew is true: summarize(piecewise(doc[j]=docUpdate,piecewise(i=w[j],1,0),0), kb, j) = [Index(size(word_prior), i, w[j], Index(size(z), docUpdate, doc[j], Add(1))), summary -> summary[w[j]+1][docUpdate+1]] Recursive call to summarize assuming i=w[j]: summarize(piecewise(doc[j]=docUpdate,1,0), kb, j) = [Index(size(z), docUpdate, doc[j], Add(1)), summary -> summary[docUpdate+1]] Recursive call to summarize assuming k=zNew is false: summarize(piecewise(doc[j]=docUpdate,0,0), kb, j) = [Index(size(z), docUpdate, doc[j], Nop()), summary -> 0]","title":"Hakaru Maple"},{"location":"transforms/hk-maple/#hakaru-maple","text":"Hakaru uses the computer algebra system Maple to aid in performing program transformations. You can use this functionality of Hakaru if you have Maple installed locally or can access Maple remotely. Maple can be accessed through the module Language.Hakaru.Maple or through the Hakaru program hk-maple . The hk-maple command invokes a Maple command on a Hakaru program. Given a Hakaru program in concrete syntax and a Maple-Hakaru command, typecheck the program invoke the Maple command on the program and its type pretty print, parse and typecheck the program resulting from Maple. See the --help flag of hk-maple for more information. The currently available Maple-Hakaru commands (also called subcommands): Simplify Disintegrate Summarize Note: calls to Maple may take a very long time. To see if your program is taking an appreciable amount of time to parse and typecheck, use the --debug flag.","title":"Hakaru-Maple"},{"location":"transforms/hk-maple/#subcommands","text":"","title":"Subcommands"},{"location":"transforms/hk-maple/#simplify","text":"Hakaru programs are interpreted by Maple as linear operators. In this interpretation, many commonly understood (by Maple) and powerful tools for simplification become available. The metric for simplification as understood by this command is sampling efficiency. Simplify attempts to be as conservative as possible in changing the given program. In particular, it should not change terms unless an improvement with respect to sampling is performed; in this case, arbitrary rearrangement may happen, even if an expression more similair to the original could be produced. Simplify is the default subcommand. Simplify preserves the semantics of the given program up to normalization of weights. If the stronger sense of equivalence is needed, the output of Simplify can be passed to normalize . Historical note: the Simplify subcommand of hk-maple used to be known as a separate command named simplify . If you encounter simplify someprog.hk <options> in any documentation, you may replace it by hk-maple -c Simplify someprog.hk <options> .","title":"Simplify"},{"location":"transforms/hk-maple/#disintegrate","text":"The Maple disintegrator is an alternative implementation of the program transformation described in Disintegrate . Semantically, the Maple disintegrator and Haskell disintegrator implement the same transformation. In particular, their outputs are not (often) identical, but have equivalent sampling semantics. In practice, the ouputs may differ, since one may fail where the other succeeds. If in doubt about which disintegrator to use, consider the following order: disintegrate x disintegrate x | hk-maple - hk-maple --command disintegrate x hk-maple x | disintegrate - etc\u2026 All of the above programs should be equivalent as samplers. The disintegrator internally relies heavily on the Simplify command, so if the given problem is an easy disintegration problem but a difficult simplification problem, it is preferred to use the Haskell disintegrator followed by a call to Simplify . The chance that the Maple disintegrator produces a good program (or any program at all) is proportional to the type of program it is given. In addition to programs whose disintegration by Haskell is not efficient as a sampler, the following programs are good candidates: programs which contain superpositions with complicated conditions programs which contain complicated rational polynomials The Maple disintegrator follows the same conventions as the Haskell disintegrator. Like Simplify , Disintegrate preserves the semantics of the given program only up to normalization of weights.","title":"Disintegrate"},{"location":"transforms/hk-maple/#summarize","text":"Recall that our simplifier generates sampler code such as fn as array(prob): fn z array(nat): fn t array(real): fn docUpdate nat: array zNew of size(as): ... (summate _b from 0 to size(as): ... (summate i from 0 to size(t): if _b == (if i == docUpdate: zNew else: z[i]): t[i] else: 0) ...) ... (that\u2019s from examples/gmm_gibbs.hk ) and fn topic_prior array(prob): fn word_prior array(prob): fn z array(nat): fn w array(nat): fn doc array(nat): fn docUpdate nat: ... (array zNew of size(topic_prior): product k from 0 to size(topic_prior): product i from 0 to size(word_prior): ... (summate j from 0 to size(w): if doc[j] == docUpdate: if k == zNew && i == w[j]: 1 else: 0 else: 0) ...) ... (that\u2019s from examples/naive_bayes_gibbs.hk ). In this Hakaru code, fn makes a function (whose argument type is explicit), and array , summate , and product are looping constructs. In particular, the innermost loops ( summate i and summate j ) run many times due to the outer loops. Furthermore, most iterations of the innermost loops produce 0 and so don\u2019t contribute to the result. We can dramatically speed up this computation by precomputing a \u201csummary\u201d outside the loops then replacing the innermost loop by an expression that reuses the summary rather than looping. To understand our current approach to this optimization, let\u2019s look at a simpler example. Suppose the innermost loop is merely summate i from 0 to size(t): if _b == z[i]: t[i] else: 0 where the free variable _b denotes a natural number known to be bounded by size(as) . This loop denotes a real number that depends on _b and the arrays z and t . It turns out that we can rewrite it to the equivalent expression let summary = Bucket i from 0 to size(t): Index _b = z[i] of size(as): Add t[i] in summary[_b] where the capitalized keywords are newly introduced to support this optimization. The variable summary is bound to an array whose size is size(as) and whose element at each index _b is the sum of those t[i] whose corresponding z[i] matches _b . A good way to compute the summary on sequential hardware is to initialize the summary to an all-zero mutable array then for i from 0 to size(t): summary[z[i]] += t[i] A good way to compute the summary on parallel hardware is to divide the data among the cores and summarize each portion in parallel then sum the summaries elementwise. The Bucket construct just introduced can carry out either of these implementation strategies, by accordingly interpreting the sub-language of map-reduce loops that Index and Add are part of. (Hence realizing this optimization from end to end in Hakaru calls for adding the capitalized constructs to the Hakaru grammar and extending the code generator(s) to handle them.) Out of context, the let-expression above seems like a waste because it computes a summary then uses only one element of it. But the right-hand-side of the summary-binding does not contain _b free, only t and z and as , because the occurrences of i and _b are not uses but bindings. So, as an instance of loop-invariant code motion, we can move the let-binding out of the loop over _b , and reuse the same summary across all iterations over _b . This way, we cut time complexity by a factor of size(as) (the number of iterations over _b ). To pave the way for loop-invariant code motion, the summary should depend on as few inner-scoped variables as possible. This goal is illustrated by the very first example above (from gmm_gibbs.hk ). Following the footsteps of the let-expression above, the summate i expression is equivalent to let summary = Bucket i from 0 to size(t): Index _b = (if i == docUpdate: zNew else: z[i]) of size(as): Add t[i] in summary[_b] but this summary depends on zNew (and on docUpdate). Our prototype implementation of summarization finds a better rewrite: let summary = Bucket i from 0 to size(t): Split i == docUpdate: Fanout(Add t[i], Nop) else: Index _b = z[i] of size(as): Add t[i] in (if _b == zNew: fst(fst(summary)) else: 0) + snd(summary)[_b] The type of this summary is not array(real) but pair(pair(real,unit), array(real)) . The array(real) summarizes those i that are not equal to docUpdate , whereas the pair(real,unit) turns out to be equal to (t[docUpdate],()) . The check _b == zNew is postponed until the body of the let, so the summary does not depend on zNew and can be moved out of the array zNew loop, reducing time complexity by another factor of size(as) . Another way to understand this optimization is that we use loop exchange to move the innermost loop out, then sum sparse arrays inside. At least for the examples above, it turns out to be not very difficult to automate rewriting a summate loop into a let-expression of the form let summary = Bucket i... in ...summary... whose body does not loop over i . The automatic rewriting traverses the body of the original loop, using a set of rewriting rules \u2013 one for each of Index , Add , Split , Fanout , and Nop \u2013 that are easy to pretend for expository purposes to have been derived by equational reasoning from the monoidal denotational semantics of the newly introduced constructs. The trick is as usual to design the map-reduce sub-language right. The design notes below provide some details, couched somewhat in Maple syntax. The summarization optimization can be accessed with hk-maple -c Summarize , which only calls Summarize on the input program, and with summary , which calls Summarize as well as generating Haskell code corresponding to the summarized program. The summary command has its own options regarding the generation of Haskell code; see summary --help for details.","title":"Summarize"},{"location":"transforms/hk-maple/#the-language-of-mapreductions","text":"A mapreduction mr denotes a monoid along with a map from indices (i) to monoid elements (such as e being t[i]). There is an implicit index \u2018i\u2019, allowed to appear in e and cond. mr ::= Fanout(mr,mr) | Index(n,o,e,mr) | Split(cond,mr,mr) | Nop() | Add(e) Nop() denotes the trivial monoid along with the constant map. Add(e) denotes the monoid of numbers under addition, along with the map i->e. Fanout(mr1,mr2) and Split(cond,mr1,mr2) both denote the product monoid of the monoids denoted by mr1 and by mr2 . But if the maps denoted by mr1 and mr2 are map1 and map2 then Fanout(mr1,mr2) denotes the map i->[map1(i),map2(i)] whereas Split(cond,mr1,mr2) denotes the map i->piecewise(cond,[map1(i),identity],[identity,map2(i)]) . Index(n,o,e,mr(o)) denotes the product monoid of the monoids denoted by mr(0)..mr(n-1) , along with a map that returns a tuple that is all identity except at o = e . Run-time helpers: Bucket(mr, i=rng) = summary := Init(mr); for iv=rng do Accum(i, iv, mr, summary) end do; return summary Init(Fanout(mr1, mr2)) = [Init(mr1), Init(mr2)] Init(Index(n, o, e, mr)) = [seq(Init(mr), o=0..n-1)] Init(Split(cond, mr1, mr2)) = [Init(mr1), Init(mr2)] Init(Nop()) = [] Init(Add(e)) = 0 Accum(i, iv, Fanout(mr1, mr2), summary) = Accum(i, iv, mr1, summary[1]); Accum(i, iv, mr2, summary[2]); Accum(i, iv, Index(n, o, e, mr), summary) = ov := eval(e, i=iv); if o::nonnegint and o<n then Accum(i, iv, mr, summary[ov+1]) end if; Accum(i, iv, Split(cond, mr1, mr2), summary) = if eval(cond, i=iv) then Accum(i, iv, mr1, summary[1]) else Accum(i, iv, mr2, summary[2]) end if; Accum(i, iv, Nop(), summary) = ; Accum(i, iv, Add(e), summary) = summary += eval(e, i=iv); Specification: If [mr, f] = summarize(e, kb, i) then f(Bucket(mr, i=rng)) = sum(e, i=rng) and we try to make mr depend on as little as possible Implementation: summarize(C[piecewise(cond,a,b)], kb, i) = [Fanout(mr1, mr2), summary -> piecewise(cond, f1(summary[1]), f2(summary[2]))] where [mr1, f1] = summarize(C[a], kb, i) [mr2, f2] = summarize(C[b], kb, i) if not depends(cond, i) Choose between the two rules below by outermosting the variables in indets(e,name) minus {i} versus indets(cond,name) minus {i} summarize(piecewise(o=e,a,0), kb, i) = [Index(n, o, e, mr), summary -> piecewise(o::nonnegint and o<n, f(summary[o+1]), 0)] where [mr, f] = summarize(a, kb, i) if not depends(o, i) and kb entails ((o::nonnegint and o<n) or (e::nonnegint and i<n)) summarize(C[piecewise(cond,a,b)], kb, i) = [Split(cond, mr1, mr2), summary -> f1(summary[1]) + f2(summary[2])] where [mr1, f1] = summarize(C[a], kb, i) [mr2, f2] = summarize(C[b], kb, i) summarize(0, kb, i) = [Nop(), summary -> 0] summarize(e, kb, i) = [Add(e), summary -> summary]","title":"The language of mapreductions:"},{"location":"transforms/hk-maple/#examples","text":"","title":"Examples"},{"location":"transforms/hk-maple/#simplify_1","text":"This program takes in a value of type prob and returns a measure of type real : fn a prob: x <~ normal(a,1) y <~ normal(x,1) z <~ normal(y,1) return z The returned value, z , is generated by passing the last value generated by the function, starting with the original function argument. This indicates that it might be reducible to a smaller program. Assuming that we named the program simplify_before.hk , we can call the Simplify transform by running: hk-maple simplify_before.hk Note: The output for Simplify will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling hk-maple model1.hk > model2.hk . For this example, we will call our new program simplify_after.hk . When you open our new program, simplify_after.hk , you will see that the original five-line program has been reduced to a single line: fn a prob: normal(prob2real(a), sqrt(3))","title":"Simplify"},{"location":"transforms/hk-maple/#disintegrate_1","text":"The following examples are simplified versions of the Borel-Kolmogorov paradox. Both programs choose x and y uniformly from the range 0..1 and return a pair consisting of a polynomial P(x,y) and the coordinates x and y themselves. $ pretty examples/borel-sub.hk x <~ uniform(nat2real(0), nat2real(1)) y <~ uniform(nat2real(0), nat2real(1)) return (y - nat2real(2) * x, (x, y)) $ pretty examples/borel-div.hk x <~ uniform(nat2real(0), nat2real(1)) y <~ uniform(nat2real(0), nat2real(1)) return (y / x, (x, y)) Disintegrating these two programs produces results which immediately reject values of P(x,y) which are impossible (e.g. y/x cannot be less than zero for x,y in [0,1] x [0,1] ). For remaining permitted values of x and y , y is parametrized in terms of x ; that is, the equation P(x,y)=0 is solved for x . $ hk-maple -c Disint examples/borel-sub.hk fn t1 real: if +1/1 < t1 || t1 < -2/1: reject. measure(pair(real, real)) else: if -2/1 < t1 && t1 < -1/1: weight (real2prob(+1/1 + t1 * (+1/2)), x9 <~ uniform(t1 * (-1/2), +1/1) return (x9, x9 * (+2/1) + t1)) else: if -1/1 < t1 && t1 < +0/1: weight (1/2, x9 <~ uniform(t1 * (-1/2), t1 * (-1/2) + (+1/2)) return (x9, x9 * (+2/1) + t1)) else: weight (real2prob(t1 * (-1/2) + (+1/2)), x9 <~ uniform(+0/1, t1 * (-1/2) + (+1/2)) return (x9, x9 * (+2/1) + t1)) $ hk-maple -c Disint borel-div.hk fn t1 real: if t1 < +0/1: reject. measure(pair(real, real)) else: if +1/1 < t1: weight (real2prob(1/ t1), x9 <~ uniform(+0/1, 1/ t1) weight(real2prob(x9), return (x9, x9 * t1))) else: weight (1/2, x9 <~ beta(2/1, 1/1) return (prob2real(x9), prob2real(x9) * t1))","title":"Disintegrate"},{"location":"transforms/hk-maple/#summarize_1","text":"These examples are given in Maple syntax as opposed to typical examples in Hakaru syntax. First \u201csummate i\u201d in offshore under gmm_gibbs.hk summarize(piecewise(_b=piecewise(i=docUpdate,zNew,z[i]),t[i],0), kb, i) = [Split(i=docUpdate, Fanout(Add(t[i]), Nop()), Index(size(as), _b, z[i], Add(t[i]))), summary -> piecewise(_b=zNew, summary[1][1], 0) + summary[2][_b+1]] Recursive call to summarize assuming i=docUpdate is true: summarize(piecewise(_b=zNew,t[i],0), kb, i) = [Fanout(Add(t[i]), Nop()), summary -> piecewise(_b=zNew, summary[1], 0)] Recursive call to summarize assuming i=docUpdate is false: summarize(piecewise(_b=z[i],t[i],0), kb, i) = [Index(size(as), _b, z[i], Add(t[i])), summary -> summary[_b+1]] summarize(t[i], kb, i) = [Add(t[i]), summary -> summary] First \u201csummate j\u201d in offshore under naive_bayes_gibbs.hk summarize(piecewise(doc[j]=docUpdate,piecewise(k=zNew,piecewise(i=w[j],1,0),0),0), kb, j) = [Fanout(Index(size(word_prior), i, w[j], Index(size(z), docUpdate, doc[j], Add(1))), Index(size(z), docUpdate, doc[j], Nop())), summary -> piecewise(k=zNew, summary[1][w[j]+1][docUpdate+1]], 0)] Recursive call to summarize assuming k=zNew is true: summarize(piecewise(doc[j]=docUpdate,piecewise(i=w[j],1,0),0), kb, j) = [Index(size(word_prior), i, w[j], Index(size(z), docUpdate, doc[j], Add(1))), summary -> summary[w[j]+1][docUpdate+1]] Recursive call to summarize assuming i=w[j]: summarize(piecewise(doc[j]=docUpdate,1,0), kb, j) = [Index(size(z), docUpdate, doc[j], Add(1)), summary -> summary[docUpdate+1]] Recursive call to summarize assuming k=zNew is false: summarize(piecewise(doc[j]=docUpdate,0,0), kb, j) = [Index(size(z), docUpdate, doc[j], Nop()), summary -> 0]","title":"Summarize"},{"location":"transforms/hkc/","text":"Compiling to C hkc is a command line tool to compiler Hakaru programs to C. HKC was created with portability and speed in mind. More recently, OpenMP support is being added to gain more performance on multi-core machines. Basic command line usage of HKC is much like other compilers: hkc foo.hk -o foo.c It is possible to go straight to an executable with the --make ARG flag, where the argument is the C compiler you would like to use. Type Conversions The types available in Hakaru programs are the following: nat , int , real , prob , array(<type>) , measure(<type>) , and datum like true and false . nat and int have a trivial mapping to the C int type. real becomes a C double . The prob type in Hakaru is stored in the log-domain to avoid underflow. In C this corresponds to a double , but we first take the log of it before storing it, so we have to take the exp of it to bring it back to the real numbers. Arrays become structs that contain the size and a pointer to data stored within. The structs are generated at compile time, but there are only four which are named after the type they contain. Here they all are: struct arrayNat { int size; int * data; }; struct arrayInt { int size; int * data; }; struct arrayReal { int size; double * data; }; struct arrayProb { int size; double * data; }; Measures Measures compile to C functions that take a location for a sample, return the weight of the measure and store a sample in the location is was given. A simple example is uniform(0,1) a measure over type real . #include <time.h> #include <stdlib.h> #include <stdio.h> #include <math.h> double measure(double * s_a) { *s_a = ((double)0) + ((double)rand()) / ((double)RAND_MAX) * ((double)1) - ((double)0); return 0; } int main() { double sample; while (1) { measure(&sample); printf(\"%.17f\\n\",sample); } return 0; } Recall that weights have type prob and are stored in the log-domain. This example has a weight of 1. Calling hkc on a measure will create a function like the one above and also a main function that infinitely takes samples. Using hkc -F ARG will produce just the function with the name of its argument. Lambdas Lambdas compile to functions in C: fn x array(real): (summate i from 0 to size(x): x[i]) * prob2real(recip(nat2prob((size(x) + 1)))) Becomes: #include <stdlib.h> #include <stdio.h> #include <math.h> struct arrayReal { int size; double * data; }; double fn_a(struct arrayReal x_b) { unsigned int i_c; double acc_d; double p_e; double _f; double r_g; acc_d = 0; for (i_c = 0; i_c < x_b.size; i_c++) { acc_d += *(x_b.data + i_c); } p_e = log1p(((1 + x_b.size) - 1)); _f = -p_e; r_g = (expm1(_f) + 1); return (r_g * acc_d); } Using the -F flag will allow the user to add their own name to a function, otherwise the name is chosen automatically as fn_<unique identifier> . Computations When compiling a computation, HKC just creates a main function to compute the value and print it. For example: summate i from 1 to 100000000: nat2real(i) / nat2real(i) becomes: #include <stdlib.h> #include <stdio.h> #include <math.h> int main() { double result; int i_a; double acc_b; double _c; acc_b = 0; for (i_a = 1; i_a < 100000000; i_a++) { _c = (1 / ((double)i_a)); acc_b += (_c * ((double)i_a)); } result = acc_b; printf(\"%.17f\\n\",result); return 0; } Parallel Programs Calling HKC with the -j flag will generate the code with parallel regions to compute the value. The parallel code uses OpenMP directives. To check if you\u2019re compiler supports OpenMP, check here . For example, GCC requires the -fopenmp flag for OpenMP support: hkc -j foo.hk -o foo.c gcc -lm -fopenmp foo.c -o foo.bin","title":"Compiling to C"},{"location":"transforms/hkc/#compiling-to-c","text":"hkc is a command line tool to compiler Hakaru programs to C. HKC was created with portability and speed in mind. More recently, OpenMP support is being added to gain more performance on multi-core machines. Basic command line usage of HKC is much like other compilers: hkc foo.hk -o foo.c It is possible to go straight to an executable with the --make ARG flag, where the argument is the C compiler you would like to use.","title":"Compiling to C"},{"location":"transforms/hkc/#type-conversions","text":"The types available in Hakaru programs are the following: nat , int , real , prob , array(<type>) , measure(<type>) , and datum like true and false . nat and int have a trivial mapping to the C int type. real becomes a C double . The prob type in Hakaru is stored in the log-domain to avoid underflow. In C this corresponds to a double , but we first take the log of it before storing it, so we have to take the exp of it to bring it back to the real numbers. Arrays become structs that contain the size and a pointer to data stored within. The structs are generated at compile time, but there are only four which are named after the type they contain. Here they all are: struct arrayNat { int size; int * data; }; struct arrayInt { int size; int * data; }; struct arrayReal { int size; double * data; }; struct arrayProb { int size; double * data; };","title":"Type Conversions"},{"location":"transforms/hkc/#measures","text":"Measures compile to C functions that take a location for a sample, return the weight of the measure and store a sample in the location is was given. A simple example is uniform(0,1) a measure over type real . #include <time.h> #include <stdlib.h> #include <stdio.h> #include <math.h> double measure(double * s_a) { *s_a = ((double)0) + ((double)rand()) / ((double)RAND_MAX) * ((double)1) - ((double)0); return 0; } int main() { double sample; while (1) { measure(&sample); printf(\"%.17f\\n\",sample); } return 0; } Recall that weights have type prob and are stored in the log-domain. This example has a weight of 1. Calling hkc on a measure will create a function like the one above and also a main function that infinitely takes samples. Using hkc -F ARG will produce just the function with the name of its argument.","title":"Measures"},{"location":"transforms/hkc/#lambdas","text":"Lambdas compile to functions in C: fn x array(real): (summate i from 0 to size(x): x[i]) * prob2real(recip(nat2prob((size(x) + 1)))) Becomes: #include <stdlib.h> #include <stdio.h> #include <math.h> struct arrayReal { int size; double * data; }; double fn_a(struct arrayReal x_b) { unsigned int i_c; double acc_d; double p_e; double _f; double r_g; acc_d = 0; for (i_c = 0; i_c < x_b.size; i_c++) { acc_d += *(x_b.data + i_c); } p_e = log1p(((1 + x_b.size) - 1)); _f = -p_e; r_g = (expm1(_f) + 1); return (r_g * acc_d); } Using the -F flag will allow the user to add their own name to a function, otherwise the name is chosen automatically as fn_<unique identifier> .","title":"Lambdas"},{"location":"transforms/hkc/#computations","text":"When compiling a computation, HKC just creates a main function to compute the value and print it. For example: summate i from 1 to 100000000: nat2real(i) / nat2real(i) becomes: #include <stdlib.h> #include <stdio.h> #include <math.h> int main() { double result; int i_a; double acc_b; double _c; acc_b = 0; for (i_a = 1; i_a < 100000000; i_a++) { _c = (1 / ((double)i_a)); acc_b += (_c * ((double)i_a)); } result = acc_b; printf(\"%.17f\\n\",result); return 0; }","title":"Computations"},{"location":"transforms/hkc/#parallel-programs","text":"Calling HKC with the -j flag will generate the code with parallel regions to compute the value. The parallel code uses OpenMP directives. To check if you\u2019re compiler supports OpenMP, check here . For example, GCC requires the -fopenmp flag for OpenMP support: hkc -j foo.hk -o foo.c gcc -lm -fopenmp foo.c -o foo.bin","title":"Parallel Programs"},{"location":"transforms/mh/","text":"Metropolis Hastings In Hakaru, all inference algorithms are represented as program transformations. In particular, the Metropolis-Hastings transform takes as input a probabilistic program representing the target distribution, and a probabilistic program representing the proposal distribution and returns a probabilistic program representing the MH transition kernel. mh command You can access this functionality using the mh command. It takes two files as input representing the target distribution and proposal kernel. For example, suppose we would like to make a Markov Chain for the normal distribution, where the proposal distribution is a random walk. Target # target.hk normal(0,1) Proposal # proposal.hk fn x real: normal(x, 0.04) We can use mh to create a transition kernel. mh target.hk proposal.hk x5 = x2 = fn x0 real: (exp((negate(((x0 - nat2real(0)) ^ 2)) / prob2real((2 * (nat2prob(1) ^ 2))))) / nat2prob(1) / sqrt((2 * pi)) / 1) fn x1 real: x0 <~ normal(x1, (1/25)) return (x0, (x2(x0) / x2(x1))) fn x4 real: x3 <~ x5(x4) (match x3: (x1, x2): x0 <~ weight(min(1, x2), return true) <|> weight(real2prob((prob2real(1) - prob2real(min(1, x2)))), return false) return (match x0: true: x1 false: x4)) This can then be simplified. mh target.hk proposal.hk | hk-maple -c Simplify - fn x4 real: x03 <~ normal(x4, (1/25)) weight(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))), return x03) <|> weight(real2prob((1 + (prob2real(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2))))) * (-1)))), return x4) This can then be run using hakaru . Hakaru when run with two arguments will assume that the first file is a transition kernel, and the second file represents a measure to initialize from. mh target.hk proposal.hk | hk-maple -c Simplify - | hakaru -w --transition-kernel - target.hk | head -0.6133542972818671 -0.6111567543723275 -0.5963756142974966 -0.5661156231637984 -0.6280335079595971 -0.616432866701967 -0.6053631512209712 -0.5964839795872353 -0.6020821843203473 -0.6535246137595148","title":"Metropolis Hastings"},{"location":"transforms/mh/#metropolis-hastings","text":"In Hakaru, all inference algorithms are represented as program transformations. In particular, the Metropolis-Hastings transform takes as input a probabilistic program representing the target distribution, and a probabilistic program representing the proposal distribution and returns a probabilistic program representing the MH transition kernel.","title":"Metropolis Hastings"},{"location":"transforms/mh/#mh-command","text":"You can access this functionality using the mh command. It takes two files as input representing the target distribution and proposal kernel. For example, suppose we would like to make a Markov Chain for the normal distribution, where the proposal distribution is a random walk.","title":"mh command"},{"location":"transforms/mh/#target","text":"# target.hk normal(0,1)","title":"Target"},{"location":"transforms/mh/#proposal","text":"# proposal.hk fn x real: normal(x, 0.04) We can use mh to create a transition kernel. mh target.hk proposal.hk x5 = x2 = fn x0 real: (exp((negate(((x0 - nat2real(0)) ^ 2)) / prob2real((2 * (nat2prob(1) ^ 2))))) / nat2prob(1) / sqrt((2 * pi)) / 1) fn x1 real: x0 <~ normal(x1, (1/25)) return (x0, (x2(x0) / x2(x1))) fn x4 real: x3 <~ x5(x4) (match x3: (x1, x2): x0 <~ weight(min(1, x2), return true) <|> weight(real2prob((prob2real(1) - prob2real(min(1, x2)))), return false) return (match x0: true: x1 false: x4)) This can then be simplified. mh target.hk proposal.hk | hk-maple -c Simplify - fn x4 real: x03 <~ normal(x4, (1/25)) weight(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))), return x03) <|> weight(real2prob((1 + (prob2real(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2))))) * (-1)))), return x4) This can then be run using hakaru . Hakaru when run with two arguments will assume that the first file is a transition kernel, and the second file represents a measure to initialize from. mh target.hk proposal.hk | hk-maple -c Simplify - | hakaru -w --transition-kernel - target.hk | head -0.6133542972818671 -0.6111567543723275 -0.5963756142974966 -0.5661156231637984 -0.6280335079595971 -0.616432866701967 -0.6053631512209712 -0.5964839795872353 -0.6020821843203473 -0.6535246137595148","title":"Proposal"},{"location":"transforms/normalize/","text":"Normalize We also provide a normalize command. This command takes as input a program representing any measure and reweights it into a program representing a probability distribution. For example in a slightly contrived example, we can weight a normal distribution by two. Normalizing it will then remove this weight. > echo \"weight(2, normal(0,1))\" | normalize | hk-maple -c Simplify - normal(0, 1)","title":"Normalize"},{"location":"transforms/normalize/#normalize","text":"We also provide a normalize command. This command takes as input a program representing any measure and reweights it into a program representing a probability distribution. For example in a slightly contrived example, we can weight a normal distribution by two. Normalizing it will then remove this weight. > echo \"weight(2, normal(0,1))\" | normalize | hk-maple -c Simplify - normal(0, 1)","title":"Normalize"},{"location":"workflow/continuous/","text":"Tutorial: Hakaru Workflow for a Continuous Model The real world is a complex and unpredictable place, so many statistical problems involve random real numbers. Hakaru is able to tackle these real world problems using a similar approach to the one used for discrete models. To illustrate this workflow, the calibration of thermometers is used 1 . In this scenario, we are building thermometers that measure the temperature of a room. A reliable thermometer here relies on two attributes: Temperature noise, or how much the room\u2019s temperature fluctuates over time Measurement noise, or how often the thermometer measures the wrong value due to device defects In order to calibrate our thermometers, we want to approximate these values as accurately as possible so that our thermometers can tune its measurements based on its knowledge of temperature and measurement noise in the environment. Modelling For our thermometer model, we must first make a few assumptions about the environment. Normally this information would be collected as part of the problem\u2019s domain knowledge. For this example, we will use the following information: The temperature noise follows a uniform distribution on the interval [ 3, 8 ] The measurement noise follows a uniform distribution with a range of [ 1, 4 ] Temperature and measurement samples follow a normal distribution The initial temperature of the room is 21 ^{\\circ} C Our model starts with the definition of the temperature and measurement noise. From our assumptions, we know that these values follow a uniform distribution with real number intervals. In addition to defining distributions for these values, we will also use coercions to cast the values from real to prob values: nT <~ uniform(3,8) nM <~ uniform(1,4) noiseT = real2prob(nT) noiseM = real2prob(nM) Note: See Let and Bind for usage differences. The values generated for noiseT and noiseM are used as the standard deviation required by the normal primitive probability distribution when generating values for temperature ( t1 , t2 ) and measurement ( m1 , m2 ). For temperature, we need two values. The first is a temperature centered about the initial room temperature (21 ^{\\circ} C) and the second is a future room temperature centered about the the first measured temperature. Both follow a normal distribution with a standard deviation of noiseT : t1 <~ normal(21, noiseT) t2 <~ normal(t1, noiseT) Temperature measurements are centered about the temperature value being measured, therefore the values t1 and t2 are used. We made the initial assumption that measurement values follow a normal distribution, and we have generated noiseM as the standard deviation: m1 <~ normal(t1, noiseM) m2 <~ normal(t2, noiseM) Finally, we must return the information that we are interested in from our model. For measurement tuning, we are interested in the values of m1 and m2 . We would also like to know what kind of noise was generated in our model, noiseT and noiseM . We will package these values together in related pairs. Instead of returning two seperate pairs of information, we will encapsulate the sets into a container pair: return ((m1, m2), (noiseT, noiseM)) The return statement completes our model definition. We have defined two values, noiseT and noiseM , to represent the environmental noise in the temperature and measurement samples. We generated two room temperatures, t1 and t2 , which we used to generate the dependent measurement samples m1 and m2 . Once all the values have been generated, we return an information set containing the two measurement samples and the environmental noise. With our completed model, we can use Hakaru program transformations to determine plausible thermometer calibration values. Transformation To calibrate our thermometers, we need to know how differences in environmental noises affect temperature measurements. For our scenario, this means that we want to know what the conditional distribution on environmental noise given the measurement data. Unlike the discrete model example , this problem would be extremely difficult to reason about without transforming it in any way. To generate a conditional distribution for this problem, we will use Hakaru\u2019s disintegrate transform . This transformation requires that the target model have a return statement that presents information in the order of known information followed by unknown information. We have already configured our model in this manner, so we can run the disintegrate transform immediately: fn x8 pair(real, real): match x8: (x25, x26): nT <~ uniform(+3/1, +8/1) nM <~ uniform(+1/1, +4/1) noiseT = real2prob(nT) noiseM = real2prob(nM) t1 <~ normal(+21/1, noiseT) t2 <~ normal(t1, noiseT) x28 <~ weight (exp((-(x25 - t1) ^ 2) / prob2real(2/1 * noiseM ^ 2)) / noiseM / sqrt(2/1 * pi), return ()) x27 <~ weight (exp((-(x26 - t2) ^ 2) / prob2real(2/1 * noiseM ^ 2)) / noiseM / sqrt(2/1 * pi), return ()) return (noiseT, noiseM) _: reject. measure(pair(prob, prob)) An additional Hakaru transformation that can be performed at this stage is the Hakaru-Maple simplify subcommand . This will call Maple to algebraically simplify Hakaru models. The result is a more efficient program, sampling from two uniform distributions instead of four. fn x8 pair(real, real): match x8: (r3, r1): weight (1/ pi * (1/2), nTd <~ uniform(+3/1, +8/1) nMb <~ uniform(+1/1, +4/1) weight (exp ((nMb ^ 2 * r1 ^ 2 + nMb ^ 2 * r3 ^ 2 + nTd ^ 2 * r1 ^ 2 + nTd ^ 2 * r1 * r3 * (-2/1) + nTd ^ 2 * r3 ^ 2 * (+2/1) + nMb ^ 2 * r1 * (-42/1) + r3 * nMb ^ 2 * (-42/1) + r3 * nTd ^ 2 * (-42/1) + nMb ^ 2 * (+882/1) + nTd ^ 2 * (+441/1)) / (nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4) * (-1/2)) / sqrt(real2prob(nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4)), return (real2prob(nTd), real2prob(nMb)))) The two models are equivalent, so you must decide which model that you want to use for your application. For the purposes of this tutorial, we will use the unsimplified version of the model ( thermometer_disintegrate.hk ). Our thermometer model is two dimensional, so it is possible for us to tune our model values using importance sampling or an exhaustive search. However, these approaches will not be possible in higher dimensions. Therefore, we will use a Markov Chain Monte Carlo method called Metropolis-Hastings to demonstrate how Hakaru can be used for problems with high dimensionality. To use the Metropolis-Hastings transform , you must have a target distribution and a transition kernel. The thermometer model that we have already built will be our target distribution, but we have yet to create the transition kernel. When specifying a model to be the transition kernel, our goal is to propose samples that are representative of the posterior model. For this example, we will hold one of the noise parameters constant will updating the other by drawing new values from a uniform distribution. This allows the sampler to remember a good setting for a parameter when one is found, allowing it to concentrate on the remaining parameters: fn noise pair(prob, prob): match noise: (noiseTprev, noiseMprev): weight(1/2, noiseTprime <~ uniform(3,8) return (real2prob(noiseTprime), noiseMprev)) <|> weight(1/2, noiseMprime <~ uniform(1,4) return (noiseTprev, real2prob(noiseMprime))) Note: Like any model in Hakaru, this program can be passed to other program transformations such as hk-maple . Application With both our target distribution and transition kernel defined, we can now use the Metropolis-Hastings method to transform our program. However, instead of calling mh in the command prompt, we will include it as part of our Hakaru program by using the mcmc(<kernel>, <target>) syntactic transform: mcmc( simplify( fn noise pair(prob, prob): match noise: (noiseTprev, noiseMprev): weight(1/2, noiseTprime <~ uniform(3,8) return (real2prob(noiseTprime), noiseMprev)) <|> weight(1/2, noiseMprime <~ uniform(1,4) return (noiseTprev, real2prob(noiseMprime)))) , simplify( fn x8 pair(real, real): match x8: (r3, r1): weight (1/ pi * (1/2), nTd <~ uniform(+3/1, +8/1) nMb <~ uniform(+1/1, +4/1) weight (exp ((nMb ^ 2 * r1 ^ 2 + nMb ^ 2 * r3 ^ 2 + nTd ^ 2 * r1 ^ 2 + nTd ^ 2 * r1 * r3 * (-2/1) + nTd ^ 2 * r3 ^ 2 * (+2/1) + nMb ^ 2 * r1 * (-42/1) + r3 * nMb ^ 2 * (-42/1) + r3 * nTd ^ 2 * (-42/1) + nMb ^ 2 * (+882/1) + nTd ^ 2 * (+441/1)) / (nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4) * (-1/2)) / sqrt(real2prob(nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4)), return (real2prob(nTd), real2prob(nMb))))) ) Note: Each model within the mcmc syntax must be wrapped within another syntactic transform. For this example, we are using the simplify transform. The mcmc syntactic transform must also be wrapped in a Hakaru function that has the same type signature as the target model. Due to the self-referential nature of MCMC methods, this function is referenced at the end of the target model\u2019s definition. We will call this function recurse : fn recurse pair(real, real): mcmc( simplify( fn noise pair(prob, prob): match noise: (noiseTprev, noiseMprev): weight(1/2, noiseTprime <~ uniform(3,8) return (real2prob(noiseTprime), noiseMprev)) <|> weight(1/2, noiseMprime <~ uniform(1,4) return (noiseTprev, real2prob(noiseMprime)))) , simplify( fn x8 pair(real, real): match x8: (r3, r1): weight (1/ pi * (1/2), nTd <~ uniform(+3/1, +8/1) nMb <~ uniform(+1/1, +4/1) weight (exp ((nMb ^ 2 * r1 ^ 2 + nMb ^ 2 * r3 ^ 2 + nTd ^ 2 * r1 ^ 2 + nTd ^ 2 * r1 * r3 * (-2/1) + nTd ^ 2 * r3 ^ 2 * (+2/1) + nMb ^ 2 * r1 * (-42/1) + r3 * nMb ^ 2 * (-42/1) + r3 * nTd ^ 2 * (-42/1) + nMb ^ 2 * (+882/1) + nTd ^ 2 * (+441/1)) / (nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4) * (-1/2)) / sqrt(real2prob(nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4)), return (real2prob(nTd), real2prob(nMb)))))(recurse) ) Our MCMC transform is now defined and ready to be processed. To convert this model into a form understood by the hakaru command, you must run the hk-maple transform: $ hk-maple examples/documentation/thermometer_mcmc.hk fn recurse pair(real, real): x5 = x17 = fn x16 pair(prob, prob): 1/1 * (1/1) * (match x16: (x35, x36): x37 = prob2real(x35) x38 = prob2real(x36) match recurse: (r3, r1): 1/ pi * (1/2) * (match +3/1 <= x37 && x37 <= +8/1: true: 1/ real2prob(+8/1 - (+3/1)) * (nTdd = prob2real(x35) match +1/1 <= x38 && x38 <= +4/1: true: 1/ real2prob(+4/1 - (+1/1)) * (x44 = () nMbb = prob2real(x36) exp ((nMbb ^ 2 * r1 ^ 2 + nMbb ^ 2 * r3 ^ 2 + nTdd ^ 2 * r1 ^ 2 + nTdd ^ 2 * r1 * r3 * (-2/1) + nTdd ^ 2 * r3 ^ 2 * (+2/1) + nMbb ^ 2 * r1 * (-42/1) + nMbb ^ 2 * r3 * (-42/1) + nTdd ^ 2 * r3 * (-42/1) + nMbb ^ 2 * (+882/1) + nTdd ^ 2 * (+441/1)) / (nMbb ^ 4 + nMbb ^ 2 * nTdd ^ 2 * (+3/1) + nTdd ^ 4) * (-1/2)) / sqrt (real2prob(nMbb ^ 4 + nMbb ^ 2 * nTdd ^ 2 * (+3/1) + nTdd ^ 4)) * (1/1)) _: 0/1) _: 0/1) _: 0/1 _: 0/1) fn x16 pair(prob, prob): x0 <~ (fn noise pair(prob, prob): match noise: (r3, r1): weight (1/2, noiseTprime7 <~ uniform(+3/1, +8/1) return (real2prob(noiseTprime7), r1)) <|> weight (1/2, noiseMprime9 <~ uniform(+1/1, +4/1) return (r3, real2prob(noiseMprime9)))) (x16) return (x0, x17(x0) / x17(x16)) fn x4 pair(prob, prob): x3 <~ x5(x4) match x3: (x1, x2): x0 <~ x0 <~ categorical ([min(1/1, x2), real2prob(prob2real(1/1) - prob2real(min(1/1, x2)))]) return [true, false][x0] return if x0: x1 else: x4 Note: You can run the hk-maple function on the resulting program to simplify it . With our model defined and processed, we can now assign it values to generate samples from. For the sake of this example, let\u2019s say that we observed temperature measurements of 29 ^{\\circ} C and 26 ^{\\circ} C. To use these values, we must turn our anonymous Hakaru functions into callable ones so that we can assign them the pair (29,26). For our transition kernel ( thermometer_mcmc_processed.hk ), this change would be: therm = fn recurse pair(real, real): match recurse: ... return (rf, rd))) therm((29,26)) Note: This is the simplified version of our transition kernel. Due to its length, this program has been shortened to make the changes more apparent. After the same change, our target Hakru program ( thermometer_disintegrate_simplify.hk ) becomes: thermometer = fn x8 pair(real, real): match x8: (r3, r1): weight (1/ pi * (1/2), nTd <~ uniform(+3/1, +8/1) nMb <~ uniform(+1/1, +4/1) weight (exp ((nMb ^ 2 * r1 ^ 2 + nMb ^ 2 * r3 ^ 2 + nTd ^ 2 * r1 ^ 2 + nTd ^ 2 * r1 * r3 * (-2/1) + nTd ^ 2 * r3 ^ 2 * (+2/1) + nMb ^ 2 * r1 * (-42/1) + r3 * nMb ^ 2 * (-42/1) + r3 * nTd ^ 2 * (-42/1) + nMb ^ 2 * (+882/1) + nTd ^ 2 * (+441/1)) / (nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4) * (-1/2)) / sqrt(real2prob(nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4)), return (real2prob(nTd), real2prob(nMb)))) thermometer((29,26)) With these alterations, we can finally use the hakaru command to generate samples from our model: $ hakaru --transition-kernel thermometer_mcmc_processed.hk thermometer_disintegrate_simplify.hk (3.1995679303602578, 2.3093879567135325) (3.1995679303602578, 2.3093879567135325) (3.1995679303602578, 3.664010086898677) (3.1995679303602578, 3.664010086898677) (3.512655151270474, 3.664010086898677) (6.535434584595698, 3.664010086898677) (7.944581473529944, 3.664010086898677) (6.960163985266382, 3.664010086898677) (6.960163985266382, 1.850724571692917) (6.960163985266382, 1.850724571692917) (6.960163985266382, 1.850724571692917) ... In the Hakaru system definition paper 1 , a graph is generated from the data by collecting every fifth sample from 20,000 generated samples. You can create a file with this information by using an awk script, which can then be imported into a graphing software package such as Maple: $ hakaru --transition-kernel thermometer_mcmc_processed.hk thermometer_disintegrate_simplify.hk | head -n 20000 | awk 'BEGIN{i = 0}{if (i % 5 == 0) a[i/5] = $0; i = i + 1}END{for (j in a) print a[j]}' > thermometer_output.txt Extra: A Syntactic Definition This tutorial demonstrates how both command line and syntactic Hakaru transforms can be used in the same problem. This might not always be necessary because you might be able to use only command line or only syntactic Hakaru transforms. For example, the thermometer model can be expressed using only syntactic transforms : simplify( fn x pair(real, real): mcmc( simplify( fn noise pair(prob, prob): match noise: (noiseTprev, noiseMprev): weight(1/2, noiseTprime <~ uniform(3,8) return (real2prob(noiseTprime), noiseMprev)) <|> weight(1/2, noiseMprime <~ uniform(1,4) return (noiseTprev, real2prob(noiseMprime)))) , simplify( disint( nT <~ uniform(3,8) nM <~ uniform(1,4) noiseT = real2prob(nT) noiseM = real2prob(nM) t1 <~ normal(21, noiseT) t2 <~ normal(t1, noiseT) m1 <~ normal(t1, noiseM) m2 <~ normal(t2, noiseM) return ((m1, m2), (noiseT, noiseM))))(x) ) ) This Hakaru program will produce the same output program as the mixed-usage example when evaluated using hk-maple . P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic Programming, pp. 62-79, 2016. \u21a9 \u21a9","title":"Tutorial: Hakaru Workflow for Continuous Models"},{"location":"workflow/continuous/#tutorial-hakaru-workflow-for-a-continuous-model","text":"The real world is a complex and unpredictable place, so many statistical problems involve random real numbers. Hakaru is able to tackle these real world problems using a similar approach to the one used for discrete models. To illustrate this workflow, the calibration of thermometers is used 1 . In this scenario, we are building thermometers that measure the temperature of a room. A reliable thermometer here relies on two attributes: Temperature noise, or how much the room\u2019s temperature fluctuates over time Measurement noise, or how often the thermometer measures the wrong value due to device defects In order to calibrate our thermometers, we want to approximate these values as accurately as possible so that our thermometers can tune its measurements based on its knowledge of temperature and measurement noise in the environment.","title":"Tutorial: Hakaru Workflow for a Continuous Model"},{"location":"workflow/continuous/#modelling","text":"For our thermometer model, we must first make a few assumptions about the environment. Normally this information would be collected as part of the problem\u2019s domain knowledge. For this example, we will use the following information: The temperature noise follows a uniform distribution on the interval [ 3, 8 ] The measurement noise follows a uniform distribution with a range of [ 1, 4 ] Temperature and measurement samples follow a normal distribution The initial temperature of the room is 21 ^{\\circ} C Our model starts with the definition of the temperature and measurement noise. From our assumptions, we know that these values follow a uniform distribution with real number intervals. In addition to defining distributions for these values, we will also use coercions to cast the values from real to prob values: nT <~ uniform(3,8) nM <~ uniform(1,4) noiseT = real2prob(nT) noiseM = real2prob(nM) Note: See Let and Bind for usage differences. The values generated for noiseT and noiseM are used as the standard deviation required by the normal primitive probability distribution when generating values for temperature ( t1 , t2 ) and measurement ( m1 , m2 ). For temperature, we need two values. The first is a temperature centered about the initial room temperature (21 ^{\\circ} C) and the second is a future room temperature centered about the the first measured temperature. Both follow a normal distribution with a standard deviation of noiseT : t1 <~ normal(21, noiseT) t2 <~ normal(t1, noiseT) Temperature measurements are centered about the temperature value being measured, therefore the values t1 and t2 are used. We made the initial assumption that measurement values follow a normal distribution, and we have generated noiseM as the standard deviation: m1 <~ normal(t1, noiseM) m2 <~ normal(t2, noiseM) Finally, we must return the information that we are interested in from our model. For measurement tuning, we are interested in the values of m1 and m2 . We would also like to know what kind of noise was generated in our model, noiseT and noiseM . We will package these values together in related pairs. Instead of returning two seperate pairs of information, we will encapsulate the sets into a container pair: return ((m1, m2), (noiseT, noiseM)) The return statement completes our model definition. We have defined two values, noiseT and noiseM , to represent the environmental noise in the temperature and measurement samples. We generated two room temperatures, t1 and t2 , which we used to generate the dependent measurement samples m1 and m2 . Once all the values have been generated, we return an information set containing the two measurement samples and the environmental noise. With our completed model, we can use Hakaru program transformations to determine plausible thermometer calibration values.","title":"Modelling"},{"location":"workflow/continuous/#transformation","text":"To calibrate our thermometers, we need to know how differences in environmental noises affect temperature measurements. For our scenario, this means that we want to know what the conditional distribution on environmental noise given the measurement data. Unlike the discrete model example , this problem would be extremely difficult to reason about without transforming it in any way. To generate a conditional distribution for this problem, we will use Hakaru\u2019s disintegrate transform . This transformation requires that the target model have a return statement that presents information in the order of known information followed by unknown information. We have already configured our model in this manner, so we can run the disintegrate transform immediately: fn x8 pair(real, real): match x8: (x25, x26): nT <~ uniform(+3/1, +8/1) nM <~ uniform(+1/1, +4/1) noiseT = real2prob(nT) noiseM = real2prob(nM) t1 <~ normal(+21/1, noiseT) t2 <~ normal(t1, noiseT) x28 <~ weight (exp((-(x25 - t1) ^ 2) / prob2real(2/1 * noiseM ^ 2)) / noiseM / sqrt(2/1 * pi), return ()) x27 <~ weight (exp((-(x26 - t2) ^ 2) / prob2real(2/1 * noiseM ^ 2)) / noiseM / sqrt(2/1 * pi), return ()) return (noiseT, noiseM) _: reject. measure(pair(prob, prob)) An additional Hakaru transformation that can be performed at this stage is the Hakaru-Maple simplify subcommand . This will call Maple to algebraically simplify Hakaru models. The result is a more efficient program, sampling from two uniform distributions instead of four. fn x8 pair(real, real): match x8: (r3, r1): weight (1/ pi * (1/2), nTd <~ uniform(+3/1, +8/1) nMb <~ uniform(+1/1, +4/1) weight (exp ((nMb ^ 2 * r1 ^ 2 + nMb ^ 2 * r3 ^ 2 + nTd ^ 2 * r1 ^ 2 + nTd ^ 2 * r1 * r3 * (-2/1) + nTd ^ 2 * r3 ^ 2 * (+2/1) + nMb ^ 2 * r1 * (-42/1) + r3 * nMb ^ 2 * (-42/1) + r3 * nTd ^ 2 * (-42/1) + nMb ^ 2 * (+882/1) + nTd ^ 2 * (+441/1)) / (nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4) * (-1/2)) / sqrt(real2prob(nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4)), return (real2prob(nTd), real2prob(nMb)))) The two models are equivalent, so you must decide which model that you want to use for your application. For the purposes of this tutorial, we will use the unsimplified version of the model ( thermometer_disintegrate.hk ). Our thermometer model is two dimensional, so it is possible for us to tune our model values using importance sampling or an exhaustive search. However, these approaches will not be possible in higher dimensions. Therefore, we will use a Markov Chain Monte Carlo method called Metropolis-Hastings to demonstrate how Hakaru can be used for problems with high dimensionality. To use the Metropolis-Hastings transform , you must have a target distribution and a transition kernel. The thermometer model that we have already built will be our target distribution, but we have yet to create the transition kernel. When specifying a model to be the transition kernel, our goal is to propose samples that are representative of the posterior model. For this example, we will hold one of the noise parameters constant will updating the other by drawing new values from a uniform distribution. This allows the sampler to remember a good setting for a parameter when one is found, allowing it to concentrate on the remaining parameters: fn noise pair(prob, prob): match noise: (noiseTprev, noiseMprev): weight(1/2, noiseTprime <~ uniform(3,8) return (real2prob(noiseTprime), noiseMprev)) <|> weight(1/2, noiseMprime <~ uniform(1,4) return (noiseTprev, real2prob(noiseMprime))) Note: Like any model in Hakaru, this program can be passed to other program transformations such as hk-maple .","title":"Transformation"},{"location":"workflow/continuous/#application","text":"With both our target distribution and transition kernel defined, we can now use the Metropolis-Hastings method to transform our program. However, instead of calling mh in the command prompt, we will include it as part of our Hakaru program by using the mcmc(<kernel>, <target>) syntactic transform: mcmc( simplify( fn noise pair(prob, prob): match noise: (noiseTprev, noiseMprev): weight(1/2, noiseTprime <~ uniform(3,8) return (real2prob(noiseTprime), noiseMprev)) <|> weight(1/2, noiseMprime <~ uniform(1,4) return (noiseTprev, real2prob(noiseMprime)))) , simplify( fn x8 pair(real, real): match x8: (r3, r1): weight (1/ pi * (1/2), nTd <~ uniform(+3/1, +8/1) nMb <~ uniform(+1/1, +4/1) weight (exp ((nMb ^ 2 * r1 ^ 2 + nMb ^ 2 * r3 ^ 2 + nTd ^ 2 * r1 ^ 2 + nTd ^ 2 * r1 * r3 * (-2/1) + nTd ^ 2 * r3 ^ 2 * (+2/1) + nMb ^ 2 * r1 * (-42/1) + r3 * nMb ^ 2 * (-42/1) + r3 * nTd ^ 2 * (-42/1) + nMb ^ 2 * (+882/1) + nTd ^ 2 * (+441/1)) / (nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4) * (-1/2)) / sqrt(real2prob(nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4)), return (real2prob(nTd), real2prob(nMb))))) ) Note: Each model within the mcmc syntax must be wrapped within another syntactic transform. For this example, we are using the simplify transform. The mcmc syntactic transform must also be wrapped in a Hakaru function that has the same type signature as the target model. Due to the self-referential nature of MCMC methods, this function is referenced at the end of the target model\u2019s definition. We will call this function recurse : fn recurse pair(real, real): mcmc( simplify( fn noise pair(prob, prob): match noise: (noiseTprev, noiseMprev): weight(1/2, noiseTprime <~ uniform(3,8) return (real2prob(noiseTprime), noiseMprev)) <|> weight(1/2, noiseMprime <~ uniform(1,4) return (noiseTprev, real2prob(noiseMprime)))) , simplify( fn x8 pair(real, real): match x8: (r3, r1): weight (1/ pi * (1/2), nTd <~ uniform(+3/1, +8/1) nMb <~ uniform(+1/1, +4/1) weight (exp ((nMb ^ 2 * r1 ^ 2 + nMb ^ 2 * r3 ^ 2 + nTd ^ 2 * r1 ^ 2 + nTd ^ 2 * r1 * r3 * (-2/1) + nTd ^ 2 * r3 ^ 2 * (+2/1) + nMb ^ 2 * r1 * (-42/1) + r3 * nMb ^ 2 * (-42/1) + r3 * nTd ^ 2 * (-42/1) + nMb ^ 2 * (+882/1) + nTd ^ 2 * (+441/1)) / (nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4) * (-1/2)) / sqrt(real2prob(nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4)), return (real2prob(nTd), real2prob(nMb)))))(recurse) ) Our MCMC transform is now defined and ready to be processed. To convert this model into a form understood by the hakaru command, you must run the hk-maple transform: $ hk-maple examples/documentation/thermometer_mcmc.hk fn recurse pair(real, real): x5 = x17 = fn x16 pair(prob, prob): 1/1 * (1/1) * (match x16: (x35, x36): x37 = prob2real(x35) x38 = prob2real(x36) match recurse: (r3, r1): 1/ pi * (1/2) * (match +3/1 <= x37 && x37 <= +8/1: true: 1/ real2prob(+8/1 - (+3/1)) * (nTdd = prob2real(x35) match +1/1 <= x38 && x38 <= +4/1: true: 1/ real2prob(+4/1 - (+1/1)) * (x44 = () nMbb = prob2real(x36) exp ((nMbb ^ 2 * r1 ^ 2 + nMbb ^ 2 * r3 ^ 2 + nTdd ^ 2 * r1 ^ 2 + nTdd ^ 2 * r1 * r3 * (-2/1) + nTdd ^ 2 * r3 ^ 2 * (+2/1) + nMbb ^ 2 * r1 * (-42/1) + nMbb ^ 2 * r3 * (-42/1) + nTdd ^ 2 * r3 * (-42/1) + nMbb ^ 2 * (+882/1) + nTdd ^ 2 * (+441/1)) / (nMbb ^ 4 + nMbb ^ 2 * nTdd ^ 2 * (+3/1) + nTdd ^ 4) * (-1/2)) / sqrt (real2prob(nMbb ^ 4 + nMbb ^ 2 * nTdd ^ 2 * (+3/1) + nTdd ^ 4)) * (1/1)) _: 0/1) _: 0/1) _: 0/1 _: 0/1) fn x16 pair(prob, prob): x0 <~ (fn noise pair(prob, prob): match noise: (r3, r1): weight (1/2, noiseTprime7 <~ uniform(+3/1, +8/1) return (real2prob(noiseTprime7), r1)) <|> weight (1/2, noiseMprime9 <~ uniform(+1/1, +4/1) return (r3, real2prob(noiseMprime9)))) (x16) return (x0, x17(x0) / x17(x16)) fn x4 pair(prob, prob): x3 <~ x5(x4) match x3: (x1, x2): x0 <~ x0 <~ categorical ([min(1/1, x2), real2prob(prob2real(1/1) - prob2real(min(1/1, x2)))]) return [true, false][x0] return if x0: x1 else: x4 Note: You can run the hk-maple function on the resulting program to simplify it . With our model defined and processed, we can now assign it values to generate samples from. For the sake of this example, let\u2019s say that we observed temperature measurements of 29 ^{\\circ} C and 26 ^{\\circ} C. To use these values, we must turn our anonymous Hakaru functions into callable ones so that we can assign them the pair (29,26). For our transition kernel ( thermometer_mcmc_processed.hk ), this change would be: therm = fn recurse pair(real, real): match recurse: ... return (rf, rd))) therm((29,26)) Note: This is the simplified version of our transition kernel. Due to its length, this program has been shortened to make the changes more apparent. After the same change, our target Hakru program ( thermometer_disintegrate_simplify.hk ) becomes: thermometer = fn x8 pair(real, real): match x8: (r3, r1): weight (1/ pi * (1/2), nTd <~ uniform(+3/1, +8/1) nMb <~ uniform(+1/1, +4/1) weight (exp ((nMb ^ 2 * r1 ^ 2 + nMb ^ 2 * r3 ^ 2 + nTd ^ 2 * r1 ^ 2 + nTd ^ 2 * r1 * r3 * (-2/1) + nTd ^ 2 * r3 ^ 2 * (+2/1) + nMb ^ 2 * r1 * (-42/1) + r3 * nMb ^ 2 * (-42/1) + r3 * nTd ^ 2 * (-42/1) + nMb ^ 2 * (+882/1) + nTd ^ 2 * (+441/1)) / (nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4) * (-1/2)) / sqrt(real2prob(nMb ^ 4 + nTd ^ 2 * nMb ^ 2 * (+3/1) + nTd ^ 4)), return (real2prob(nTd), real2prob(nMb)))) thermometer((29,26)) With these alterations, we can finally use the hakaru command to generate samples from our model: $ hakaru --transition-kernel thermometer_mcmc_processed.hk thermometer_disintegrate_simplify.hk (3.1995679303602578, 2.3093879567135325) (3.1995679303602578, 2.3093879567135325) (3.1995679303602578, 3.664010086898677) (3.1995679303602578, 3.664010086898677) (3.512655151270474, 3.664010086898677) (6.535434584595698, 3.664010086898677) (7.944581473529944, 3.664010086898677) (6.960163985266382, 3.664010086898677) (6.960163985266382, 1.850724571692917) (6.960163985266382, 1.850724571692917) (6.960163985266382, 1.850724571692917) ... In the Hakaru system definition paper 1 , a graph is generated from the data by collecting every fifth sample from 20,000 generated samples. You can create a file with this information by using an awk script, which can then be imported into a graphing software package such as Maple: $ hakaru --transition-kernel thermometer_mcmc_processed.hk thermometer_disintegrate_simplify.hk | head -n 20000 | awk 'BEGIN{i = 0}{if (i % 5 == 0) a[i/5] = $0; i = i + 1}END{for (j in a) print a[j]}' > thermometer_output.txt","title":"Application"},{"location":"workflow/continuous/#extra-a-syntactic-definition","text":"This tutorial demonstrates how both command line and syntactic Hakaru transforms can be used in the same problem. This might not always be necessary because you might be able to use only command line or only syntactic Hakaru transforms. For example, the thermometer model can be expressed using only syntactic transforms : simplify( fn x pair(real, real): mcmc( simplify( fn noise pair(prob, prob): match noise: (noiseTprev, noiseMprev): weight(1/2, noiseTprime <~ uniform(3,8) return (real2prob(noiseTprime), noiseMprev)) <|> weight(1/2, noiseMprime <~ uniform(1,4) return (noiseTprev, real2prob(noiseMprime)))) , simplify( disint( nT <~ uniform(3,8) nM <~ uniform(1,4) noiseT = real2prob(nT) noiseM = real2prob(nM) t1 <~ normal(21, noiseT) t2 <~ normal(t1, noiseT) m1 <~ normal(t1, noiseM) m2 <~ normal(t2, noiseM) return ((m1, m2), (noiseT, noiseM))))(x) ) ) This Hakaru program will produce the same output program as the mixed-usage example when evaluated using hk-maple . P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic Programming, pp. 62-79, 2016. \u21a9 \u21a9","title":"Extra: A Syntactic Definition"},{"location":"workflow/discrete/","text":"Tutorial: Hakaru Workflow for a Discrete Model The problem of a burglary alarm has been used to illustrate the workflow for modelling discrete distributions in Hakaru 1 . It has been adapted from Pearl\u2019s textbook on probabilistic reasoning (page 35) 2 : Imagine being awakened one night by the shrill sound of your burglar alarm. What is your degree of belief that a burglary attempt has taken place? For illustrative purposes we make the following judgements: (a) There is a 95% chance that an attempted burglary will trigger the alarm system \u2013 P(Alarm|Burglary) = 0.95; (b) based on previous false alarms, there is a slight (1 percent) chance that the alarm will be triggered by a mechanism other than an attempted burglary \u2013 P(Alarm|No Burglary) = 0.01; (c) previous crime patterns indicate that there is a one in ten thousand chance that a given house will be burglarized on a given night \u2013 P(Burglary) = 10^-4. Modelling To determine whether or not you believe that a burglary is taking place, you must begin by modelling the prior probability distribution. This requires you to create a model based on what you have observed (the sounding of the alarm) and what you would like to know or infer (is a burglary happening?). This is a discrete model because you can only select from distinct choices for each of the knowledge sets that you have. One way that you can model this scenario in Hakaru is by first creating a function for a Bernoulli experiment, which will return either true or false : def bern(p prob): x <~ categorical([p, real2prob(1 - p)]) return [true, false][x] This helper function makes it simpler to represent the information that we are provided in Pearl\u2019s problem. For example, the knowledge P(Burglary) = 10^{-4} ( burglary == true ) can now be encoded in the Hakaru program as: burglary <~ bern(0.0001) Now that we have encoded the likelihood of a burglary taking place, we can also encode the conditional probabilities of the alarm going off in the case of a burglary, and the chances of a false alarm. We want alarm to be true with a 95% probability when a burglary is taking place ( burglary == true ). We also want alarm to be true with a 1% probability when there is no burglary ( burglary == false ). Since the likelihood of the alarm going off is dependent on the value of burglary , we can use Hakaru\u2019s conditionals to determine which distribution to pick from. We can then pass this decision to our bern function to get the value for alarm : alarm <~ bern(if burglary: 0.95 else: 0.01) To complete our model, we return the values for burglary and alarm . Note that the order that the values are returned in matters for the next step in the Hakaru workflow. Since we want to make an inference based on known information, we must return values in order of known ( alarm ) followed by unknown ( burglary ) knowledge: return (alarm, burglary) Transformation For this problem, we are only interested in the cases where the alarm is sounding ( alarm == true ). If we were to skip down to the application step in the Hakaru workflow, we would have a difficult time collecting enough samples for this set because they happen infrequently. Instead, we will use the transformation step of the Hakaru workflow to convert our prior model into a conditional distribution. This will make it easier to collect the samples that we want later on. For our burglary problem, our transformation stage only requires the disintegrate transformation . Assuming that you saved your model as burglary.hk , we can use the disintegrate transformation on it by calling disintegrate burglary.hk in the command line. It will produce a conditional model represented as a new Hakaru program: fn x5 bool: bern = fn p prob: x <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][x] burglary <~ bern(1/10000) p = (match burglary: true: 19/20 false: 1/100) x16 <~ weight(([p, real2prob((1 - prob2real(p)))][(match x5: true: 0 false: 1)] / (summate x0 from 0 to size([p, real2prob((1 - prob2real(p)))]): [p, real2prob((1 - prob2real(p)))][x0])), return ()) return burglary Note: The output for disintegrate will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling disintegrate model1.hk > model2.hk . For this example, we will call our new program burglary_disintegrate.hk . This Hakaru program represents the mapping from our known knowledge (the alarm is sounding) and the knowledge that we want to infer from it (are we being burglarized?). As a result of the disintegration, our original variable alarm has been renamed to x5 . An additional Hakaru transformation that can be performed at this stage is the Hakaru-Maple simplify subcommand . This will call Maple to algebraically simplify Hakaru models. Calling hk-maple burglary_disintegrate.hk produces a new, simpler, model of our burglary alarm scenario: fn x5 bool: (match (x5 == true): true: weight(19/200000, return true) <|> weight(9999/1000000, return false) false: weight(1/200000, return true) <|> weight(989901/1000000, return false)) Without any further work, we can already see that, when the alarm is triggered, it is most likely a false alarm. However, the simplify transformation will not always produce a clear result such as this one. Therefore, we must still perform the application step of the Hakaru workflow. Application Once we have transformed our original model ( burglary.hk ) into a function that is better suited to making the inference that we are interested in ( burglary_disintegrate_simplify.hk ), we can use the generated function to create a new Hakaru program that knows that the alarm has been triggered ( alarm == true ). In this case, the only change that needs to be made is to the line fn x5 bool: , which tells the Hakaru program what state the alarm is in. Since we are only interested in cases where alarm == true , we must change this program so that it uses the known information. The program produced by disintegrate is an anonymous function , which means that we can assign it to a name and use it later in the program: burglary = fn x5 bool: (match (x5 == true): true: weight(19/200000, return true) <|> weight(9999/1000000, return false) false: weight(1/200000, return true) <|> weight(989901/1000000, return false)) burglary(true) We have finished conditioning our Hakaru program to answer our original question using the hakaru command : if the alarm is sounding, what is the likelihood that the house is being burglarized? How does the hakaru command answer our original question? If you simply call the command hakaru burglary_disintegrate_simplify.hk in the command line, you will create an infinite stream of samples: $ hakaru burglary_disintegrate_simplify.hk 1.0093999999999992e-2 false 1.0093999999999992e-2 false 1.0093999999999992e-2 false 1.0093999999999992e-2 false 1.0093999999999992e-2 false ... This does not quite answer our original question because there is no summary of the samples that we have generated. Instead, we should summarize the samples by their weight: $ hakaru burglary_disintegrate.hk | head -n 100000 | awk '{a[$2]+=$1}END{for (i in a) print i, a[i]}' false 999.811 true 9.5893 Note: This summary is limited to 100,000 samples. This summary of information makes it much easier to determine that, even though you hear the burglary alarm, the chances of the house being burglarized is very low. P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic Programming, pp. 62-79, 2016. \u21a9 J. Pearl, Probabilistic reasoning in intelligent systems: Networks of plausible inference. San Francisco: M. Kaufmann, 1988. \u21a9","title":"Tutorial: Hakaru Workflow for Discrete Models"},{"location":"workflow/discrete/#tutorial-hakaru-workflow-for-a-discrete-model","text":"The problem of a burglary alarm has been used to illustrate the workflow for modelling discrete distributions in Hakaru 1 . It has been adapted from Pearl\u2019s textbook on probabilistic reasoning (page 35) 2 : Imagine being awakened one night by the shrill sound of your burglar alarm. What is your degree of belief that a burglary attempt has taken place? For illustrative purposes we make the following judgements: (a) There is a 95% chance that an attempted burglary will trigger the alarm system \u2013 P(Alarm|Burglary) = 0.95; (b) based on previous false alarms, there is a slight (1 percent) chance that the alarm will be triggered by a mechanism other than an attempted burglary \u2013 P(Alarm|No Burglary) = 0.01; (c) previous crime patterns indicate that there is a one in ten thousand chance that a given house will be burglarized on a given night \u2013 P(Burglary) = 10^-4.","title":"Tutorial: Hakaru Workflow for a Discrete Model"},{"location":"workflow/discrete/#modelling","text":"To determine whether or not you believe that a burglary is taking place, you must begin by modelling the prior probability distribution. This requires you to create a model based on what you have observed (the sounding of the alarm) and what you would like to know or infer (is a burglary happening?). This is a discrete model because you can only select from distinct choices for each of the knowledge sets that you have. One way that you can model this scenario in Hakaru is by first creating a function for a Bernoulli experiment, which will return either true or false : def bern(p prob): x <~ categorical([p, real2prob(1 - p)]) return [true, false][x] This helper function makes it simpler to represent the information that we are provided in Pearl\u2019s problem. For example, the knowledge P(Burglary) = 10^{-4} ( burglary == true ) can now be encoded in the Hakaru program as: burglary <~ bern(0.0001) Now that we have encoded the likelihood of a burglary taking place, we can also encode the conditional probabilities of the alarm going off in the case of a burglary, and the chances of a false alarm. We want alarm to be true with a 95% probability when a burglary is taking place ( burglary == true ). We also want alarm to be true with a 1% probability when there is no burglary ( burglary == false ). Since the likelihood of the alarm going off is dependent on the value of burglary , we can use Hakaru\u2019s conditionals to determine which distribution to pick from. We can then pass this decision to our bern function to get the value for alarm : alarm <~ bern(if burglary: 0.95 else: 0.01) To complete our model, we return the values for burglary and alarm . Note that the order that the values are returned in matters for the next step in the Hakaru workflow. Since we want to make an inference based on known information, we must return values in order of known ( alarm ) followed by unknown ( burglary ) knowledge: return (alarm, burglary)","title":"Modelling"},{"location":"workflow/discrete/#transformation","text":"For this problem, we are only interested in the cases where the alarm is sounding ( alarm == true ). If we were to skip down to the application step in the Hakaru workflow, we would have a difficult time collecting enough samples for this set because they happen infrequently. Instead, we will use the transformation step of the Hakaru workflow to convert our prior model into a conditional distribution. This will make it easier to collect the samples that we want later on. For our burglary problem, our transformation stage only requires the disintegrate transformation . Assuming that you saved your model as burglary.hk , we can use the disintegrate transformation on it by calling disintegrate burglary.hk in the command line. It will produce a conditional model represented as a new Hakaru program: fn x5 bool: bern = fn p prob: x <~ categorical([p, real2prob((1 - prob2real(p)))]) return [true, false][x] burglary <~ bern(1/10000) p = (match burglary: true: 19/20 false: 1/100) x16 <~ weight(([p, real2prob((1 - prob2real(p)))][(match x5: true: 0 false: 1)] / (summate x0 from 0 to size([p, real2prob((1 - prob2real(p)))]): [p, real2prob((1 - prob2real(p)))][x0])), return ()) return burglary Note: The output for disintegrate will be printed in the console. You can easily save this program to a file by redirecting the output to a file by calling disintegrate model1.hk > model2.hk . For this example, we will call our new program burglary_disintegrate.hk . This Hakaru program represents the mapping from our known knowledge (the alarm is sounding) and the knowledge that we want to infer from it (are we being burglarized?). As a result of the disintegration, our original variable alarm has been renamed to x5 . An additional Hakaru transformation that can be performed at this stage is the Hakaru-Maple simplify subcommand . This will call Maple to algebraically simplify Hakaru models. Calling hk-maple burglary_disintegrate.hk produces a new, simpler, model of our burglary alarm scenario: fn x5 bool: (match (x5 == true): true: weight(19/200000, return true) <|> weight(9999/1000000, return false) false: weight(1/200000, return true) <|> weight(989901/1000000, return false)) Without any further work, we can already see that, when the alarm is triggered, it is most likely a false alarm. However, the simplify transformation will not always produce a clear result such as this one. Therefore, we must still perform the application step of the Hakaru workflow.","title":"Transformation"},{"location":"workflow/discrete/#application","text":"Once we have transformed our original model ( burglary.hk ) into a function that is better suited to making the inference that we are interested in ( burglary_disintegrate_simplify.hk ), we can use the generated function to create a new Hakaru program that knows that the alarm has been triggered ( alarm == true ). In this case, the only change that needs to be made is to the line fn x5 bool: , which tells the Hakaru program what state the alarm is in. Since we are only interested in cases where alarm == true , we must change this program so that it uses the known information. The program produced by disintegrate is an anonymous function , which means that we can assign it to a name and use it later in the program: burglary = fn x5 bool: (match (x5 == true): true: weight(19/200000, return true) <|> weight(9999/1000000, return false) false: weight(1/200000, return true) <|> weight(989901/1000000, return false)) burglary(true) We have finished conditioning our Hakaru program to answer our original question using the hakaru command : if the alarm is sounding, what is the likelihood that the house is being burglarized? How does the hakaru command answer our original question? If you simply call the command hakaru burglary_disintegrate_simplify.hk in the command line, you will create an infinite stream of samples: $ hakaru burglary_disintegrate_simplify.hk 1.0093999999999992e-2 false 1.0093999999999992e-2 false 1.0093999999999992e-2 false 1.0093999999999992e-2 false 1.0093999999999992e-2 false ... This does not quite answer our original question because there is no summary of the samples that we have generated. Instead, we should summarize the samples by their weight: $ hakaru burglary_disintegrate.hk | head -n 100000 | awk '{a[$2]+=$1}END{for (i in a) print i, a[i]}' false 999.811 true 9.5893 Note: This summary is limited to 100,000 samples. This summary of information makes it much easier to determine that, even though you hear the burglary alarm, the chances of the house being burglarized is very low. P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic Programming, pp. 62-79, 2016. \u21a9 J. Pearl, Probabilistic reasoning in intelligent systems: Networks of plausible inference. San Francisco: M. Kaufmann, 1988. \u21a9","title":"Application"},{"location":"workflow/intro/","text":"What is the Hakaru Workflow? When you write a Hakaru program, you must follow the workflow of Bayesian inference. Bayesian inference is a method of statistical inference where Bayes\u2019 theorem is used to update the probability of a statistical hypothesis as more information becomes available 1 . Bayes theorem describes the probability of an event based on knowledge of event-related conditions 2 . In Hakaru, the workflow of Bayesian inference appears as modelling , transformation , and application stages 3 : In the modelling stage, you must create a probabilistic model of the environment as a prior probability distribution. This model requires you to include the known information and identify what information is to be inferred. The Hakaru language contains distributions and tools which formalize the modelling stage. In the transformation stage, the prior distribution created in the modelling stage is transformed into a conditional distribution. During this stage, a function is created which maps known knowledge to a distribution that models the information that you want to infer. To automate this stage, Hakaru includes program transformations for model conditioning. In the application stage, the function generated in the transformation stage is applied to the known knowledge that you have provided to get the posterior distribution on what is to be inferred. At the end of this stage, you can use Hakaru to show the resulting distribution as both a stream of samples and as a term in the Hakaru language. Bayesian inference (Wikipedia) \u21a9 Bayes\u2019 theorem (Wikipedia) \u21a9 P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic Programming, pp. 62-79, 2016. \u21a9","title":"What is the Hakaru Workflow?"},{"location":"workflow/intro/#what-is-the-hakaru-workflow","text":"When you write a Hakaru program, you must follow the workflow of Bayesian inference. Bayesian inference is a method of statistical inference where Bayes\u2019 theorem is used to update the probability of a statistical hypothesis as more information becomes available 1 . Bayes theorem describes the probability of an event based on knowledge of event-related conditions 2 . In Hakaru, the workflow of Bayesian inference appears as modelling , transformation , and application stages 3 : In the modelling stage, you must create a probabilistic model of the environment as a prior probability distribution. This model requires you to include the known information and identify what information is to be inferred. The Hakaru language contains distributions and tools which formalize the modelling stage. In the transformation stage, the prior distribution created in the modelling stage is transformed into a conditional distribution. During this stage, a function is created which maps known knowledge to a distribution that models the information that you want to infer. To automate this stage, Hakaru includes program transformations for model conditioning. In the application stage, the function generated in the transformation stage is applied to the known knowledge that you have provided to get the posterior distribution on what is to be inferred. At the end of this stage, you can use Hakaru to show the resulting distribution as both a stream of samples and as a term in the Hakaru language. Bayesian inference (Wikipedia) \u21a9 Bayes\u2019 theorem (Wikipedia) \u21a9 P. Narayanan, J. Carette, W. Romano, C. Shan and R. Zinkov, \u201cProbabilistic Inference by Program Transformation in Hakaru (System Description)\u201d, Functional and Logic Programming, pp. 62-79, 2016. \u21a9","title":"What is the Hakaru Workflow?"}]}