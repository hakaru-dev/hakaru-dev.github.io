{
    "docs": [
        {
            "location": "/",
            "text": "Hakaru\n\n\n\n\n\nHakaru is a probabilistic programming language. A probabilistic programming\nlanguage is a language specifically designed for manipulating probability\ndistributions. These sorts of languages are great for machine learning and\nstochastic modeling.\n\n\nOverview\n\n\nThis manual provides a guide for how to use Hakaru.\n\n\nIntroduction\n\n\nWhat is Probabilistic Programming\n\n\nProbabilistic programming systems allow us to write programs which\ndescribe probability distributions, and provide mechanisms to\nsample and condition the distributions they represent on data. In\nthis page, we give a sense of the sorts of problems Hakaru is\ngreat at solving, and how you would describe them in Hakaru.\n\n\nInstallation\n\n\nLearn how to install Hakaru\n\n\nQuickstart\n\n\nGet started with this quickstart page. Where we show\nhow to sample and condition from a small Hakaru program.\n\n\nExamples\n\n\nHere we go through several more involved examples of the kinds of\nproblems Hakaru is uniquely well-suited to solve.\n\n\nIn particular, we describe a model for Gaussian Mixture Models and\nusing a form of Bayesian Naives Bayes as applied to document\nclassification.\n\n\nLanguage Guide\n\n\nThe language section provides an overview of the syntax of Hakaru as\nwell as some of the primitives in the language.\n\n\nRandom Primitives\n\n\nThese are the built-in probability distributions.\n\n\nLet and Bind\n\n\nThis is how we can give names to subexpressions and a\ndraw from a probability distribution.\n\n\nConditionals\n\n\nHakaru supports a restricted \nif\n expression\n\n\nTypes and Coercions\n\n\nHakaru is a simply-typed language. This section\ndescribes the types available and functions for\nmoving between them.\n\n\nFunctions\n\n\nDefining and using functions\n\n\nDatatypes and match\n\n\nHakaru supports a few built-in datatypes, and offers functionality for\ntaking them apart and reconstructing them.\n\n\nArrays and loops\n\n\nWe offer special support for arrays, and for probability\ndistributions over arrays.\nWe also express loops that compute sums and products.\n\n\nTransformations\n\n\nHakaru implements its inference algorithms predominately as\nprogram transformations. The following are the major ones\nour system provides.\n\n\nExpect\n\n\nComputing expectation of a measure\n\n\nDisintegrate\n\n\nA transformation which takes a joint distribution and\nproduces a program representing the conditional distribution.\n\n\nSimplify\n\n\nAny Hakaru expression can be simplified, using\nthe Maple computer-algebra system.\n\n\nMetropolis Hastings\n\n\nAutomatically transform a measure into a transition kernel usable\nin a Metropolis Hastings algorithm.\n\n\nCompiling to Haskell\n\n\nCompiling to C\n\n\nInternals\n\n\nThe internals section of the manual provides some insight into how\nHakaru is implemented and offers guidance into how the system can\nbe extended.\n\n\nAST\n\n\nABT\n\n\nDatums\n\n\nCoercions\n\n\nTransformations\n\n\nTesting\n\n\nAdding a Language Feature",
            "title": "Home"
        },
        {
            "location": "/#overview",
            "text": "This manual provides a guide for how to use Hakaru.",
            "title": "Overview"
        },
        {
            "location": "/#introduction",
            "text": "What is Probabilistic Programming  Probabilistic programming systems allow us to write programs which\ndescribe probability distributions, and provide mechanisms to\nsample and condition the distributions they represent on data. In\nthis page, we give a sense of the sorts of problems Hakaru is\ngreat at solving, and how you would describe them in Hakaru.  Installation  Learn how to install Hakaru  Quickstart  Get started with this quickstart page. Where we show\nhow to sample and condition from a small Hakaru program.",
            "title": "Introduction"
        },
        {
            "location": "/#examples",
            "text": "Here we go through several more involved examples of the kinds of\nproblems Hakaru is uniquely well-suited to solve.  In particular, we describe a model for Gaussian Mixture Models and\nusing a form of Bayesian Naives Bayes as applied to document\nclassification.",
            "title": "Examples"
        },
        {
            "location": "/#language-guide",
            "text": "The language section provides an overview of the syntax of Hakaru as\nwell as some of the primitives in the language.  Random Primitives  These are the built-in probability distributions.  Let and Bind  This is how we can give names to subexpressions and a\ndraw from a probability distribution.  Conditionals  Hakaru supports a restricted  if  expression  Types and Coercions  Hakaru is a simply-typed language. This section\ndescribes the types available and functions for\nmoving between them.  Functions  Defining and using functions  Datatypes and match  Hakaru supports a few built-in datatypes, and offers functionality for\ntaking them apart and reconstructing them.  Arrays and loops  We offer special support for arrays, and for probability\ndistributions over arrays.\nWe also express loops that compute sums and products.",
            "title": "Language Guide"
        },
        {
            "location": "/#transformations",
            "text": "Hakaru implements its inference algorithms predominately as\nprogram transformations. The following are the major ones\nour system provides.  Expect  Computing expectation of a measure  Disintegrate  A transformation which takes a joint distribution and\nproduces a program representing the conditional distribution.  Simplify  Any Hakaru expression can be simplified, using\nthe Maple computer-algebra system.  Metropolis Hastings  Automatically transform a measure into a transition kernel usable\nin a Metropolis Hastings algorithm.  Compiling to Haskell  Compiling to C",
            "title": "Transformations"
        },
        {
            "location": "/#internals",
            "text": "The internals section of the manual provides some insight into how\nHakaru is implemented and offers guidance into how the system can\nbe extended.  AST  ABT  Datums  Coercions  Transformations  Testing  Adding a Language Feature",
            "title": "Internals"
        },
        {
            "location": "/intro/probprog/",
            "text": "What is Probabilistic Programming?\n\n\nProbabilistic programs are programs which represent probability\ndistributions. For example, the program \npoisson(5)\n represents the\npoisson distribution with a rate of five. Why do we need a\nlanguage for describing probability distributions?\n\n\nThe world is intrinsically an uncertain place. When we try to predict\nwhat will happen in the world given some data we have collected, we\nare inherently engaging in some sort of probabilistic modeling. In\nprobabilistic modeling, we treat the quantity we wish to predict as a\nparameter, and then describe our data as some noisy function of this\nparameter. This function is called \nlikelihood\n, and depending on which\nstatistical regime you use can be used in predominately two ways.\n\n\nFor instance, we might want to estimate the average time it takes for\na bus to arrive at a stop, based on actual arrival times. In this situation,\nthe likelihood function would be:\n\n\n$$ x \\sim \\text{Poisson}(\\lambda) $$\n\n\nwhere $x$ is the actual arrival time, and $\\lambda$ is the quantity we\nwish to predict. In other words, this likelihood says our data is a\nnoisy measurement of the average waiting time which follows a Poisson\ndistribution. We can also represent this likelihood function as a\ndensity function which for a given choice of $\\lambda$ returns how\nlikely it is for $x$ to be generated under that parameter.\n\n\n$$ f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} $$\n\n\nUnder a frequentist regime we perform maximum likelihood, where we find\nthe best parameter by finding the $\\lambda$ which maximizes $f$.\n\n\nUnder a Bayesian regime, we don\u2019t estimate a single best value for the\nparameter. Instead we place a prior distribution on the parameters and\nestimate that posterior distribution conditioned on our data.\n\n\nIn Hakaru, it is possible to use either regime for solving your\nproblems.  We will call the distribution or program which describes\nour data the \nmodel\n.\n\n\nTug of War\n\n\nWe demonstrate the value of this problem-solving approach using a\nsimplified version of the\n\ntug of war\n\nexample from probmods.org. For this problem we will take a Bayesian\napproach to prediction.\n\n\nFor this problem, we have three friends, Alice, Bob and Carol who take\nturns playing a tug of war against each other and we\u2019d like to know\nwhich of them is the strongest. We can pose this problem as a\nprobabilistic program. In particular, we will try to predict who will\nwin match3 given we have observed who won the first two matches.\n\n\nWe can start by assuming each player\u2019s strength comes from a standard\nnormal distribution. Then we assume the strength they pull with some\nnormal distribution centered around their true strength, and the\nperson who pulled harder wins.\n\n\ndef pulls(strength real):\n    normal(strength, 1)\n\ndef winner(a real, b real):\n    a_pull <~ pulls(a)\n    b_pull <~ pulls(b)\n    return (a_pull > b_pull)\n\nalice <~ normal(0,1)\nbob   <~ normal(0,1)\ncarol <~ normal(0,1)\n\nmatch1 <~ winner(alice, bob)\nmatch2 <~ winner(bob, carol)\nmatch3 <~ winner(alice, carol)\n\n\n\n\nWe then restrict the set of events to only those where Alice won the\nfirst match and Bob won the second, and return the results of the\nthird match.\n\n\nif match1 && match2:\n   return match3\nelse:\n   reject. measure(bool)\n\n\n\n\nWe can then run the above model using hakaru, which shows that Alice\nis likely to win her match against Carol.\n\n\nhakaru tugofwar.hk | head -n 10000 | sort | uniq -c\n   3060 false\n   6940 true\n\n\n\n\nSimulation vs Inference\n\n\nOf course, in the above program we performed inference, by taking\nour model and throwing out all events that didn\u2019t agree with\nthe data we had. How well would this work if we changed our\nmodel slightly? Suppose our data wasn\u2019t boolean values, but instead\nthe difference of strengths, and we want to not just whether Alice\nwill win, but by how much.\n\n\nAs we pose more complex questions, posing our models as rejection\nsamplers becomes increasing inefficient. Instead we would like to\ndirectly transform our models into those which only generate the\ndata we observed and don\u2019t waste any computation simulating data\nwhich we know will never exist.\n\n\nMetropolis Hastings\n\n\nBy default hakaru performs importance sampling. This works well\nfor inference in low dimensions, but we want to use MCMC for\nmore realistic problems. Hakaru provides a \nmh\n command tool\nto transform probabilistic programs into a Markov Chain.",
            "title": "What is Probabilistic Programming"
        },
        {
            "location": "/intro/probprog/#what-is-probabilistic-programming",
            "text": "Probabilistic programs are programs which represent probability\ndistributions. For example, the program  poisson(5)  represents the\npoisson distribution with a rate of five. Why do we need a\nlanguage for describing probability distributions?  The world is intrinsically an uncertain place. When we try to predict\nwhat will happen in the world given some data we have collected, we\nare inherently engaging in some sort of probabilistic modeling. In\nprobabilistic modeling, we treat the quantity we wish to predict as a\nparameter, and then describe our data as some noisy function of this\nparameter. This function is called  likelihood , and depending on which\nstatistical regime you use can be used in predominately two ways.  For instance, we might want to estimate the average time it takes for\na bus to arrive at a stop, based on actual arrival times. In this situation,\nthe likelihood function would be:  $$ x \\sim \\text{Poisson}(\\lambda) $$  where $x$ is the actual arrival time, and $\\lambda$ is the quantity we\nwish to predict. In other words, this likelihood says our data is a\nnoisy measurement of the average waiting time which follows a Poisson\ndistribution. We can also represent this likelihood function as a\ndensity function which for a given choice of $\\lambda$ returns how\nlikely it is for $x$ to be generated under that parameter.  $$ f(\\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} $$  Under a frequentist regime we perform maximum likelihood, where we find\nthe best parameter by finding the $\\lambda$ which maximizes $f$.  Under a Bayesian regime, we don\u2019t estimate a single best value for the\nparameter. Instead we place a prior distribution on the parameters and\nestimate that posterior distribution conditioned on our data.  In Hakaru, it is possible to use either regime for solving your\nproblems.  We will call the distribution or program which describes\nour data the  model .",
            "title": "What is Probabilistic Programming?"
        },
        {
            "location": "/intro/probprog/#tug-of-war",
            "text": "We demonstrate the value of this problem-solving approach using a\nsimplified version of the tug of war \nexample from probmods.org. For this problem we will take a Bayesian\napproach to prediction.  For this problem, we have three friends, Alice, Bob and Carol who take\nturns playing a tug of war against each other and we\u2019d like to know\nwhich of them is the strongest. We can pose this problem as a\nprobabilistic program. In particular, we will try to predict who will\nwin match3 given we have observed who won the first two matches.  We can start by assuming each player\u2019s strength comes from a standard\nnormal distribution. Then we assume the strength they pull with some\nnormal distribution centered around their true strength, and the\nperson who pulled harder wins.  def pulls(strength real):\n    normal(strength, 1)\n\ndef winner(a real, b real):\n    a_pull <~ pulls(a)\n    b_pull <~ pulls(b)\n    return (a_pull > b_pull)\n\nalice <~ normal(0,1)\nbob   <~ normal(0,1)\ncarol <~ normal(0,1)\n\nmatch1 <~ winner(alice, bob)\nmatch2 <~ winner(bob, carol)\nmatch3 <~ winner(alice, carol)  We then restrict the set of events to only those where Alice won the\nfirst match and Bob won the second, and return the results of the\nthird match.  if match1 && match2:\n   return match3\nelse:\n   reject. measure(bool)  We can then run the above model using hakaru, which shows that Alice\nis likely to win her match against Carol.  hakaru tugofwar.hk | head -n 10000 | sort | uniq -c\n   3060 false\n   6940 true",
            "title": "Tug of War"
        },
        {
            "location": "/intro/probprog/#simulation-vs-inference",
            "text": "Of course, in the above program we performed inference, by taking\nour model and throwing out all events that didn\u2019t agree with\nthe data we had. How well would this work if we changed our\nmodel slightly? Suppose our data wasn\u2019t boolean values, but instead\nthe difference of strengths, and we want to not just whether Alice\nwill win, but by how much.  As we pose more complex questions, posing our models as rejection\nsamplers becomes increasing inefficient. Instead we would like to\ndirectly transform our models into those which only generate the\ndata we observed and don\u2019t waste any computation simulating data\nwhich we know will never exist.  Metropolis Hastings  By default hakaru performs importance sampling. This works well\nfor inference in low dimensions, but we want to use MCMC for\nmore realistic problems. Hakaru provides a  mh  command tool\nto transform probabilistic programs into a Markov Chain.",
            "title": "Simulation vs Inference"
        },
        {
            "location": "/intro/installation/",
            "text": "Installation\n\n\nInstall Hakaru by cloning the latest version from our Github repo\n\n\ngit clone https://github.com/hakaru-dev/hakaru\ncd hakaru\n\n\n\n\nHakaru can then be installed either with \ncabal install\n or \nstack install\n\n\nDue to a \nghc bug\n, Windows users\nusing GHC 7.10 and below need to install the logfloat library separately\n\n\ncabal install -j logfloat -f -useffi\ncd hakaru\ncabal install\n\n\n\n\nMaple extension\n\n\nWithin Hakaru, we use \nMaple\n to perform\ncomputer-algebra guided optimizations. To get access to these optimizations\nyou must have a licensed copy of Maple installed.\n\n\nIn addition to this, we must autoload some Maple libraries that come\nwith the system to access this functionality\n\n\nexport LOCAL_MAPLE=\"`which maple`\"\ncd hakaru/maple\nmaple update-archive.mpl\necho 'libname := \"/path-to-hakaru/hakaru/maple\",libname:' >> ~/.mapleinit\n\n\n\n\nUnder Windows the instructions become\n\n\nSETX LOCAL_MAPLE \"<path to Maple bin directory>\\cmaple.exe\"\ncd hakaru\\maple \ncmaple update-archive.mpl\necho 'libname := \"C:\\\\<path to hakaru>\\\\hakaru\\\\maple\",libname:' >> \"C:\\<path to maple>\\lib\\maple.ini\"\n\n\n\n\nIf the Maple extension has been properly installed running\n\n\necho \"normal(0,1)\" | simplify -\n\n\n\n\nshould return\n\n\nnormal(0, 1)\n\n\n\n\nIf the \nLOCAL_MAPLE\n environment variable is not set, then \nsimplify\n\ndefaults to invoking \nssh\n to access a remote installation of Maple.\nThe invocation is\n\n\n\"$MAPLE_SSH\" -l \"$MAPLE_USER\" \"$MAPLE_SERVER\" \"$MAPLE_COMMAND -q -t\"\n\n\n\n\nand defaults to\n\n\n/usr/bin/ssh -l ppaml karst.uits.iu.edu \"maple -q -t\"",
            "title": "Installation"
        },
        {
            "location": "/intro/installation/#installation",
            "text": "Install Hakaru by cloning the latest version from our Github repo  git clone https://github.com/hakaru-dev/hakaru\ncd hakaru  Hakaru can then be installed either with  cabal install  or  stack install  Due to a  ghc bug , Windows users\nusing GHC 7.10 and below need to install the logfloat library separately  cabal install -j logfloat -f -useffi\ncd hakaru\ncabal install",
            "title": "Installation"
        },
        {
            "location": "/intro/installation/#maple-extension",
            "text": "Within Hakaru, we use  Maple  to perform\ncomputer-algebra guided optimizations. To get access to these optimizations\nyou must have a licensed copy of Maple installed.  In addition to this, we must autoload some Maple libraries that come\nwith the system to access this functionality  export LOCAL_MAPLE=\"`which maple`\"\ncd hakaru/maple\nmaple update-archive.mpl\necho 'libname := \"/path-to-hakaru/hakaru/maple\",libname:' >> ~/.mapleinit  Under Windows the instructions become  SETX LOCAL_MAPLE \"<path to Maple bin directory>\\cmaple.exe\"\ncd hakaru\\maple \ncmaple update-archive.mpl\necho 'libname := \"C:\\\\<path to hakaru>\\\\hakaru\\\\maple\",libname:' >> \"C:\\<path to maple>\\lib\\maple.ini\"  If the Maple extension has been properly installed running  echo \"normal(0,1)\" | simplify -  should return  normal(0, 1)  If the  LOCAL_MAPLE  environment variable is not set, then  simplify \ndefaults to invoking  ssh  to access a remote installation of Maple.\nThe invocation is  \"$MAPLE_SSH\" -l \"$MAPLE_USER\" \"$MAPLE_SERVER\" \"$MAPLE_COMMAND -q -t\"  and defaults to  /usr/bin/ssh -l ppaml karst.uits.iu.edu \"maple -q -t\"",
            "title": "Maple extension"
        },
        {
            "location": "/intro/quickstart/",
            "text": "Quickstart\n\n\nAssuming you have Hakaru \ninstalled\n, let\u2019s\nsample a simple a model.\n\n\nx <~ bern(0.5)\ny <~ match x:\n      true:  normal(0,1)\n      false: uniform(0,1)\nreturn (y,x)\n\n\n\n\nThe generative model here has us flip a coin with bias 0.5, and then\nhave \nx\n be a draw from that distribution. We then check if \nx\n is\ntrue or false. Based on that we either have \ny\n be a draw from\na normal or uniform distribution, and then we return both \nx\n and \ny\n.\nBecause we are choosing between a normal and a uniform distribution,\nprograms like these are sometimes called \nmixture\n models.\n\n\nAssuming we save this file to \ntwomixture.hk\n we can sample from it by\npassing it as an argument to the \nhakaru\n command. \n\n\nhakaru twomixture.hk\n\n\n\n\nHakaru will then produce an infinite stream of samples from the\ndistribution this program represents.\n\n\n(0.8614855008328531, false)\n(0.27145378737815007, false)\n(6.137461559047042e-4, false)\n(0.9699201771404777, true)\n(1.2904529857533733, true)\n(8.605226081336681e-2, false)\n(-0.7713069511457459, true)\n(0.18162205213257607, true)\n(-1.143049106224509, true)\n(0.3667084406816875, false)\n...\n\n\n\n\nOf course, Hakaru wouldn\u2019t be very interesting if that was all it\ndid. Often what we wish to do is condition a distribution on\ndata. Suppose for \ntwomixture.hk\n we knew \ny\n, and would like to\nsample \nx\n conditioned on this information. We can symbolically\nproduce the unnormalized conditional distribution, which we call the\n\ndisintegration\n of the program.\n\n\ndisintegrate twomixture.hk\n\n\n\n\nThis returns\n\n\nfn x2 real: \n weight(0.5,\n        weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n                1.0 * \n                recip(natroot((2.0 * pi), 2))),\n               x = true\n               return x)) <|> \n weight(0.5,\n        match (not((x2 < 0.0)) && not((1.0 < x2))): \n         true: \n          x = false\n          return x\n         false: reject. measure(bool))\n\n\n\n\nDisintegrate returns a function, to make it easier to sample\nfrom, we\u2019ll give a value for x2. We\u2019ll call this file\n\ntwomixture2.hk\n\n\nx2 = 0.3\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x)) <|> \nweight(0.5,\n    match (not((x2 < 0.0)) && not((1.0 < x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))\n\n\n\n\nWhich we can run through some unix commands to get a sense of\nthe distribution\n\n\nhakaru twomixture2.hk | head -n 1000 | sort | uniq -c\n\n    526 false\n    474 true\n\n\n\n\nAs we can see, when x2 = 0.3, the uniform distribution is slightly more\nlikely. If we change x2 to be 3.0\n\n\nx2 = 3.0\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x)) <|> \nweight(0.5,\n    match (not((x2 < 0.0)) && not((1.0 < x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))\n\n\n\n\nThis reflects that only the normal case is possible.\n\n\nhakaru twomixture3.hk | head -n 1000 | sort | uniq -c\n\n    1000 true",
            "title": "Quick Example"
        },
        {
            "location": "/intro/quickstart/#quickstart",
            "text": "Assuming you have Hakaru  installed , let\u2019s\nsample a simple a model.  x <~ bern(0.5)\ny <~ match x:\n      true:  normal(0,1)\n      false: uniform(0,1)\nreturn (y,x)  The generative model here has us flip a coin with bias 0.5, and then\nhave  x  be a draw from that distribution. We then check if  x  is\ntrue or false. Based on that we either have  y  be a draw from\na normal or uniform distribution, and then we return both  x  and  y .\nBecause we are choosing between a normal and a uniform distribution,\nprograms like these are sometimes called  mixture  models.  Assuming we save this file to  twomixture.hk  we can sample from it by\npassing it as an argument to the  hakaru  command.   hakaru twomixture.hk  Hakaru will then produce an infinite stream of samples from the\ndistribution this program represents.  (0.8614855008328531, false)\n(0.27145378737815007, false)\n(6.137461559047042e-4, false)\n(0.9699201771404777, true)\n(1.2904529857533733, true)\n(8.605226081336681e-2, false)\n(-0.7713069511457459, true)\n(0.18162205213257607, true)\n(-1.143049106224509, true)\n(0.3667084406816875, false)\n...  Of course, Hakaru wouldn\u2019t be very interesting if that was all it\ndid. Often what we wish to do is condition a distribution on\ndata. Suppose for  twomixture.hk  we knew  y , and would like to\nsample  x  conditioned on this information. We can symbolically\nproduce the unnormalized conditional distribution, which we call the disintegration  of the program.  disintegrate twomixture.hk  This returns  fn x2 real: \n weight(0.5,\n        weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n                1.0 * \n                recip(natroot((2.0 * pi), 2))),\n               x = true\n               return x)) <|> \n weight(0.5,\n        match (not((x2 < 0.0)) && not((1.0 < x2))): \n         true: \n          x = false\n          return x\n         false: reject. measure(bool))  Disintegrate returns a function, to make it easier to sample\nfrom, we\u2019ll give a value for x2. We\u2019ll call this file twomixture2.hk  x2 = 0.3\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x)) <|> \nweight(0.5,\n    match (not((x2 < 0.0)) && not((1.0 < x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))  Which we can run through some unix commands to get a sense of\nthe distribution  hakaru twomixture2.hk | head -n 1000 | sort | uniq -c\n\n    526 false\n    474 true  As we can see, when x2 = 0.3, the uniform distribution is slightly more\nlikely. If we change x2 to be 3.0  x2 = 3.0\nweight(0.5,\n    weight((exp((negate(((x2 + 0.0) ^ 2)) * 0.5)) * \n        1.0 * \n            recip(natroot((2.0 * pi), 2))),\n            x = true\n            return x)) <|> \nweight(0.5,\n    match (not((x2 < 0.0)) && not((1.0 < x2))): \n        true:\n         x = false\n         return x\n        false: reject. measure(bool))  This reflects that only the normal case is possible.  hakaru twomixture3.hk | head -n 1000 | sort | uniq -c\n\n    1000 true",
            "title": "Quickstart"
        },
        {
            "location": "/lang/rand/",
            "text": "Primitive Probability Distributions\n\n\nHakaru comes with a small set of primitive probability\ndistributions.\n\n\n\n\n\n\n\n\nnormal(mean. \nreal\n, standard_deviation. \nprob\n): \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nunivariate Normal (Gaussian) distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nuniform(low. \nreal\n, high. \nreal\n): \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nUniform distribution is a continuous univariate distribution defined from low to high\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngamma(shape. \nprob\n, scale. \nprob\n): \nmeasure(prob)\n \n\n\n\n\n\n\n\n\n\n\nGamma distribution with shape and scale parameterization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbeta(a. \nprob\n, b. \nprob\n): \nmeasure(prob)\n \n\n\n\n\n\n\n\n\n\n\nBeta distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npoisson(l. \nprob\n): \nmeasure(nat)\n \n\n\n\n\n\n\n\n\n\n\nPoisson distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncategorical(v. \narray(prob)\n): \nmeasure(nat)\n \n\n\n\n\n\n\n\n\n\n\nCategorical distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndirac(x. \na\n): \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\nDirac distribution\n\n\n\n\n\n\n\n\nThe Dirac distribution appears often enough, that we have given an\nadditional keyword in our language for it: \nreturn\n. The following\nprograms are equivalent.\n\n\ndirac(3)\n\n\n\n\nreturn 3\n\n\n\n\n\n\n\n\n\n\nlebesgue: \nmeasure(real)\n \n\n\n\n\n\n\n\n\n\n\nthe distribution constant over the real line\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweight(x. \nprob\n, m. \nmeasure(a)\n): \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\na \nm\n distribution, reweighted by \nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nreject: \nmeasure(a)\n \n\n\n\n\n\n\n\n\n\n\nThe distribution over the empty set\n\n\n\n\n\n\n\n\nFinally, we have a binary choice operator \n<|>\n, which takes two\ndistributions, and returns an unnormalized distribution which returns\none or the other.  For example, to get a distribution which where with\nprobability 0.5 draws from a uniform(0,1), and probability 0.5 draws\nfrom uniform(5,6).\n\n\nweight(0.5, uniform(0,1)) <|>\nweight(0.5, uniform(5,6))",
            "title": "Random Primitives"
        },
        {
            "location": "/lang/rand/#primitive-probability-distributions",
            "text": "Hakaru comes with a small set of primitive probability\ndistributions.     normal(mean.  real , standard_deviation.  prob ):  measure(real)        univariate Normal (Gaussian) distribution        uniform(low.  real , high.  real ):  measure(real)        Uniform distribution is a continuous univariate distribution defined from low to high        gamma(shape.  prob , scale.  prob ):  measure(prob)        Gamma distribution with shape and scale parameterization        beta(a.  prob , b.  prob ):  measure(prob)        Beta distribution        poisson(l.  prob ):  measure(nat)        Poisson distribution        categorical(v.  array(prob) ):  measure(nat)        Categorical distribution        dirac(x.  a ):  measure(a)        Dirac distribution     The Dirac distribution appears often enough, that we have given an\nadditional keyword in our language for it:  return . The following\nprograms are equivalent.  dirac(3)  return 3     lebesgue:  measure(real)        the distribution constant over the real line        weight(x.  prob , m.  measure(a) ):  measure(a)        a  m  distribution, reweighted by  x        reject:  measure(a)        The distribution over the empty set     Finally, we have a binary choice operator  <|> , which takes two\ndistributions, and returns an unnormalized distribution which returns\none or the other.  For example, to get a distribution which where with\nprobability 0.5 draws from a uniform(0,1), and probability 0.5 draws\nfrom uniform(5,6).  weight(0.5, uniform(0,1)) <|>\nweight(0.5, uniform(5,6))",
            "title": "Primitive Probability Distributions"
        },
        {
            "location": "/lang/letbind/",
            "text": "Let and Bind\n\n\nIn Hakaru, we can give names for expressions to our programs with \n=\n,\nwhich we call \nLet\n. This gives us the ability to share computation\nthat might be needed in the program.\n\n\nx = 2\nx + 3\n\n\n\n\nWe can use \n=\n to give a name to any expression in our language. The\nname you assign is in scope for the rest of the body it was defined in.\n\n\nBind\n\n\nHakaru also has the operator \n<~\n. This operator, which call \nBind\n\ncan only be used with expressions that denote probability distributions.\nBind allows us to talk about draws from a distribution using a name for\nany particular value that could have come from that distribution.\n\n\n# Bad\nx <~ 2 + 3\nx\n\n\n\n\n# Good\nx <~ normal(0,1)\nreturn x\n\n\n\n\nBecause Bind is about draws from a distribution, the rest of the body\nmust also denote a probability distribution.\n\n\n# Bad\nx <~ normal(0,1)\nx\n\n\n\n\n# Good\nx <~ normal(0,1)\nreturn x\n\n\n\n\nTo help distinguish Let and Bind. Here is a probabilistic program, where we\nlet \nf\n be equal to the normal distribution, and take draws from \nf\n.\n\n\nf = normal(0,1)\nx <~ f\nreturn x*x",
            "title": "Let and Bind"
        },
        {
            "location": "/lang/letbind/#let-and-bind",
            "text": "In Hakaru, we can give names for expressions to our programs with  = ,\nwhich we call  Let . This gives us the ability to share computation\nthat might be needed in the program.  x = 2\nx + 3  We can use  =  to give a name to any expression in our language. The\nname you assign is in scope for the rest of the body it was defined in.",
            "title": "Let and Bind"
        },
        {
            "location": "/lang/letbind/#bind",
            "text": "Hakaru also has the operator  <~ . This operator, which call  Bind \ncan only be used with expressions that denote probability distributions.\nBind allows us to talk about draws from a distribution using a name for\nany particular value that could have come from that distribution.  # Bad\nx <~ 2 + 3\nx  # Good\nx <~ normal(0,1)\nreturn x  Because Bind is about draws from a distribution, the rest of the body\nmust also denote a probability distribution.  # Bad\nx <~ normal(0,1)\nx  # Good\nx <~ normal(0,1)\nreturn x  To help distinguish Let and Bind. Here is a probabilistic program, where we\nlet  f  be equal to the normal distribution, and take draws from  f .  f = normal(0,1)\nx <~ f\nreturn x*x",
            "title": "Bind"
        },
        {
            "location": "/lang/cond/",
            "text": "Conditionals\n\n\nHakaru supports an \nif\n expression. This if must have two\nbodies. There exists no special syntax for \nelse if\n like\nyou might find in Python.\n\n\na  = 4\nb  = 5\nif a > b:\n   a + 1\nelse:\n   b - 2",
            "title": "Conditionals"
        },
        {
            "location": "/lang/cond/#conditionals",
            "text": "Hakaru supports an  if  expression. This if must have two\nbodies. There exists no special syntax for  else if  like\nyou might find in Python.  a  = 4\nb  = 5\nif a > b:\n   a + 1\nelse:\n   b - 2",
            "title": "Conditionals"
        },
        {
            "location": "/lang/coercions/",
            "text": "Types and Coercions\n\n\nHakaru is a simply-typed language which has\na few basic types and some more complicated\nones which can be built out of simpler types.\n\n\nTypes\n\n\n\n\nnat is the type for natural numbers. This includes zero.\n\n\nint is the integer type.\n\n\nprob is the type for positive real number. This includes zero.\n\n\nreal is the type for real numbers.\n\n\narray(x) is the type for arrays where each element is type x\n\n\nmeasure(x) is the type for probability distributions whose\n  sample space is type x\n\n\n\n\nCoercions\n\n\nFor the primitive numeric types we also offer coercion functions.\n\n\n\n\nprob2real\n\n\nint2real\n\n\nnat2int\n\n\nreal2prob\n\n\nreal2int\n\n\nint2nat\n\n\n\n\nFor the ones which are always safe to apply such as \nnat2int\n we will\nautomatically insert them if it is required for the program to typecheck.",
            "title": "Coercions"
        },
        {
            "location": "/lang/coercions/#types-and-coercions",
            "text": "Hakaru is a simply-typed language which has\na few basic types and some more complicated\nones which can be built out of simpler types.",
            "title": "Types and Coercions"
        },
        {
            "location": "/lang/coercions/#types",
            "text": "nat is the type for natural numbers. This includes zero.  int is the integer type.  prob is the type for positive real number. This includes zero.  real is the type for real numbers.  array(x) is the type for arrays where each element is type x  measure(x) is the type for probability distributions whose\n  sample space is type x",
            "title": "Types"
        },
        {
            "location": "/lang/coercions/#coercions",
            "text": "For the primitive numeric types we also offer coercion functions.   prob2real  int2real  nat2int  real2prob  real2int  int2nat   For the ones which are always safe to apply such as  nat2int  we will\nautomatically insert them if it is required for the program to typecheck.",
            "title": "Coercions"
        },
        {
            "location": "/lang/functions/",
            "text": "Functions\n\n\nFunctions can be defined using a Python-inspired style syntax. One\nnotable difference is that each argument must be followed by its\ntype.\n\n\ndef add(x real, y real):\n    x + y\n\nadd(4,5)\n\n\n\n\nWe may optionally provide a type for the return value of a function if\nwe wish.\n\n\ndef add(x. real, y. real) real:\n    x + y\n\nadd(4,5)\n\n\n\n\nAnonymous functions\n\n\nIf you don\u2019t wish to name your functions, we also offer a syntax\nfor anonymous functions. These only take on argument and must be\ngiven a type alongside the variable name.\n\n\nfn x real: x + 1\n\n\n\n\nInternally, there are only one argument anonymous functions, and\nlets. The first example is equivalent to the following.\n\n\nadd = fn x real:\n         fn y real:\n            x + y\nadd(4,5)",
            "title": "Functions and Let"
        },
        {
            "location": "/lang/functions/#functions",
            "text": "Functions can be defined using a Python-inspired style syntax. One\nnotable difference is that each argument must be followed by its\ntype.  def add(x real, y real):\n    x + y\n\nadd(4,5)  We may optionally provide a type for the return value of a function if\nwe wish.  def add(x. real, y. real) real:\n    x + y\n\nadd(4,5)",
            "title": "Functions"
        },
        {
            "location": "/lang/functions/#anonymous-functions",
            "text": "If you don\u2019t wish to name your functions, we also offer a syntax\nfor anonymous functions. These only take on argument and must be\ngiven a type alongside the variable name.  fn x real: x + 1  Internally, there are only one argument anonymous functions, and\nlets. The first example is equivalent to the following.  add = fn x real:\n         fn y real:\n            x + y\nadd(4,5)",
            "title": "Anonymous functions"
        },
        {
            "location": "/lang/datatypes/",
            "text": "Data types and Match\n\n\nHakaru with several built-in data types.\n\n\n\n\npair\n\n\nunit\n\n\neither\n\n\nbool\n\n\n\n\nMatch\n\n\nWe use \nmatch\n to deconstruct out data types\nand access their elements.\n\n\nmatch left(3). either(int,bool):\n  left(x) : 1\n  right(x): 2\n\n\n\n\nWe do include special syntax for pairs\n\n\nmatch (1,2):\n  (x,y): x + y",
            "title": "Datatypes and match"
        },
        {
            "location": "/lang/datatypes/#data-types-and-match",
            "text": "Hakaru with several built-in data types.   pair  unit  either  bool",
            "title": "Data types and Match"
        },
        {
            "location": "/lang/datatypes/#match",
            "text": "We use  match  to deconstruct out data types\nand access their elements.  match left(3). either(int,bool):\n  left(x) : 1\n  right(x): 2  We do include special syntax for pairs  match (1,2):\n  (x,y): x + y",
            "title": "Match"
        },
        {
            "location": "/lang/arrays/",
            "text": "Arrays and Plate\n\n\nHakaru provides special syntax for arrays, which\nis distinct from the other data types.\n\n\nArrays\n\n\nTo construct arrays, we provide an index variable, size argument, and\nan expression body. This body is evaluated for each index of the\narray. For example, to construct the array \n[0,1,2,3]\n:\n\n\narray i of 4: i\n\n\n\n\nArray Literals\n\n\nWe can also create arrays using the literal syntax a comma delimited\nlist surrounded by brackets: \n[0,1,2,3]\n\n\nPlate\n\n\nBeyond, arrays Hakaru includes special syntax for describing measures\nover arrays called \nplate\n. Plate using the same syntax as \narray\n but\nthe body must have a measure type. It returns a measure over arrays.\nFor example, if we wish to have a distribution over three independent\nnormal distributions we would do so as follows:\n\n\nplate _ of 3: normal(0,1)\n\n\n\n\nArray size and indexing\n\n\nIf \na\n is an array, then \nsize(a)\n is its number of elements, which is a \nnat\n.\nIf \ni\n is a \nnat\n then \na[i]\n is the element of \na\n at index \ni\n.\nIndices start at zero, so the maximum valid value of \ni\n is \nsize(a)-1\n.\n\n\nLoops\n\n\nWe also express loops that compute sums (\nsummate\n) and products (\nproduct\n).\nThe syntax of these loops begins by declaring an \ninclusive\n lower bound and\nan \nexclusive\n upper bound.  For example, the factorial of \nn\n is not\n\nproduct i from 1 to n: i\n but rather \nproduct i from 1 to n+1: i\n.\nThis convention takes some getting used to but it makes it easy to deal\nwith arrays.  For example, if \na\n is an array of numbers then their sum is\n\nsummate i from 0 to size(a): a[i]\n.",
            "title": "Arrays"
        },
        {
            "location": "/lang/arrays/#arrays-and-plate",
            "text": "Hakaru provides special syntax for arrays, which\nis distinct from the other data types.",
            "title": "Arrays and Plate"
        },
        {
            "location": "/lang/arrays/#arrays",
            "text": "To construct arrays, we provide an index variable, size argument, and\nan expression body. This body is evaluated for each index of the\narray. For example, to construct the array  [0,1,2,3] :  array i of 4: i",
            "title": "Arrays"
        },
        {
            "location": "/lang/arrays/#array-literals",
            "text": "We can also create arrays using the literal syntax a comma delimited\nlist surrounded by brackets:  [0,1,2,3]",
            "title": "Array Literals"
        },
        {
            "location": "/lang/arrays/#plate",
            "text": "Beyond, arrays Hakaru includes special syntax for describing measures\nover arrays called  plate . Plate using the same syntax as  array  but\nthe body must have a measure type. It returns a measure over arrays.\nFor example, if we wish to have a distribution over three independent\nnormal distributions we would do so as follows:  plate _ of 3: normal(0,1)",
            "title": "Plate"
        },
        {
            "location": "/lang/arrays/#array-size-and-indexing",
            "text": "If  a  is an array, then  size(a)  is its number of elements, which is a  nat .\nIf  i  is a  nat  then  a[i]  is the element of  a  at index  i .\nIndices start at zero, so the maximum valid value of  i  is  size(a)-1 .",
            "title": "Array size and indexing"
        },
        {
            "location": "/lang/arrays/#loops",
            "text": "We also express loops that compute sums ( summate ) and products ( product ).\nThe syntax of these loops begins by declaring an  inclusive  lower bound and\nan  exclusive  upper bound.  For example, the factorial of  n  is not product i from 1 to n: i  but rather  product i from 1 to n+1: i .\nThis convention takes some getting used to but it makes it easy to deal\nwith arrays.  For example, if  a  is an array of numbers then their sum is summate i from 0 to size(a): a[i] .",
            "title": "Loops"
        },
        {
            "location": "/transforms/expect/",
            "text": "Expectation transformation\n\n\nThe expectation transformation takes a program representing a measure,\nand a function over the sample space, and returns a program computing\nthe expectation over that measure with respect to the given function.\n\n\nExpect\n\n\nExpect can be used inside programs with the \nexpect\n keyword.\n\n\nexpect x uniform(1,3):\n    real2prob(2*x + 1)\n\n\n\n\nThis program computes the expectation of \nuniform(1,3)\n using the\nfunction \n2*x + 1\n. This program expands to the following equivalent\nprogram:\n\n\nintegrate x from 1 to 3: \n recip(real2prob(3 - 1)) * real2prob(2*x + 1)\n\n\n\n\nThis can be optimized by piping by it into the \nsimplify\n program. It\nwill in turn return \n5\n.\n\n\nNormalize\n\n\nWe also provide a \nnormalize\n command. This command takes as input a\nprogram representing any measure and reweights it into a program\nrepresenting a probability distribution.\n\n\nFor example in a slightly contrived example, we can weight a normal\ndistribution by two. Normalizing it will then remove this weight.\n\n\n> echo \"weight(2, normal(0,1))\" | normalize | simplify -\nnormal(0, 1)",
            "title": "Expect"
        },
        {
            "location": "/transforms/expect/#expectation-transformation",
            "text": "The expectation transformation takes a program representing a measure,\nand a function over the sample space, and returns a program computing\nthe expectation over that measure with respect to the given function.",
            "title": "Expectation transformation"
        },
        {
            "location": "/transforms/expect/#expect",
            "text": "Expect can be used inside programs with the  expect  keyword.  expect x uniform(1,3):\n    real2prob(2*x + 1)  This program computes the expectation of  uniform(1,3)  using the\nfunction  2*x + 1 . This program expands to the following equivalent\nprogram:  integrate x from 1 to 3: \n recip(real2prob(3 - 1)) * real2prob(2*x + 1)  This can be optimized by piping by it into the  simplify  program. It\nwill in turn return  5 .",
            "title": "Expect"
        },
        {
            "location": "/transforms/expect/#normalize",
            "text": "We also provide a  normalize  command. This command takes as input a\nprogram representing any measure and reweights it into a program\nrepresenting a probability distribution.  For example in a slightly contrived example, we can weight a normal\ndistribution by two. Normalizing it will then remove this weight.  > echo \"weight(2, normal(0,1))\" | normalize | simplify -\nnormal(0, 1)",
            "title": "Normalize"
        },
        {
            "location": "/transforms/disintegrate/",
            "text": "Disintegrations transformation\n\n\nThe disintegration transformation takes as input a program\nrepresenting a joint probability distribution, and returns\na program which represents an posterior distribution.\n\n\nFor example, if we have the following joint distribution \nhello.hk\n\n\n\u03b8 <~ normal(0,1)\nx <~ normal(\u03b8,1)\nreturn (x,\u03b8)\n\n\n\n\nWhen we call \ndisintegrate hello.hk\n we obtain:\n\n\nfn x2 real: \n \u03b8 <~ normal(0, 1)\n x7 <~ weight((exp((negate(((x2 - \u03b8) ^ 2)) / 2))\n                / \n               1\n                / \n               sqrt((2 * pi))),\n              return ())\n return \u03b8\n\n\n\n\nThis represents the posterior on \n\u03b8\n given a value of \nx\n which\nhas been renamed \nx2\n.\n\n\nDensity\n\n\nFinding the density of a probability distribution at a particular\npoint is actually a special-case of disintegrate and is\ndefined in terms of it and the expectation transformation.\n\n\nWe also have a command \ndensity\n for doing so.\n\n\necho \"normal(0,1)\" | density -\n\nfn x0 real: \n (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1)",
            "title": "Disintegrate"
        },
        {
            "location": "/transforms/disintegrate/#disintegrations-transformation",
            "text": "The disintegration transformation takes as input a program\nrepresenting a joint probability distribution, and returns\na program which represents an posterior distribution.  For example, if we have the following joint distribution  hello.hk  \u03b8 <~ normal(0,1)\nx <~ normal(\u03b8,1)\nreturn (x,\u03b8)  When we call  disintegrate hello.hk  we obtain:  fn x2 real: \n \u03b8 <~ normal(0, 1)\n x7 <~ weight((exp((negate(((x2 - \u03b8) ^ 2)) / 2))\n                / \n               1\n                / \n               sqrt((2 * pi))),\n              return ())\n return \u03b8  This represents the posterior on  \u03b8  given a value of  x  which\nhas been renamed  x2 .",
            "title": "Disintegrations transformation"
        },
        {
            "location": "/transforms/disintegrate/#density",
            "text": "Finding the density of a probability distribution at a particular\npoint is actually a special-case of disintegrate and is\ndefined in terms of it and the expectation transformation.  We also have a command  density  for doing so.  echo \"normal(0,1)\" | density -\n\nfn x0 real: \n (exp((negate(((x0 + 0) ^ 2)) / 2)) / 1 / sqrt((2 * pi)) / 1)",
            "title": "Density"
        },
        {
            "location": "/transforms/simplify/",
            "text": "Simplify transformation\n\n\nThe simplify transformation provides a way to automaticaly improve our\nprograms. Simplify works by turning our programs into their expectation\nrepresentation and sending to Maple to be algebraically-simplified.\n\n\nFor example, the following represents a program from values of type\n\nprob\n to a measure of real numbers.\n\n\nfn a prob:\n  x <~ normal(a,1)\n  y <~ normal(x,1)\n  z <~ normal(y,1)\n  return z\n\n\n\n\nAnd it will simplify to the following equivalent program.\n\n\nfn a prob: normal(prob2real(a), sqrt(3))",
            "title": "Simplify"
        },
        {
            "location": "/transforms/simplify/#simplify-transformation",
            "text": "The simplify transformation provides a way to automaticaly improve our\nprograms. Simplify works by turning our programs into their expectation\nrepresentation and sending to Maple to be algebraically-simplified.  For example, the following represents a program from values of type prob  to a measure of real numbers.  fn a prob:\n  x <~ normal(a,1)\n  y <~ normal(x,1)\n  z <~ normal(y,1)\n  return z  And it will simplify to the following equivalent program.  fn a prob: normal(prob2real(a), sqrt(3))",
            "title": "Simplify transformation"
        },
        {
            "location": "/transforms/mh/",
            "text": "Metropolis Hastings transform\n\n\nIn Hakaru, all inference algorithms are represented as program\ntransformations. In particular, the Metropolis-Hastings transform\ntakes as input a probabilistic program representing the target\ndistribution, and a probabilistic program representing the proposal\ndistribution and returns a probabilistic program representing the MH\ntransition kernel.\n\n\nmh command\n\n\nYou can access this functionality using the \nmh\n command. It takes\ntwo files as input representing the target distribution and proposal\nkernel.\n\n\nFor example, suppose we would like to make a Markov Chain for the\nnormal distribution, where the proposal distribution is a random walk.\n\n\nTarget\n\n\n# target.hk\nnormal(0,1)\n\n\n\n\nProposal\n\n\n# proposal.hk\nfn x real: normal(x, 0.04)\n\n\n\n\nWe can use \nmh\n to create a transition kernel.\n\n\nmh target.hk proposal.hk\n\nx5 = x2 = fn x0 real: \n           (exp((negate(((x0 - nat2real(0)) ^ 2))\n                  / \n                 prob2real((2 * (nat2prob(1) ^ 2)))))\n             / \n            nat2prob(1)\n             / \n            sqrt((2 * pi))\n             / \n            1)\n     fn x1 real: \n      x0 <~ normal(x1, (1/25))\n      return (x0, (x2(x0) / x2(x1)))\nfn x4 real: \n x3 <~ x5(x4)\n (match x3: \n   (x1, x2): \n    x0 <~ weight(min(1, x2), return true) <|> \n          weight(real2prob((prob2real(1) - prob2real(min(1, x2)))),\n                 return false)\n    return (match x0: \n             true: x1\n             false: x4))\n\n\n\n\n\nThis can then be simplified.\n\n\nmh target.hk proposal.hk | simplify -\n\nfn x4 real: \n x03 <~ normal(x4, (1/25))\n weight(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))),\n        return x03) <|> \n weight(real2prob((1\n                    + \n                   (prob2real(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))))\n                     * \n                    (-1)))),\n        return x4)\n\n\n\n\nThis can then be run using \nhakaru\n. Hakaru when run with two\narguments will assume that the first file is a transition kernel,\nand the second file represents a measure to initialize from.\n\n\nmh target.hk proposal.hk | simplify - | hakaru - target.hk | head\n\n-0.6133542972818671\n-0.6111567543723275\n-0.5963756142974966\n-0.5661156231637984\n-0.6280335079595971\n-0.616432866701967\n-0.6053631512209712\n-0.5964839795872353\n-0.6020821843203473\n-0.6535246137595148",
            "title": "Metropolis Hastings"
        },
        {
            "location": "/transforms/mh/#metropolis-hastings-transform",
            "text": "In Hakaru, all inference algorithms are represented as program\ntransformations. In particular, the Metropolis-Hastings transform\ntakes as input a probabilistic program representing the target\ndistribution, and a probabilistic program representing the proposal\ndistribution and returns a probabilistic program representing the MH\ntransition kernel.",
            "title": "Metropolis Hastings transform"
        },
        {
            "location": "/transforms/mh/#mh-command",
            "text": "You can access this functionality using the  mh  command. It takes\ntwo files as input representing the target distribution and proposal\nkernel.  For example, suppose we would like to make a Markov Chain for the\nnormal distribution, where the proposal distribution is a random walk.  Target  # target.hk\nnormal(0,1)  Proposal  # proposal.hk\nfn x real: normal(x, 0.04)  We can use  mh  to create a transition kernel.  mh target.hk proposal.hk\n\nx5 = x2 = fn x0 real: \n           (exp((negate(((x0 - nat2real(0)) ^ 2))\n                  / \n                 prob2real((2 * (nat2prob(1) ^ 2)))))\n             / \n            nat2prob(1)\n             / \n            sqrt((2 * pi))\n             / \n            1)\n     fn x1 real: \n      x0 <~ normal(x1, (1/25))\n      return (x0, (x2(x0) / x2(x1)))\nfn x4 real: \n x3 <~ x5(x4)\n (match x3: \n   (x1, x2): \n    x0 <~ weight(min(1, x2), return true) <|> \n          weight(real2prob((prob2real(1) - prob2real(min(1, x2)))),\n                 return false)\n    return (match x0: \n             true: x1\n             false: x4))  This can then be simplified.  mh target.hk proposal.hk | simplify -\n\nfn x4 real: \n x03 <~ normal(x4, (1/25))\n weight(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))),\n        return x03) <|> \n weight(real2prob((1\n                    + \n                   (prob2real(min(1, (exp(((x03 ^ 2) * -1/2)) * exp(((x4 ^ 2) / 2)))))\n                     * \n                    (-1)))),\n        return x4)  This can then be run using  hakaru . Hakaru when run with two\narguments will assume that the first file is a transition kernel,\nand the second file represents a measure to initialize from.  mh target.hk proposal.hk | simplify - | hakaru - target.hk | head\n\n-0.6133542972818671\n-0.6111567543723275\n-0.5963756142974966\n-0.5661156231637984\n-0.6280335079595971\n-0.616432866701967\n-0.6053631512209712\n-0.5964839795872353\n-0.6020821843203473\n-0.6535246137595148",
            "title": "mh command"
        },
        {
            "location": "/transforms/compile/",
            "text": "Compiling to Haskell\n\n\nHakaru can be compiled to Haskell using the \ncompile\n command.\n\n\nFor example if we wish to compile \nexample.hk\n\n\nx <~ normal(0,1)\ny <~ normal(x,1)\nreturn y\n\n\n\n\nWe call \ncompile example.hk\n, which produces a file \nexample.hs\n.\n\n\ncat example.hs\n\n\n\n\n{-# LANGUAGE DataKinds, NegativeLiterals #-}\nmodule Main where\n\nimport           Prelude                          hiding (product)\nimport           Language.Hakaru.Runtime.Prelude\nimport           Language.Hakaru.Types.Sing\nimport qualified System.Random.MWC                as MWC\nimport           Control.Monad\n\nprog = \n  normal (nat2real (nat_ 0)) (nat2prob (nat_ 1)) >>= \\ x0 ->\n  normal x0 (nat2prob (nat_ 1)) >>= \\ y1 ->\n  dirac y1\n\nmain :: IO ()\nmain = do\n  g <- MWC.createSystemRandom\n  forever $ run g prog\n\n\n\n\nThis is a regular Haskell file, which can then be furthered compiled into\nmachine code.",
            "title": "Compiling to Haskell"
        },
        {
            "location": "/transforms/compile/#compiling-to-haskell",
            "text": "Hakaru can be compiled to Haskell using the  compile  command.  For example if we wish to compile  example.hk  x <~ normal(0,1)\ny <~ normal(x,1)\nreturn y  We call  compile example.hk , which produces a file  example.hs .  cat example.hs  {-# LANGUAGE DataKinds, NegativeLiterals #-}\nmodule Main where\n\nimport           Prelude                          hiding (product)\nimport           Language.Hakaru.Runtime.Prelude\nimport           Language.Hakaru.Types.Sing\nimport qualified System.Random.MWC                as MWC\nimport           Control.Monad\n\nprog = \n  normal (nat2real (nat_ 0)) (nat2prob (nat_ 1)) >>= \\ x0 ->\n  normal x0 (nat2prob (nat_ 1)) >>= \\ y1 ->\n  dirac y1\n\nmain :: IO ()\nmain = do\n  g <- MWC.createSystemRandom\n  forever $ run g prog  This is a regular Haskell file, which can then be furthered compiled into\nmachine code.",
            "title": "Compiling to Haskell"
        },
        {
            "location": "/transforms/hkc/",
            "text": "HKC Compilation\n\n\nhkc\n is a command line tool to compiler Hakaru programs to C. HKC was\ncreated with portability and speed in mind. More recently, OpenMP support is\nbeing added to gain more performance on multi-core machines. Basic command line\nusage of HKC is much like other compilers:\n\n\nhkc foo.hk -o foo.c\n\n\n\n\nIt is possible to go straight to an executable with the \n--make ARG\n flag, where\nthe argument is the C compiler you would like to use.\n\n\nType Conversions\n\n\nThe types available in Hakaru programs are the following: \nnat\n, \nint\n, \nreal\n,\n\nprob\n, \narray(<type>)\n, \nmeasure(<type>)\n, and datum like \ntrue\n and \nfalse\n.\n\n\nnat\n and \nint\n have a trivial mapping to the C \nint\n type. \nreal\n becomes a C\n\ndouble\n. The \nprob\n type in Hakaru is stored in the log-domain to avoid\nunderflow. In C this corresponds to a \ndouble\n, but we first take the log of it\nbefore storing it, so we have to take the exp of it to bring it back to the real\nnumbers.\n\n\nArrays become structs that contain the size and a pointer to data stored within.\nThe structs are generated at compile time, but there are only four which are\nnamed after the type they contain. Here they all are:\n\n\nstruct arrayNat {\n  int size; int * data;\n};\n\nstruct arrayInt {\n  int size; int * data;\n};\n\nstruct arrayReal {\n  int size; double * data;\n};\n\nstruct arrayProb {\n  int size; double * data;\n};\n\n\n\n\nMeasures\n\n\nMeasures compile to C functions that take a location for a sample, return the\nweight of the measure and store a sample in the location is was given. A simple\nexample is \nuniform(0,1)\n a measure over type \nreal\n.\n\n\n#include <time.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\ndouble measure(double * s_a)\n {\n    *s_a = ((double)0) + ((double)rand()) / ((double)RAND_MAX) * ((double)1) - ((double)0);\n    return 0;\n }\n\nint main()\n {\n    double sample;\n    while (1)\n    {\n        measure(&sample);\n        printf(\"%.17f\\n\",sample);\n    }\n    return 0;\n }\n\n\n\n\nRecall that weights have type \nprob\n and are stored in the log-domain. This\nexample has a weight of 1.\n\n\nCalling \nhkc\n on a measure will create a function like the one above and also a\nmain function that infinitely takes samples. Using \nhkc -F ARG\n will produce\njust the function with the name of its argument.\n\n\nLambdas\n\n\nLambdas compile to functions in C:\n\n\nfn x array(real):\n  (summate i from 0 to size(x): x[i])\n   *\n  prob2real(recip(nat2prob((size(x) + 1))))\n\n\n\n\n\nBecomes:\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\nstruct arrayReal {\n   int size; double * data;\n };\n\ndouble fn_a(struct arrayReal x_b)\n {\n   unsigned int i_c;\n   double acc_d;\n   double p_e;\n   double _f;\n   double r_g;\n   acc_d = 0;\n   for (i_c = 0; i_c < x_b.size; i_c++)\n   {\n     acc_d += *(x_b.data + i_c);\n   }\n   p_e = log1p(((1 + x_b.size) - 1));\n   _f = -p_e;\n   r_g = (expm1(_f) + 1);\n   return (r_g * acc_d);\n }\n\n\n\n\nUsing the \n-F\n flag will allow the user to add their own name to a function,\notherwise the name is chosen automatically as \nfn_<unique identifier>\n.\n\n\nComputations\n\n\nWhen compiling a computation, HKC just creates a main function to compute the\nvalue and print it. For example:\n\n\nsummate i from 1 to 100000000:\n  nat2real(i) / nat2real(i)\n\n\n\n\nbecomes:\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\nint main()\n {\n    double result;\n    int i_a;\n    double acc_b;\n    double _c;\n    acc_b = 0;\n    for (i_a = 1; i_a < 100000000; i_a++)\n    {\n       _c = (1 / ((double)i_a));\n       acc_b += (_c * ((double)i_a));\n    }\n    result = acc_b;\n    printf(\"%.17f\\n\",result);\n    return 0;\n }\n\n\n\n\nParallel Programs\n\n\nCalling HKC with the \n-j\n flag will generate the code with parallel regions to\ncompute the value. The parallel code uses OpenMP directives. To check if you\u2019re\ncompiler supports OpenMP, check \nhere\n.\n\n\nFor example, GCC requires the \n-fopenmp\n flag for OpenMP support:\n\n\nhkc -j foo.hk -o foo.c\ngcc -lm -fopenmp foo.c -o foo.bin",
            "title": "Compiling to C"
        },
        {
            "location": "/transforms/hkc/#hkc-compilation",
            "text": "hkc  is a command line tool to compiler Hakaru programs to C. HKC was\ncreated with portability and speed in mind. More recently, OpenMP support is\nbeing added to gain more performance on multi-core machines. Basic command line\nusage of HKC is much like other compilers:  hkc foo.hk -o foo.c  It is possible to go straight to an executable with the  --make ARG  flag, where\nthe argument is the C compiler you would like to use.",
            "title": "HKC Compilation"
        },
        {
            "location": "/transforms/hkc/#type-conversions",
            "text": "The types available in Hakaru programs are the following:  nat ,  int ,  real , prob ,  array(<type>) ,  measure(<type>) , and datum like  true  and  false .  nat  and  int  have a trivial mapping to the C  int  type.  real  becomes a C double . The  prob  type in Hakaru is stored in the log-domain to avoid\nunderflow. In C this corresponds to a  double , but we first take the log of it\nbefore storing it, so we have to take the exp of it to bring it back to the real\nnumbers.  Arrays become structs that contain the size and a pointer to data stored within.\nThe structs are generated at compile time, but there are only four which are\nnamed after the type they contain. Here they all are:  struct arrayNat {\n  int size; int * data;\n};\n\nstruct arrayInt {\n  int size; int * data;\n};\n\nstruct arrayReal {\n  int size; double * data;\n};\n\nstruct arrayProb {\n  int size; double * data;\n};",
            "title": "Type Conversions"
        },
        {
            "location": "/transforms/hkc/#measures",
            "text": "Measures compile to C functions that take a location for a sample, return the\nweight of the measure and store a sample in the location is was given. A simple\nexample is  uniform(0,1)  a measure over type  real .  #include <time.h>\n#include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\ndouble measure(double * s_a)\n {\n    *s_a = ((double)0) + ((double)rand()) / ((double)RAND_MAX) * ((double)1) - ((double)0);\n    return 0;\n }\n\nint main()\n {\n    double sample;\n    while (1)\n    {\n        measure(&sample);\n        printf(\"%.17f\\n\",sample);\n    }\n    return 0;\n }  Recall that weights have type  prob  and are stored in the log-domain. This\nexample has a weight of 1.  Calling  hkc  on a measure will create a function like the one above and also a\nmain function that infinitely takes samples. Using  hkc -F ARG  will produce\njust the function with the name of its argument.",
            "title": "Measures"
        },
        {
            "location": "/transforms/hkc/#lambdas",
            "text": "Lambdas compile to functions in C:  fn x array(real):\n  (summate i from 0 to size(x): x[i])\n   *\n  prob2real(recip(nat2prob((size(x) + 1))))  Becomes:  #include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\nstruct arrayReal {\n   int size; double * data;\n };\n\ndouble fn_a(struct arrayReal x_b)\n {\n   unsigned int i_c;\n   double acc_d;\n   double p_e;\n   double _f;\n   double r_g;\n   acc_d = 0;\n   for (i_c = 0; i_c < x_b.size; i_c++)\n   {\n     acc_d += *(x_b.data + i_c);\n   }\n   p_e = log1p(((1 + x_b.size) - 1));\n   _f = -p_e;\n   r_g = (expm1(_f) + 1);\n   return (r_g * acc_d);\n }  Using the  -F  flag will allow the user to add their own name to a function,\notherwise the name is chosen automatically as  fn_<unique identifier> .",
            "title": "Lambdas"
        },
        {
            "location": "/transforms/hkc/#computations",
            "text": "When compiling a computation, HKC just creates a main function to compute the\nvalue and print it. For example:  summate i from 1 to 100000000:\n  nat2real(i) / nat2real(i)  becomes:  #include <stdlib.h>\n#include <stdio.h>\n#include <math.h>\n\nint main()\n {\n    double result;\n    int i_a;\n    double acc_b;\n    double _c;\n    acc_b = 0;\n    for (i_a = 1; i_a < 100000000; i_a++)\n    {\n       _c = (1 / ((double)i_a));\n       acc_b += (_c * ((double)i_a));\n    }\n    result = acc_b;\n    printf(\"%.17f\\n\",result);\n    return 0;\n }",
            "title": "Computations"
        },
        {
            "location": "/transforms/hkc/#parallel-programs",
            "text": "Calling HKC with the  -j  flag will generate the code with parallel regions to\ncompute the value. The parallel code uses OpenMP directives. To check if you\u2019re\ncompiler supports OpenMP, check  here .  For example, GCC requires the  -fopenmp  flag for OpenMP support:  hkc -j foo.hk -o foo.c\ngcc -lm -fopenmp foo.c -o foo.bin",
            "title": "Parallel Programs"
        },
        {
            "location": "/internals/ast/",
            "text": "Internal Representation of Hakaru terms\n\n\nThe Hakaru AST can be found defined in\n\nhaskell/Language/Hakaru/Syntax/AST.hs\n. It is made up of several parts which this section and the next one will explain.\n\n\nWe should note, this datatype makes use of\n\nAbstract Binding Trees\n\nwhich we discuss in more detail in the next\n\nsection\n. ABTs can be understood as a way to abstract\nthe use of variables in the AST. The advantage of this is it allows\nall variable substitution and manipulation logic to live in one place\nand not be specific to a particular AST.\n\n\nDatakind\n\n\nThe AST is typed using the Hakaru kind, defined in \nhaskell/Language/Types/DataKind.hs\n. All Hakaru types are defined in terms of\nthe primitives in this datakind.\n\n\n-- | The universe\\/kind of Hakaru types.\ndata Hakaru\n    = HNat -- ^ The natural numbers; aka, the non-negative integers.\n\n    -- | The integers.\n    | HInt\n\n    -- | Non-negative real numbers. Unlike what you might expect,\n    -- this is /not/ restructed to the @[0,1]@ interval!\n    | HProb\n\n    -- | The affinely extended real number line. That is, the real\n    -- numbers extended with positive and negative infinities.\n    | HReal\n\n    -- | The measure monad\n    | HMeasure !Hakaru\n\n    -- | The built-in type for uniform arrays.\n    | HArray !Hakaru\n\n    -- | The type of Hakaru functions.\n    | !Hakaru :-> !Hakaru\n\n    -- | A user-defined polynomial datatype. Each such type is\n    -- specified by a \\\"tag\\\" (the @HakaruCon@) which names the type, and a sum-of-product representation of the type itself.\n    | HData !HakaruCon [[HakaruFun]]\n\n\n\n\n\nPlease read Datakind.hs for more details.\n\n\nTerm\n\n\nThe Term datatype includes all the syntactic constructions for the Hakaru language.\nFor all those where we know the number of arguments we expect that language construct\nto get, we define the \n(:$)\n constructor, which takes \nSCons\n and \nSArgs\n datatypes\nas arguments.\n\n\n-- | The generating functor for Hakaru ASTs. This type is given in\n-- open-recursive form, where the first type argument gives the\n-- recursive form. The recursive form @abt@ does not have exactly\n-- the same kind as @Term abt@ because every 'Term' represents a\n-- locally-closed term whereas the underlying @abt@ may bind some\n-- variables.\ndata Term :: ([Hakaru] -> Hakaru -> *) -> Hakaru -> * where\n    -- Simple syntactic forms (i.e., generalized quantifiers)\n    (:$) :: !(SCon args a) -> !(SArgs abt args) -> Term abt a\n\n    -- N-ary operators\n    NaryOp_ :: !(NaryOp a) -> !(Seq (abt '[] a)) -> Term abt a\n\n    -- Literal\\/Constant values\n    Literal_ :: !(Literal a) -> Term abt a\n\n    Empty_ :: !(Sing ('HArray a)) -> Term abt ('HArray a)\n    Array_\n        :: !(abt '[] 'HNat)\n        -> !(abt '[ 'HNat ] a)\n        -> Term abt ('HArray a)\n\n    -- -- User-defined data types\n    -- A data constructor applied to some expressions. N.B., this\n    -- definition only accounts for data constructors which are\n    -- fully saturated. Unsaturated constructors will need to be\n    -- eta-expanded.\n    Datum_ :: !(Datum (abt '[]) (HData' t)) -> Term abt (HData' t)\n\n    -- Generic case-analysis (via ABTs and Structural Focalization).\n    Case_ :: !(abt '[] a) -> [Branch a abt b] -> Term abt b\n\n    -- Linear combinations of measures.\n    Superpose_\n        :: L.NonEmpty (abt '[] 'HProb, abt '[] ('HMeasure a))\n        -> Term abt ('HMeasure a)\n\n    Reject_ :: !(Sing ('HMeasure a)) -> Term abt ('HMeasure a)\n\n\n\n\nSCons and SArgs\n\n\nWhen using \n(:$)\n we have a way to describe primitives where we\nknow the number of arguments they should get. In that regard,\nSArgs is a typed list of abt terms indexed by its size.\n\n\n-- | The arguments to a @(':$')@ node in the 'Term'; that is, a list\n-- of ASTs, where the whole list is indexed by a (type-level) list\n-- of the indices of each element.\ndata SArgs :: ([Hakaru] -> Hakaru -> *) -> [([Hakaru], Hakaru)] -> *\n    where\n    End :: SArgs abt '[]\n    (:*) :: !(abt vars a)\n        -> !(SArgs abt args)\n        -> SArgs abt ( '(vars, a) ': args)\n\n\n\n\nThese are combined with SCons which describes the constructor, and\nthe types it expects for its arguments. For example suppose we had\nan AST for a function \nf\n and it\u2019s argument \nx\n, we could construct\na Term for applying \nf\n to \nx\n by writing \nApp_:$ f :* x :* End\n.\n\n\n-- | The constructor of a @(':$')@ node in the 'Term'. Each of these\n-- constructors denotes a \\\"normal\\/standard\\/basic\\\" syntactic\n-- form (i.e., a generalized quantifier). In the literature, these\n-- syntactic forms are sometimes called \\\"operators\\\", but we avoid\n-- calling them that so as not to introduce confusion vs 'PrimOp'\n-- etc. Instead we use the term \\\"operator\\\" to refer to any primitive\n-- function or constant; that is, non-binding syntactic forms. Also\n-- in the literature, the 'SCon' type itself is usually called the\n-- \\\"signature\\\" of the term language. However, we avoid calling\n-- it that since our 'Term' has constructors other than just @(:$)@,\n-- so 'SCon' does not give a complete signature for our terms.\n--\n-- The main reason for breaking this type out and using it in\n-- conjunction with @(':$')@ and 'SArgs' is so that we can easily\n-- pattern match on /fully saturated/ nodes. For example, we want\n-- to be able to match @MeasureOp_ Uniform :$ lo :* hi :* End@\n-- without needing to deal with 'App_' nodes nor 'viewABT'.\ndata SCon :: [([Hakaru], Hakaru)] -> Hakaru -> * where\n    Lam_ :: SCon '[ '( '[ a ], b ) ] (a ':-> b)\n    App_ :: SCon '[ LC (a ':-> b ), LC a ] b\n    Let_ :: SCon '[ LC a, '( '[ a ], b ) ] b\n\n    CoerceTo_   :: !(Coercion a b) -> SCon '[ LC a ] b\n    UnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a\n\n    PrimOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(PrimOp typs a) -> SCon args a\n    ArrayOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(ArrayOp typs a) -> SCon args a\n    MeasureOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(MeasureOp typs a) -> SCon args ('HMeasure a)\n\n    Dirac :: SCon '[ LC a ] ('HMeasure a)\n\n    MBind :: SCon\n        '[ LC ('HMeasure a)\n        ,  '( '[ a ], 'HMeasure b)\n        ] ('HMeasure b)\n\n    Plate :: SCon\n        '[ LC 'HNat\n        , '( '[ 'HNat ], 'HMeasure a)\n        ] ('HMeasure ('HArray a))\n\n    Chain :: SCon\n        '[ LC 'HNat, LC s\n        , '( '[ s ],  'HMeasure (HPair a s))\n        ] ('HMeasure (HPair ('HArray a) s))\n\n    Integrate\n        :: SCon '[ LC 'HReal, LC 'HReal, '( '[ 'HReal ], 'HProb) ] 'HProb\n\n    Summate\n        :: HDiscrete a\n        -> HSemiring b\n        -> SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Product\n        :: HDiscrete a\n        -> HSemiring b\n        -> SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Expect :: SCon '[ LC ('HMeasure a), '( '[ a ], 'HProb) ] 'HProb\n\n    Observe :: SCon '[ LC ('HMeasure a), LC a ] ('HMeasure a)\n\n\n\n\nYou\u2019ll notice in \nSCon\n there are definitions for PrimOp, MeasureOp, and ArrayOp\nthese are done more organizational purposes and have constructions for the\ndifferent categories of primitives.\n\n\nMeasureOp\n\n\nPrimitives of type measure are defined in MeasureOp.\n\n\n-- | Primitive operators to produce, consume, or transform\n-- distributions\\/measures. This corresponds to the old @Mochastic@\n-- class, except that 'MBind' and 'Superpose_' are handled elsewhere\n-- since they are not simple operators. (Also 'Dirac' is handled\n-- elsewhere since it naturally fits with 'MBind', even though it\n-- is a siple operator.)\ndata MeasureOp :: [Hakaru] -> Hakaru -> * where\n    Lebesgue    :: MeasureOp '[]                 'HReal\n    Counting    :: MeasureOp '[]                 'HInt\n    Categorical :: MeasureOp '[ 'HArray 'HProb ] 'HNat\n    Uniform     :: MeasureOp '[ 'HReal, 'HReal ] 'HReal\n    Normal      :: MeasureOp '[ 'HReal, 'HProb ] 'HReal\n    Poisson     :: MeasureOp '[ 'HProb         ] 'HNat\n    Gamma       :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n    Beta        :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n\n\n\n\nArrayOp\n\n\nPrimitives that involve manipulating value of type array,\nend up in ArrayOp.\n\n\n-- | Primitive operators for consuming or transforming arrays.\ndata ArrayOp :: [Hakaru] -> Hakaru -> * where\n    Index  :: !(Sing a) -> ArrayOp '[ 'HArray a, 'HNat ] a\n    Size   :: !(Sing a) -> ArrayOp '[ 'HArray a ] 'HNat\n    Reduce :: !(Sing a) -> ArrayOp '[ a ':-> a ':-> a, a, 'HArray a ] a\n\n\n\n\nPrimOp\n\n\nAll primitive operations which don\u2019t return something\nof type array or measure are placed in PrimOp\n\n\n-- | Simple primitive functions, and constants.\ndata PrimOp :: [Hakaru] -> Hakaru -> * where\n\n    -- -- -- Here we have /monomorphic/ operators\n    -- -- The Boolean operators\n    Not  :: PrimOp '[ HBool ] HBool\n    -- And, Or, Xor, Iff\n    Impl :: PrimOp '[ HBool, HBool ] HBool\n    -- Impl x y == Or (Not x) y\n    Diff :: PrimOp '[ HBool, HBool ] HBool\n    -- Diff x y == Not (Impl x y)\n    Nand :: PrimOp '[ HBool, HBool ] HBool\n    -- Nand aka Alternative Denial, Sheffer stroke\n    Nor  :: PrimOp '[ HBool, HBool ] HBool\n    -- Nor aka Joint Denial, aka Quine dagger, aka Pierce arrow\n\n    -- -- Trigonometry operators\n    Pi    :: PrimOp '[] 'HProb\n    Sin   :: PrimOp '[ 'HReal ] 'HReal\n    Cos   :: PrimOp '[ 'HReal ] 'HReal\n    Tan   :: PrimOp '[ 'HReal ] 'HReal\n    Asin  :: PrimOp '[ 'HReal ] 'HReal\n    Acos  :: PrimOp '[ 'HReal ] 'HReal\n    Atan  :: PrimOp '[ 'HReal ] 'HReal\n    Sinh  :: PrimOp '[ 'HReal ] 'HReal\n    Cosh  :: PrimOp '[ 'HReal ] 'HReal\n    Tanh  :: PrimOp '[ 'HReal ] 'HReal\n    Asinh :: PrimOp '[ 'HReal ] 'HReal\n    Acosh :: PrimOp '[ 'HReal ] 'HReal\n    Atanh :: PrimOp '[ 'HReal ] 'HReal\n\n    -- -- Other Real\\/Prob-valued operators\n    RealPow   :: PrimOp '[ 'HProb, 'HReal ] 'HProb\n    Exp       :: PrimOp '[ 'HReal ] 'HProb\n    Log       :: PrimOp '[ 'HProb ] 'HReal\n    Infinity  :: HIntegrable a -> PrimOp '[] a\n    GammaFunc :: PrimOp '[ 'HReal ] 'HProb\n    BetaFunc  :: PrimOp '[ 'HProb, 'HProb ] 'HProb\n\n    -- -- -- Here we have the /polymorphic/ operators\n    -- -- HEq and HOrd operators\n    Equal :: !(HEq  a) -> PrimOp '[ a, a ] HBool\n    Less  :: !(HOrd a) -> PrimOp '[ a, a ] HBool\n\n    -- -- HSemiring operators (the non-n-ary ones)\n    NatPow :: !(HSemiring a) -> PrimOp '[ a, 'HNat ] a\n\n    -- -- HRing operators\n    Negate :: !(HRing a) -> PrimOp '[ a ] a\n    Abs    :: !(HRing a) -> PrimOp '[ a ] (NonNegative a)\n    Signum :: !(HRing a) -> PrimOp '[ a ] a\n\n    -- -- HFractional operators\n    Recip :: !(HFractional a) -> PrimOp '[ a ] a\n\n    -- -- HRadical operators\n    NatRoot :: !(HRadical a) -> PrimOp '[ a, 'HNat ] a\n\n    -- -- HContinuous operators\n    Erf :: !(HContinuous a) -> PrimOp '[ a ] a",
            "title": "AST and Hakaru Datakind"
        },
        {
            "location": "/internals/ast/#internal-representation-of-hakaru-terms",
            "text": "The Hakaru AST can be found defined in haskell/Language/Hakaru/Syntax/AST.hs . It is made up of several parts which this section and the next one will explain.  We should note, this datatype makes use of Abstract Binding Trees \nwhich we discuss in more detail in the next section . ABTs can be understood as a way to abstract\nthe use of variables in the AST. The advantage of this is it allows\nall variable substitution and manipulation logic to live in one place\nand not be specific to a particular AST.",
            "title": "Internal Representation of Hakaru terms"
        },
        {
            "location": "/internals/ast/#datakind",
            "text": "The AST is typed using the Hakaru kind, defined in  haskell/Language/Types/DataKind.hs . All Hakaru types are defined in terms of\nthe primitives in this datakind.  -- | The universe\\/kind of Hakaru types.\ndata Hakaru\n    = HNat -- ^ The natural numbers; aka, the non-negative integers.\n\n    -- | The integers.\n    | HInt\n\n    -- | Non-negative real numbers. Unlike what you might expect,\n    -- this is /not/ restructed to the @[0,1]@ interval!\n    | HProb\n\n    -- | The affinely extended real number line. That is, the real\n    -- numbers extended with positive and negative infinities.\n    | HReal\n\n    -- | The measure monad\n    | HMeasure !Hakaru\n\n    -- | The built-in type for uniform arrays.\n    | HArray !Hakaru\n\n    -- | The type of Hakaru functions.\n    | !Hakaru :-> !Hakaru\n\n    -- | A user-defined polynomial datatype. Each such type is\n    -- specified by a \\\"tag\\\" (the @HakaruCon@) which names the type, and a sum-of-product representation of the type itself.\n    | HData !HakaruCon [[HakaruFun]]  Please read Datakind.hs for more details.",
            "title": "Datakind"
        },
        {
            "location": "/internals/ast/#term",
            "text": "The Term datatype includes all the syntactic constructions for the Hakaru language.\nFor all those where we know the number of arguments we expect that language construct\nto get, we define the  (:$)  constructor, which takes  SCons  and  SArgs  datatypes\nas arguments.  -- | The generating functor for Hakaru ASTs. This type is given in\n-- open-recursive form, where the first type argument gives the\n-- recursive form. The recursive form @abt@ does not have exactly\n-- the same kind as @Term abt@ because every 'Term' represents a\n-- locally-closed term whereas the underlying @abt@ may bind some\n-- variables.\ndata Term :: ([Hakaru] -> Hakaru -> *) -> Hakaru -> * where\n    -- Simple syntactic forms (i.e., generalized quantifiers)\n    (:$) :: !(SCon args a) -> !(SArgs abt args) -> Term abt a\n\n    -- N-ary operators\n    NaryOp_ :: !(NaryOp a) -> !(Seq (abt '[] a)) -> Term abt a\n\n    -- Literal\\/Constant values\n    Literal_ :: !(Literal a) -> Term abt a\n\n    Empty_ :: !(Sing ('HArray a)) -> Term abt ('HArray a)\n    Array_\n        :: !(abt '[] 'HNat)\n        -> !(abt '[ 'HNat ] a)\n        -> Term abt ('HArray a)\n\n    -- -- User-defined data types\n    -- A data constructor applied to some expressions. N.B., this\n    -- definition only accounts for data constructors which are\n    -- fully saturated. Unsaturated constructors will need to be\n    -- eta-expanded.\n    Datum_ :: !(Datum (abt '[]) (HData' t)) -> Term abt (HData' t)\n\n    -- Generic case-analysis (via ABTs and Structural Focalization).\n    Case_ :: !(abt '[] a) -> [Branch a abt b] -> Term abt b\n\n    -- Linear combinations of measures.\n    Superpose_\n        :: L.NonEmpty (abt '[] 'HProb, abt '[] ('HMeasure a))\n        -> Term abt ('HMeasure a)\n\n    Reject_ :: !(Sing ('HMeasure a)) -> Term abt ('HMeasure a)",
            "title": "Term"
        },
        {
            "location": "/internals/ast/#scons-and-sargs",
            "text": "When using  (:$)  we have a way to describe primitives where we\nknow the number of arguments they should get. In that regard,\nSArgs is a typed list of abt terms indexed by its size.  -- | The arguments to a @(':$')@ node in the 'Term'; that is, a list\n-- of ASTs, where the whole list is indexed by a (type-level) list\n-- of the indices of each element.\ndata SArgs :: ([Hakaru] -> Hakaru -> *) -> [([Hakaru], Hakaru)] -> *\n    where\n    End :: SArgs abt '[]\n    (:*) :: !(abt vars a)\n        -> !(SArgs abt args)\n        -> SArgs abt ( '(vars, a) ': args)  These are combined with SCons which describes the constructor, and\nthe types it expects for its arguments. For example suppose we had\nan AST for a function  f  and it\u2019s argument  x , we could construct\na Term for applying  f  to  x  by writing  App_:$ f :* x :* End .  -- | The constructor of a @(':$')@ node in the 'Term'. Each of these\n-- constructors denotes a \\\"normal\\/standard\\/basic\\\" syntactic\n-- form (i.e., a generalized quantifier). In the literature, these\n-- syntactic forms are sometimes called \\\"operators\\\", but we avoid\n-- calling them that so as not to introduce confusion vs 'PrimOp'\n-- etc. Instead we use the term \\\"operator\\\" to refer to any primitive\n-- function or constant; that is, non-binding syntactic forms. Also\n-- in the literature, the 'SCon' type itself is usually called the\n-- \\\"signature\\\" of the term language. However, we avoid calling\n-- it that since our 'Term' has constructors other than just @(:$)@,\n-- so 'SCon' does not give a complete signature for our terms.\n--\n-- The main reason for breaking this type out and using it in\n-- conjunction with @(':$')@ and 'SArgs' is so that we can easily\n-- pattern match on /fully saturated/ nodes. For example, we want\n-- to be able to match @MeasureOp_ Uniform :$ lo :* hi :* End@\n-- without needing to deal with 'App_' nodes nor 'viewABT'.\ndata SCon :: [([Hakaru], Hakaru)] -> Hakaru -> * where\n    Lam_ :: SCon '[ '( '[ a ], b ) ] (a ':-> b)\n    App_ :: SCon '[ LC (a ':-> b ), LC a ] b\n    Let_ :: SCon '[ LC a, '( '[ a ], b ) ] b\n\n    CoerceTo_   :: !(Coercion a b) -> SCon '[ LC a ] b\n    UnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a\n\n    PrimOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(PrimOp typs a) -> SCon args a\n    ArrayOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(ArrayOp typs a) -> SCon args a\n    MeasureOp_\n        :: (typs ~ UnLCs args, args ~ LCs typs)\n        => !(MeasureOp typs a) -> SCon args ('HMeasure a)\n\n    Dirac :: SCon '[ LC a ] ('HMeasure a)\n\n    MBind :: SCon\n        '[ LC ('HMeasure a)\n        ,  '( '[ a ], 'HMeasure b)\n        ] ('HMeasure b)\n\n    Plate :: SCon\n        '[ LC 'HNat\n        , '( '[ 'HNat ], 'HMeasure a)\n        ] ('HMeasure ('HArray a))\n\n    Chain :: SCon\n        '[ LC 'HNat, LC s\n        , '( '[ s ],  'HMeasure (HPair a s))\n        ] ('HMeasure (HPair ('HArray a) s))\n\n    Integrate\n        :: SCon '[ LC 'HReal, LC 'HReal, '( '[ 'HReal ], 'HProb) ] 'HProb\n\n    Summate\n        :: HDiscrete a\n        -> HSemiring b\n        -> SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Product\n        :: HDiscrete a\n        -> HSemiring b\n        -> SCon '[ LC a, LC a, '( '[ a ], b) ] b\n\n    Expect :: SCon '[ LC ('HMeasure a), '( '[ a ], 'HProb) ] 'HProb\n\n    Observe :: SCon '[ LC ('HMeasure a), LC a ] ('HMeasure a)  You\u2019ll notice in  SCon  there are definitions for PrimOp, MeasureOp, and ArrayOp\nthese are done more organizational purposes and have constructions for the\ndifferent categories of primitives.",
            "title": "SCons and SArgs"
        },
        {
            "location": "/internals/ast/#measureop",
            "text": "Primitives of type measure are defined in MeasureOp.  -- | Primitive operators to produce, consume, or transform\n-- distributions\\/measures. This corresponds to the old @Mochastic@\n-- class, except that 'MBind' and 'Superpose_' are handled elsewhere\n-- since they are not simple operators. (Also 'Dirac' is handled\n-- elsewhere since it naturally fits with 'MBind', even though it\n-- is a siple operator.)\ndata MeasureOp :: [Hakaru] -> Hakaru -> * where\n    Lebesgue    :: MeasureOp '[]                 'HReal\n    Counting    :: MeasureOp '[]                 'HInt\n    Categorical :: MeasureOp '[ 'HArray 'HProb ] 'HNat\n    Uniform     :: MeasureOp '[ 'HReal, 'HReal ] 'HReal\n    Normal      :: MeasureOp '[ 'HReal, 'HProb ] 'HReal\n    Poisson     :: MeasureOp '[ 'HProb         ] 'HNat\n    Gamma       :: MeasureOp '[ 'HProb, 'HProb ] 'HProb\n    Beta        :: MeasureOp '[ 'HProb, 'HProb ] 'HProb",
            "title": "MeasureOp"
        },
        {
            "location": "/internals/ast/#arrayop",
            "text": "Primitives that involve manipulating value of type array,\nend up in ArrayOp.  -- | Primitive operators for consuming or transforming arrays.\ndata ArrayOp :: [Hakaru] -> Hakaru -> * where\n    Index  :: !(Sing a) -> ArrayOp '[ 'HArray a, 'HNat ] a\n    Size   :: !(Sing a) -> ArrayOp '[ 'HArray a ] 'HNat\n    Reduce :: !(Sing a) -> ArrayOp '[ a ':-> a ':-> a, a, 'HArray a ] a",
            "title": "ArrayOp"
        },
        {
            "location": "/internals/ast/#primop",
            "text": "All primitive operations which don\u2019t return something\nof type array or measure are placed in PrimOp  -- | Simple primitive functions, and constants.\ndata PrimOp :: [Hakaru] -> Hakaru -> * where\n\n    -- -- -- Here we have /monomorphic/ operators\n    -- -- The Boolean operators\n    Not  :: PrimOp '[ HBool ] HBool\n    -- And, Or, Xor, Iff\n    Impl :: PrimOp '[ HBool, HBool ] HBool\n    -- Impl x y == Or (Not x) y\n    Diff :: PrimOp '[ HBool, HBool ] HBool\n    -- Diff x y == Not (Impl x y)\n    Nand :: PrimOp '[ HBool, HBool ] HBool\n    -- Nand aka Alternative Denial, Sheffer stroke\n    Nor  :: PrimOp '[ HBool, HBool ] HBool\n    -- Nor aka Joint Denial, aka Quine dagger, aka Pierce arrow\n\n    -- -- Trigonometry operators\n    Pi    :: PrimOp '[] 'HProb\n    Sin   :: PrimOp '[ 'HReal ] 'HReal\n    Cos   :: PrimOp '[ 'HReal ] 'HReal\n    Tan   :: PrimOp '[ 'HReal ] 'HReal\n    Asin  :: PrimOp '[ 'HReal ] 'HReal\n    Acos  :: PrimOp '[ 'HReal ] 'HReal\n    Atan  :: PrimOp '[ 'HReal ] 'HReal\n    Sinh  :: PrimOp '[ 'HReal ] 'HReal\n    Cosh  :: PrimOp '[ 'HReal ] 'HReal\n    Tanh  :: PrimOp '[ 'HReal ] 'HReal\n    Asinh :: PrimOp '[ 'HReal ] 'HReal\n    Acosh :: PrimOp '[ 'HReal ] 'HReal\n    Atanh :: PrimOp '[ 'HReal ] 'HReal\n\n    -- -- Other Real\\/Prob-valued operators\n    RealPow   :: PrimOp '[ 'HProb, 'HReal ] 'HProb\n    Exp       :: PrimOp '[ 'HReal ] 'HProb\n    Log       :: PrimOp '[ 'HProb ] 'HReal\n    Infinity  :: HIntegrable a -> PrimOp '[] a\n    GammaFunc :: PrimOp '[ 'HReal ] 'HProb\n    BetaFunc  :: PrimOp '[ 'HProb, 'HProb ] 'HProb\n\n    -- -- -- Here we have the /polymorphic/ operators\n    -- -- HEq and HOrd operators\n    Equal :: !(HEq  a) -> PrimOp '[ a, a ] HBool\n    Less  :: !(HOrd a) -> PrimOp '[ a, a ] HBool\n\n    -- -- HSemiring operators (the non-n-ary ones)\n    NatPow :: !(HSemiring a) -> PrimOp '[ a, 'HNat ] a\n\n    -- -- HRing operators\n    Negate :: !(HRing a) -> PrimOp '[ a ] a\n    Abs    :: !(HRing a) -> PrimOp '[ a ] (NonNegative a)\n    Signum :: !(HRing a) -> PrimOp '[ a ] a\n\n    -- -- HFractional operators\n    Recip :: !(HFractional a) -> PrimOp '[ a ] a\n\n    -- -- HRadical operators\n    NatRoot :: !(HRadical a) -> PrimOp '[ a, 'HNat ] a\n\n    -- -- HContinuous operators\n    Erf :: !(HContinuous a) -> PrimOp '[ a ] a",
            "title": "PrimOp"
        },
        {
            "location": "/internals/abt/",
            "text": "Abstract Binding Trees\n\n\nHakaru makes use of many program transformations in its codebase.\nBecause of this, a special mechanism is included for handing\nvariable bindings and substitutions. We abstract this into its\nown typeclass called \nABT\n. This can be found in \nLanguage.Hakaru.Syntax.ABT\n.\n\n\nBelow is an excerpt of this typeclass\n\n\nclass ABT (syn :: ([k] -> k -> *) -> k -> *) (abt :: [k] -> k -> *) | abt -> syn where\n    -- Smart constructors for building a 'View' and then injecting it into the @abt@.\n    syn  :: syn abt  a -> abt '[] a\n    var  :: Variable a -> abt '[] a\n    bind :: Variable a -> abt xs b -> abt (a ': xs) b\n    caseBind :: abt (x ': xs) a -> (Variable x -> abt xs a -> r) -> r\n    ...\n\n\n\n\nThe advantage of having this typeclass is that we think about variable binding\nindependently of the AST for our language. For example, we can define variable\nsubstitution once and for all.",
            "title": "ABT"
        },
        {
            "location": "/internals/abt/#abstract-binding-trees",
            "text": "Hakaru makes use of many program transformations in its codebase.\nBecause of this, a special mechanism is included for handing\nvariable bindings and substitutions. We abstract this into its\nown typeclass called  ABT . This can be found in  Language.Hakaru.Syntax.ABT .  Below is an excerpt of this typeclass  class ABT (syn :: ([k] -> k -> *) -> k -> *) (abt :: [k] -> k -> *) | abt -> syn where\n    -- Smart constructors for building a 'View' and then injecting it into the @abt@.\n    syn  :: syn abt  a -> abt '[] a\n    var  :: Variable a -> abt '[] a\n    bind :: Variable a -> abt xs b -> abt (a ': xs) b\n    caseBind :: abt (x ': xs) a -> (Variable x -> abt xs a -> r) -> r\n    ...  The advantage of having this typeclass is that we think about variable binding\nindependently of the AST for our language. For example, we can define variable\nsubstitution once and for all.",
            "title": "Abstract Binding Trees"
        },
        {
            "location": "/internals/datums/",
            "text": "Data representation\n\n\nData types are stored using a sum of product representation.\nThey can be found in \nLanguage.Hakaru.Syntax.Datum\n.\n\n\n-- The first component is a hint for what the data constructor\n-- should be called when pretty-printing, giving error messages,\n-- etc. Like the hints for variable names, its value is not actually\n-- used to decide which constructor is meant or which pattern\n-- matches.\ndata Datum :: (Hakaru -> *) -> Hakaru -> * where\n    Datum\n        :: {-# UNPACK #-} !Text\n        -> !(Sing (HData' t))\n        -> !(DatumCode (Code t) ast (HData' t))\n        -> Datum ast (HData' t)\n\n-- | The intermediate components of a data constructor. The intuition\n-- behind the two indices is that the @[[HakaruFun]]@ is a functor\n-- applied to the Hakaru type. Initially the @[[HakaruFun]]@ functor\n-- will be the 'Code' associated with the Hakaru type; hence it's\n-- the one-step unrolling of the fixed point for our recursive\n-- datatypes. But as we go along, we'll be doing induction on the\n-- @[[HakaruFun]]@ functor.\ndata DatumCode :: [[HakaruFun]] -> (Hakaru -> *) -> Hakaru -> * where\n    -- Skip rightwards along the sum.\n    Inr :: !(DatumCode  xss abt a) -> DatumCode (xs ': xss) abt a\n    -- Inject into the sum.\n    Inl :: !(DatumStruct xs abt a) -> DatumCode (xs ': xss) abt a\n\ndata DatumStruct :: [HakaruFun] -> (Hakaru -> *) -> Hakaru -> * where\n    -- BUG: haddock doesn't like annotations on GADT constructors\n    -- <https://github.com/hakaru-dev/hakaru/issues/6>\n\n    -- Combine components of the product. (\\\"et\\\" means \\\"and\\\" in Latin)\n    Et  :: !(DatumFun    x         abt a)\n        -> !(DatumStruct xs        abt a)\n        ->   DatumStruct (x ': xs) abt a\n\n    -- Close off the product.\n    Done :: DatumStruct '[] abt a\n\ndata DatumFun :: HakaruFun -> (Hakaru -> *) -> Hakaru -> * where\n    -- Hit a leaf which isn't a recursive component of the datatype.\n    Konst :: !(ast b) -> DatumFun ('K b) ast a\n    -- Hit a leaf which is a recursive component of the datatype.\n    Ident :: !(ast a) -> DatumFun 'I     ast a\n\n\n\n\nIn Hakaru we have implemented Bool, Pair, Either, Maybe, and List.",
            "title": "Datums"
        },
        {
            "location": "/internals/datums/#data-representation",
            "text": "Data types are stored using a sum of product representation.\nThey can be found in  Language.Hakaru.Syntax.Datum .  -- The first component is a hint for what the data constructor\n-- should be called when pretty-printing, giving error messages,\n-- etc. Like the hints for variable names, its value is not actually\n-- used to decide which constructor is meant or which pattern\n-- matches.\ndata Datum :: (Hakaru -> *) -> Hakaru -> * where\n    Datum\n        :: {-# UNPACK #-} !Text\n        -> !(Sing (HData' t))\n        -> !(DatumCode (Code t) ast (HData' t))\n        -> Datum ast (HData' t)\n\n-- | The intermediate components of a data constructor. The intuition\n-- behind the two indices is that the @[[HakaruFun]]@ is a functor\n-- applied to the Hakaru type. Initially the @[[HakaruFun]]@ functor\n-- will be the 'Code' associated with the Hakaru type; hence it's\n-- the one-step unrolling of the fixed point for our recursive\n-- datatypes. But as we go along, we'll be doing induction on the\n-- @[[HakaruFun]]@ functor.\ndata DatumCode :: [[HakaruFun]] -> (Hakaru -> *) -> Hakaru -> * where\n    -- Skip rightwards along the sum.\n    Inr :: !(DatumCode  xss abt a) -> DatumCode (xs ': xss) abt a\n    -- Inject into the sum.\n    Inl :: !(DatumStruct xs abt a) -> DatumCode (xs ': xss) abt a\n\ndata DatumStruct :: [HakaruFun] -> (Hakaru -> *) -> Hakaru -> * where\n    -- BUG: haddock doesn't like annotations on GADT constructors\n    -- <https://github.com/hakaru-dev/hakaru/issues/6>\n\n    -- Combine components of the product. (\\\"et\\\" means \\\"and\\\" in Latin)\n    Et  :: !(DatumFun    x         abt a)\n        -> !(DatumStruct xs        abt a)\n        ->   DatumStruct (x ': xs) abt a\n\n    -- Close off the product.\n    Done :: DatumStruct '[] abt a\n\ndata DatumFun :: HakaruFun -> (Hakaru -> *) -> Hakaru -> * where\n    -- Hit a leaf which isn't a recursive component of the datatype.\n    Konst :: !(ast b) -> DatumFun ('K b) ast a\n    -- Hit a leaf which is a recursive component of the datatype.\n    Ident :: !(ast a) -> DatumFun 'I     ast a  In Hakaru we have implemented Bool, Pair, Either, Maybe, and List.",
            "title": "Data representation"
        },
        {
            "location": "/internals/coercions/",
            "text": "Coercions\n\n\nFor convenience, Hakaru offers functions to convert between the four\ndifferent numeric types in the language. These types are\n\n\n\n\nnat - Natural numbers\n\n\nint - Integers\n\n\nprob - Positive real numbers\n\n\nreal - Real numbers\n\n\n\n\nAmongst these types there are a collection of safe and unsafe\ncoercions. A safe coercion is one which is always guaranteed to\nbe valid. For example, converting a \nnat\n to an \nint\n is always\nsafe. Converting an \nint\n to a \nnat\n is unsafe as the value can\nnegative, and lead to runtime errors.\n\n\nThese are represented in the AST using the \nCoerceTo\n and \nUnsafeFrom\n\nconstructors. Note that coercions are always defined in terms of the\nsafe direction to go to.\n\n\nCoerceTo_   :: !(Coercion a b) -> SCon '[ LC a ] b\nUnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a\n\n\n\n\nInternally, coercions are specified using the \nCoercion\n datatype. This\ndatatype states that each coercion is made up of a series of primitive\ncoercions.\n\n\ndata Coercion :: Hakaru -> Hakaru -> * where\n    CNil :: Coercion a a\n    CCons :: !(PrimCoercion a b) -> !(Coercion b c) -> Coercion a c\n\n\n\n\nThese primitive coercions can either involve loosening a restriction\non the sign of the value, or changing the numeric value to be over\na continuous value. For example, to coerce from int to real, we would\nhave a single \nCoercion\n with a \nPrimCoercion\n in it with the Continuous\ndata constructor.\n\n\ndata PrimCoercion :: Hakaru -> Hakaru -> * where\n    Signed     :: !(HRing a)       -> PrimCoercion (NonNegative a) a\n    Continuous :: !(HContinuous a) -> PrimCoercion (HIntegral   a) a",
            "title": "Coercions"
        },
        {
            "location": "/internals/coercions/#coercions",
            "text": "For convenience, Hakaru offers functions to convert between the four\ndifferent numeric types in the language. These types are   nat - Natural numbers  int - Integers  prob - Positive real numbers  real - Real numbers   Amongst these types there are a collection of safe and unsafe\ncoercions. A safe coercion is one which is always guaranteed to\nbe valid. For example, converting a  nat  to an  int  is always\nsafe. Converting an  int  to a  nat  is unsafe as the value can\nnegative, and lead to runtime errors.  These are represented in the AST using the  CoerceTo  and  UnsafeFrom \nconstructors. Note that coercions are always defined in terms of the\nsafe direction to go to.  CoerceTo_   :: !(Coercion a b) -> SCon '[ LC a ] b\nUnsafeFrom_ :: !(Coercion a b) -> SCon '[ LC b ] a  Internally, coercions are specified using the  Coercion  datatype. This\ndatatype states that each coercion is made up of a series of primitive\ncoercions.  data Coercion :: Hakaru -> Hakaru -> * where\n    CNil :: Coercion a a\n    CCons :: !(PrimCoercion a b) -> !(Coercion b c) -> Coercion a c  These primitive coercions can either involve loosening a restriction\non the sign of the value, or changing the numeric value to be over\na continuous value. For example, to coerce from int to real, we would\nhave a single  Coercion  with a  PrimCoercion  in it with the Continuous\ndata constructor.  data PrimCoercion :: Hakaru -> Hakaru -> * where\n    Signed     :: !(HRing a)       -> PrimCoercion (NonNegative a) a\n    Continuous :: !(HContinuous a) -> PrimCoercion (HIntegral   a) a",
            "title": "Coercions"
        },
        {
            "location": "/internals/transforms/",
            "text": "Program transformations in Hakaru\n\n\nCoalesce\n\n\nCoalesce is an internal transformation that works on the untyped Hakaru AST. It\ntakes recursive \nNAryOp\n terms that have the same type and combines them into\na single term. For instance:\n\n\n3.0 + 1.5 + 0.3\n\n\n\n\nis parser as:\n\n\nNaryOp Sum [3.0, NaryOp Sum [1.5, NaryOp Sum [0.3]]]\n\n\n\n\nwhich when coalesced becomes:\n\n\nNaryOp Sum [3.0,1.5,0.3]\n\n\n\n\nOptimizations\n\n\nThe Hakaru AST has a suite of standard compiler optimizations which have\na substantial effect on the runtime of the resulting program.\nThe current pipeline is described by the \noptimizations\n variable in\n\nLanguage.Hakaru.Syntax.Transforms\n.\nIn order, the optimizations performed are:\n\n\n\n\nA-normalization\n\n\nUniquification of variables (needed for let-floating)\n\n\nLet-floating\n\n\nCommon subexpression elimination\n\n\nPruning of dead binders\n\n\nUniquification of variables (for the C backend)\n\n\n\n\nEach pass is described in more detail below.\n\n\nA-normalization\n\n\nFound in \nLanguage.Hakaru.Syntax.ANF\n\n\nA-normalization converts expressions into \nadministrative normal form\n (ANF).\nThis ensures that all intermediate values are named and all arguments to\nfunctions or primitive operations are either literals or variables.\nANF is a common program representation for functional language compilers which\ncan simplify some compiler passes and make others more effective.\nAs an example, consider\n\n\n(add1 (let ([x (f y)]) 5))\n\n\n\n\nThis expression in ANF looks like the following\n\n\n(let ([x (f y)]) (add1 5))\n\n\n\n\nwhich opens up the opportunity for constant folding to eliminate the \n(add1 5)\n\nexpression.\nThis pass exists mostly to simplify the implementation of CSE, but is useful for\nother passes as well.\n\n\nUniquification\n\n\nFound in \nLanguage.Hakaru.Syntax.Uniquify\n\n\nEnsures all variables in the program have unique variable identifiers.\nThis is not strictly necessary, but simplifies the implementation of other\npasses, and several of the following passes rely on this property.\n\n\nLet-floating\n\n\nFound in \nLanguage.Hakaru.Syntax.Hoist\n\n\nSee \nLet-Floating: Moving Bindings to Give Faster Programs (1996)\nby Simon Peyton Jones , Will Partain , Andr\u00e9 Santos\n\n\nLet-floating alters the bindings structure of the program in order to improve\nperformance.\nTypically, this entails moving definitions into or out of lambda expressions.\nWhen a lambda expression encodes a loop, this effectively accomplishes\nloop invariant code motion.\nThis pass only moves definitions upward in the AST.\nFor the most part, we are only interested in looping constructs like \nsummate\n and\n\nproduct\n, and moving \nsummate\n expressions out of other \nsummate\n or \nproduct\n\nexpressions when they do not depend on the index.\nThis can radically alter the asymptotics of the resulting program, as nested\nloops are converted into sequentially executed loops.\n\n\nThe only assumption this pass makes about the input AST is that all variable\nidentifiers are unique.\nThis is to handle the case where two branches of a match statement introduce the\nsame variable.\nIf both binders are hoisted out of the match statement, they one binding will\nshadow the other.\n\n\nThis pass, as implemented, unconditionally floats expression to where their data\ndependencies are fulfilled.\nThis is not safe in a general purpose language, and we may need to layer some\nheuristics on top of this pass to make it less aggressive if we end up\nintroducing performance regressions.\n\n\nCommon Subexpression Elimination\n\n\nFound in \nLanguage.Hakaru.Syntax.CSE\n\n\nCommon subexpression elimination eliminates redundant computation by reusing\nresults for equivalent expressions.\nThe current implementation of this pass relies on the program being in ANF.\n\n\nANF simplifies the implementation of CSE greatly by ensuring all expressions are\nnamed and that if two expressions may be shared, one of them is let-bound so\nthat it dominates the other.\nIn short, ANF simplifies the program to a simple top-down traversal of the AST.\nConsider the example\n\n\n(+ (add1 z) (add1 z))\n\n\n\n\nEliminating the common expression \n(add1 z)\n requires us to know the evaluation\norder of arguments, recognize when an expression is duplicated, and introduce it\nwith a new name that dominates all use sites of that expression.\nHowever, an expression in ANF allows us to perform CSE simply by keeping track\nof let-bound expressions and propagating those expressions downward into the\nAST.\nConsider the example in ANF\n\n\n(let ([t1 (add1 z)])\n  (let ([t2 (add1 z)])\n    (+ t1 t2)))\n\n\n\n\nTo remove the common subexpression, we simply have to note that the \n(add1 z)\n\nbound to \nt2\n is equivalent to the expression bound to \nt1\n and replace it with\nthe variable \nt1\n.\n\n\n(let ([t1 (add1 z)])\n  (let ([t2 t1])\n    (+ t1 t2)))\n\n\n\n\nTrivial bindings can then be eliminated, if desired, giving\n\n\n(let ([t1 (add1 z)])\n  (+ t1 t1)))\n\n\n\n\nA major goal of CSE is to cleanup any work which is duplicated by the\nlet-floating pass.\n\n\nPruning\n\n\nThis is essentially a limited form of dead code elimination.\nIf an expression is bound to a variable which is never referenced, then that\nexpression need never be executed, as the code language has no side effects.\nThis pass serves to clean up some of the junk introduced by other passes.\n\n\nCases which are handled\n\n\n\n\n(let ([x e1]) e2) => e2 if x not in fv(e2)\n\n\n(let ([x e1]) x)  => e1",
            "title": "Transformaitons"
        },
        {
            "location": "/internals/transforms/#program-transformations-in-hakaru",
            "text": "",
            "title": "Program transformations in Hakaru"
        },
        {
            "location": "/internals/transforms/#coalesce",
            "text": "Coalesce is an internal transformation that works on the untyped Hakaru AST. It\ntakes recursive  NAryOp  terms that have the same type and combines them into\na single term. For instance:  3.0 + 1.5 + 0.3  is parser as:  NaryOp Sum [3.0, NaryOp Sum [1.5, NaryOp Sum [0.3]]]  which when coalesced becomes:  NaryOp Sum [3.0,1.5,0.3]",
            "title": "Coalesce"
        },
        {
            "location": "/internals/transforms/#optimizations",
            "text": "The Hakaru AST has a suite of standard compiler optimizations which have\na substantial effect on the runtime of the resulting program.\nThe current pipeline is described by the  optimizations  variable in Language.Hakaru.Syntax.Transforms .\nIn order, the optimizations performed are:   A-normalization  Uniquification of variables (needed for let-floating)  Let-floating  Common subexpression elimination  Pruning of dead binders  Uniquification of variables (for the C backend)   Each pass is described in more detail below.  A-normalization  Found in  Language.Hakaru.Syntax.ANF  A-normalization converts expressions into  administrative normal form  (ANF).\nThis ensures that all intermediate values are named and all arguments to\nfunctions or primitive operations are either literals or variables.\nANF is a common program representation for functional language compilers which\ncan simplify some compiler passes and make others more effective.\nAs an example, consider  (add1 (let ([x (f y)]) 5))  This expression in ANF looks like the following  (let ([x (f y)]) (add1 5))  which opens up the opportunity for constant folding to eliminate the  (add1 5) \nexpression.\nThis pass exists mostly to simplify the implementation of CSE, but is useful for\nother passes as well.  Uniquification  Found in  Language.Hakaru.Syntax.Uniquify  Ensures all variables in the program have unique variable identifiers.\nThis is not strictly necessary, but simplifies the implementation of other\npasses, and several of the following passes rely on this property.  Let-floating  Found in  Language.Hakaru.Syntax.Hoist  See  Let-Floating: Moving Bindings to Give Faster Programs (1996)\nby Simon Peyton Jones , Will Partain , Andr\u00e9 Santos  Let-floating alters the bindings structure of the program in order to improve\nperformance.\nTypically, this entails moving definitions into or out of lambda expressions.\nWhen a lambda expression encodes a loop, this effectively accomplishes\nloop invariant code motion.\nThis pass only moves definitions upward in the AST.\nFor the most part, we are only interested in looping constructs like  summate  and product , and moving  summate  expressions out of other  summate  or  product \nexpressions when they do not depend on the index.\nThis can radically alter the asymptotics of the resulting program, as nested\nloops are converted into sequentially executed loops.  The only assumption this pass makes about the input AST is that all variable\nidentifiers are unique.\nThis is to handle the case where two branches of a match statement introduce the\nsame variable.\nIf both binders are hoisted out of the match statement, they one binding will\nshadow the other.  This pass, as implemented, unconditionally floats expression to where their data\ndependencies are fulfilled.\nThis is not safe in a general purpose language, and we may need to layer some\nheuristics on top of this pass to make it less aggressive if we end up\nintroducing performance regressions.  Common Subexpression Elimination  Found in  Language.Hakaru.Syntax.CSE  Common subexpression elimination eliminates redundant computation by reusing\nresults for equivalent expressions.\nThe current implementation of this pass relies on the program being in ANF.  ANF simplifies the implementation of CSE greatly by ensuring all expressions are\nnamed and that if two expressions may be shared, one of them is let-bound so\nthat it dominates the other.\nIn short, ANF simplifies the program to a simple top-down traversal of the AST.\nConsider the example  (+ (add1 z) (add1 z))  Eliminating the common expression  (add1 z)  requires us to know the evaluation\norder of arguments, recognize when an expression is duplicated, and introduce it\nwith a new name that dominates all use sites of that expression.\nHowever, an expression in ANF allows us to perform CSE simply by keeping track\nof let-bound expressions and propagating those expressions downward into the\nAST.\nConsider the example in ANF  (let ([t1 (add1 z)])\n  (let ([t2 (add1 z)])\n    (+ t1 t2)))  To remove the common subexpression, we simply have to note that the  (add1 z) \nbound to  t2  is equivalent to the expression bound to  t1  and replace it with\nthe variable  t1 .  (let ([t1 (add1 z)])\n  (let ([t2 t1])\n    (+ t1 t2)))  Trivial bindings can then be eliminated, if desired, giving  (let ([t1 (add1 z)])\n  (+ t1 t1)))  A major goal of CSE is to cleanup any work which is duplicated by the\nlet-floating pass.  Pruning  This is essentially a limited form of dead code elimination.\nIf an expression is bound to a variable which is never referenced, then that\nexpression need never be executed, as the code language has no side effects.\nThis pass serves to clean up some of the junk introduced by other passes.  Cases which are handled   (let ([x e1]) e2) => e2 if x not in fv(e2)  (let ([x e1]) x)  => e1",
            "title": "Optimizations"
        },
        {
            "location": "/internals/testing/",
            "text": "Testing infrastructure in Hakaru\n\n\nHakaru can be tested by running \ncabal test\n from the\nroot directory of the project.\n\n\nTests written in Hakaru will be found in the \ntests/\n\nsubdirectory at the root of the project. Tests written\nin Haskell can be found at \nhaskell/Tests/\n.\n\n\nNote: tests related to \nsimplify\n and which require Maple will also be\nrun if a local installation of Maple is detected.",
            "title": "Testing"
        },
        {
            "location": "/internals/testing/#testing-infrastructure-in-hakaru",
            "text": "Hakaru can be tested by running  cabal test  from the\nroot directory of the project.  Tests written in Hakaru will be found in the  tests/ \nsubdirectory at the root of the project. Tests written\nin Haskell can be found at  haskell/Tests/ .  Note: tests related to  simplify  and which require Maple will also be\nrun if a local installation of Maple is detected.",
            "title": "Testing infrastructure in Hakaru"
        },
        {
            "location": "/internals/newfeature/",
            "text": "Adding a feature to the Hakaru language\n\n\nTo add a feature to the Hakaru language you must\n\n\n\n\nAdd an entry to the AST\n\n\nUpdate symbol resolution and optionally the parser to recognize this construct\n\n\nUpdate the pretty printers if this is something exposed to users\n\n\nUpdate the typechecker to handle it\n\n\nUpdate all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it\n\n\nUpdate the sampler if this primitive is intended to exist at runtime\n\n\nUpdate the compilers to emit the right code for this symbol\n\n\n\n\nWe give an example of what this looks like by adding \ndouble\n to the language.",
            "title": "Adding a Language Feautre"
        },
        {
            "location": "/internals/newfeature/#adding-a-feature-to-the-hakaru-language",
            "text": "To add a feature to the Hakaru language you must   Add an entry to the AST  Update symbol resolution and optionally the parser to recognize this construct  Update the pretty printers if this is something exposed to users  Update the typechecker to handle it  Update all the program transformations (Expect, Disintegrate, Simplify, etc) to handle it  Update the sampler if this primitive is intended to exist at runtime  Update the compilers to emit the right code for this symbol   We give an example of what this looks like by adding  double  to the language.",
            "title": "Adding a feature to the Hakaru language"
        },
        {
            "location": "/examples/",
            "text": "Examples\n\n\nGaussian Mixture Model\n\n\nBelow is a model for a Gaussian Mixture model. This can be seen\nas a Bayesian version of K-means clustering.\n\n\n# Prelude to define dirichlet\ndef add(a prob, b prob):\n    a + b\n\ndef sum(a array(prob)):\n    reduce(add, 0, a)\n\ndef normalize(x array(prob)):\n    total = sum(x)\n    array i of size(x):\n       x[i] / total\n\ndef dirichlet(as array(prob)):\n    xs <~ plate i of int2nat(size(as)-1):\n            beta(summate j from i+1 to size(as): as[j],\n                 as[i])\n    return array i of size(as):\n             x = product j from 0 to i: xs[j]\n             x * if i+1==size(as): 1 else: real2prob(1-xs[i])\n\n\n# num of clusters\nK = 5\n# num of points\nN = 20\n\n# prior probability of picking cluster K\npi  <~ dirichlet(array _ of K: 1)\n# prior on mean and precision\nmu  <~ plate _ of K:\n         normal(0, 5e-9)\ntau <~ plate _ of K:\n         gamma(2, 0.05)\n# observed data\nx   <~ plate _ of N:\n         i <~ categorical(pi)\n         normal(mu[i], tau[i])\n\nreturn (x, mu). pair(array(real), array(real))\n\n\n\n\nLatent Dirichlet Allocation\n\n\nBelow is the LDA topic model.\n\n\nK = 2 # number of topics\nM = 3 # number of docs\nV = 7 # size of vocabulary\n\n# number of words in each document\ndoc = [4, 5, 3]\n\ntopic_prior = array _ of K: 1.0\nword_prior  = array _ of V: 1.0\n\nphi <~ plate _ of K:     # word dist for topic k\n         dirichlet(word_prior)\n\n# likelihood\nz   <~ plate m of M:\n         theta <~ dirichlet(topic_prior)\n         plate _ of doc[m]: # topic marker for word n in doc m\n           categorical(theta)\n\nw   <~ plate m of M: # for doc m\n         plate n of doc[m]: # for word n in doc m\n           categorical(phi[z[m][n]])\n\nreturn (w, z)",
            "title": "Examples"
        },
        {
            "location": "/examples/#examples",
            "text": "Gaussian Mixture Model  Below is a model for a Gaussian Mixture model. This can be seen\nas a Bayesian version of K-means clustering.  # Prelude to define dirichlet\ndef add(a prob, b prob):\n    a + b\n\ndef sum(a array(prob)):\n    reduce(add, 0, a)\n\ndef normalize(x array(prob)):\n    total = sum(x)\n    array i of size(x):\n       x[i] / total\n\ndef dirichlet(as array(prob)):\n    xs <~ plate i of int2nat(size(as)-1):\n            beta(summate j from i+1 to size(as): as[j],\n                 as[i])\n    return array i of size(as):\n             x = product j from 0 to i: xs[j]\n             x * if i+1==size(as): 1 else: real2prob(1-xs[i])\n\n\n# num of clusters\nK = 5\n# num of points\nN = 20\n\n# prior probability of picking cluster K\npi  <~ dirichlet(array _ of K: 1)\n# prior on mean and precision\nmu  <~ plate _ of K:\n         normal(0, 5e-9)\ntau <~ plate _ of K:\n         gamma(2, 0.05)\n# observed data\nx   <~ plate _ of N:\n         i <~ categorical(pi)\n         normal(mu[i], tau[i])\n\nreturn (x, mu). pair(array(real), array(real))  Latent Dirichlet Allocation  Below is the LDA topic model.  K = 2 # number of topics\nM = 3 # number of docs\nV = 7 # size of vocabulary\n\n# number of words in each document\ndoc = [4, 5, 3]\n\ntopic_prior = array _ of K: 1.0\nword_prior  = array _ of V: 1.0\n\nphi <~ plate _ of K:     # word dist for topic k\n         dirichlet(word_prior)\n\n# likelihood\nz   <~ plate m of M:\n         theta <~ dirichlet(topic_prior)\n         plate _ of doc[m]: # topic marker for word n in doc m\n           categorical(theta)\n\nw   <~ plate m of M: # for doc m\n         plate n of doc[m]: # for word n in doc m\n           categorical(phi[z[m][n]])\n\nreturn (w, z)",
            "title": "Examples"
        }
    ]
}